{"pageProps":{"posts":[{"content":"\nIn October 2020 D2IQ [announced](https://d2iq.com/blog/d2iq-takes-the-next-step-forward) that they are moving onwards with their Kubernetes offering. Vandebron has been a D2IQ customer for their DCOS offering, we were just in the middle of a migration of our first workloads to DCOS Enterprise. We have evaluated the D2IQ K8s offering and decided to go for another Kubernetes product. We had a few migrations over the years, we migrated from Azure to AWS, we migrated workloads from normal instances to spot instances and all these migrations were done with nearly any downtime. We plan to reduce the downtime to a couple of minutes this migration and this is a real challenge. The first challenge that we will discuss today: We want to pair our Kubernetes clusters to the DCOS/Mesos clusters, while we move a workload it should be able to connect to its dependencies in the DCOS cluster. We use DCOS for our NoSQL databases like Cassandra, internal data that we want to keep internal. Pairing DCOS and Kubernetes clusters enable us to reduce downtime, enabling us to switch back if we run into issues and move faster because it reduces complexity.\n\n## L4LB\n\nThe internal layer 4 load balancer DCOS provides is used in the majority of our workloads. When our data scientists schedule a spark driver, they connect to the spark dispatcher through the Layer 4 load balancer. Most of the DCOS frameworks use this Layer 4 load balancer as an internal service discovery tool, with Vandebron we use this layer 4 load balancer to communicate between services. In a default DCOS set up this load balancer responds on domain names like: `spark-dispatcher.marathon.l4lb.thisdcos.directory:7077`\n\nWhen we ping the spark dispatcher we get the following:\n\n```bash\nPING spark-dispatcher.marathon.l4lb.thisdcos.directory (11.155.161.35) 56(84) bytes of data.\n64 bytes from 11.155.161.35 (11.155.161.35): icmp_seq=1 ttl=64 time=0.024 ms\n```\n\nAfter some investigation we found out that this IP range is not actually on a network interface, it is a Linux kernel functionality called `IPVS`. With IPVS you can do layer 4 load balancing, you provide the target location and the location you want to respond on.\n\nWhen we search for the IP from the spark dispatcher with ipvsadm, we get 3 results:\n\n```bash\nsudo ipvsadm -L -n |grep --color '11.155.161.35\\|$'\nTCP  11.155.161.35:80 wlc\n  -> 10.2.7.146:16827             Masq    1      0          0\nTCP  11.155.161.35:4040 wlc\n  -> 10.2.7.146:16826             Masq    1      0          0\nTCP  11.155.161.35:7077 wlc\n  -> 10.2.7.146:16825             Masq    1      0          0\n````\n\nAs you can see the IP `11.155.161.35` points towards `10.2.7.146`, even the ports are configured and forwarded. We can add our route with ipvsadm, to understand IPVS a bit better. For example:\n\n```bash\nsudo ipvsadm -A -t 1.2.3.4:80 -s wlc # we add the target server and assign the scheduler\nsudo ipvsadm -a -r 10.2.7.146:16825 -t 1.2.3.4:80 -m # we configure the real server and target server and configure Masquerading\ncurl 1.2.3.4:80\n{\n  \"action\" : \"ErrorResponse\",\n  \"message\" : \"Missing protocol version. Please submit requests through http://[host]:[port]/v1/submissions/...\",\n  \"serverSparkVersion\" : \"2.3.4\"\n}\n```\n\nThis results in that the spark dispatcher now also is available on `1.2.3.4:80`. As mentioned before we wanted to connect our DCOS and Kubernetes clusters, getting hundreds of entries from ipvsadm and manually adding them one by one didn’t sound appealing to us. Especially if you consider that sometimes services fail and run on a different port or different host after recovery, maintaining this by hand would be a nightmare. We therefore decided to build a tool to sync IPVS entries from DCOS to Kubernetes.\n\n## Stack\n\nWithin Vandebron we have our tech stack, we strongly believe it is good to eat your own dog food. When possible and when our use cases are similar we use the same tools as our Developers use. The parts of the stack we will be using are:\n\n- AWS ELB in front of Traefik 1.7\n- DCOS\n- Kubernetes\n\nWithin our platform team, we use Golang as our scripting language. Golang gives us the ability to build binary files with all the required libraries in the binary, we don’t have to install any packages, we do not even need to install Golang on the machine the application will be running on.\n\nIn our DCOS cluster we use Traefik 1.7, this version of Traefik only forwards HTTP requests. We decided to use Traefik to expose a JSON endpoint so we can gather the IPVS information from this location.\n\n## ipvs-server\n\nWithin our DCOS cluster we will expose the IPVS information through a JSON endpoint. We have built a tool for this to expose this information in multiple ways. In the next section, we are going to discuss some of the concepts and choices we made, we won’t deep dive into Go specifics. We have provided the entire code for this project in the examples directory of our GitHub repo:\n<https://github.com/Vandebron/tech-blog>\n\nFirst, let’s discuss the library we use: <https://github.com/nanobox-io/golang-lvs>. This library in its essence translates to ipvsadm commands, it helped save us time to implement this ourselves. There are some gotcha’s, such as newlines are not filtered out from the output. We solved this by cleaning up some of the data.\n\nIn the `childChan` function we create a go channel that is responsible for polling `ipvsadm` every 10 seconds and stores the result in a couple of variables we use in our HTTP endpoints. IPVS is a Linux kernel functionality and should be highly performant, we do not want to trigger kernel panics when the server gets overloaded with requests. We expect that every 10 seconds gives us accurate enough results, we can always lower this interval to ensure faster results. We also added in this function the string manipulation to ensure all the newlines were gone in the JSON output. The newline gave issues when we tried to add the IPVS scheduler entries.\n\n```go\nfunc childChan(c chan bool) {\n   fmt.Println(\"Starting time based IPVS Admin poll\")\n\n   pollInterval := 10\n   timerCh := time.Tick(time.Duration(pollInterval) * time.Second)\n   // Time based loop to generate Global variable\n   for range timerCh {\n       select {\n       // when shutdown is received we break\n       case <-c:\n           fmt.Println(\"Received shutdown, stopping timer\")\n           break\n       default:\n           var err error\n           listIpvs.Save()\n           ipvsString = fmt.Sprintln(listIpvs.Services)\n\n           res := &responseObject{\n               Services: listIpvs.Services,\n           }\n \n           ipvsJSONbyte, err := json.Marshal(res)\n           if err != nil {\n               logToErr.Printf(\"ERROR: -- Marshal JSON -- %v\\n\", err)\n           }\n \n           ipvsString = string(ipvsJSONbyte)\n           ipvsJSON = strings.Replace(ipvsString, `\\n`, ``, -1)\n           if debug != false {\n               logToOut.Println(\"DEBUG: -- ipvsJSON --\", ipvsJSON)\n           }\n       }\n   }\n}\n```\n\nNext is the index handler, we set our headers correctly and print the result as we would receive through ipvsadm. The index is mainly for our platform engineers to debug and verify the output. Thanks to this overview we found much faster that there was a newline hidden in the scheduler output.\n\n```go\nfunc index() http.Handler {\n   // Generating the Index\n   return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\n       // Only available when debug is on\n       if debug != false {\n           logToOut.Println(\"DEBUG: -- index --\", ipvsString)\n       }\n \n       if r.URL.Path != \"/\" {\n           http.Error(w, http.StatusText(http.StatusNotFound), http.StatusNotFound)\n           return\n       }\n       w.Header().Set(\"Content-Type\", \"text/plain; charset=utf-8\")\n       // Site security testers expect this header to be set\n       w.Header().Set(\"X-Content-Type-Options\", \"nosniff\")\n       w.WriteHeader(http.StatusOK)\n       fmt.Fprintln(w, ipvsString)\n   })\n}\n```\n\nThe JSON endpoint is what we use in the client communicate with the server. \n\n```go\nfunc jsonz() http.Handler {\n   // Generating the Index\n   return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\n       // Only available when debug is on\n       if debug != false {\n           logToOut.Println(\"DEBUG: -- jsonz --\", ipvsJSON)\n       }\n \n       if r.URL.Path != \"/json\" {\n           http.Error(w, http.StatusText(http.StatusNotFound), http.StatusNotFound)\n           return\n       }\n       w.Header().Set(\"Content-Type\", \"application/json; charset=utf-8\")\n       // Site security testers expect this header to be set\n       w.Header().Set(\"X-Content-Type-Options\", \"nosniff\")\n       w.WriteHeader(http.StatusOK)\n       fmt.Fprintln(w, ipvsJSON)\n   })\n}\n```\n\nWe ask our Developers often to implement a basic health endpoint, in DCOS we use this to see if a service needs to be restarted. In our application we enable set the statusOK in the index or in the JSON endpoint.\n\n```go\nfunc healthz() http.Handler {\n   // Generating the healthz endpoint\n   return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n       if atomic.LoadInt32(&healthy) == 1 {\n           w.WriteHeader(http.StatusNoContent)\n           return\n       }\n       w.WriteHeader(http.StatusServiceUnavailable)\n   })\n}\n```\n\nIn our logging and tracing functions we want to register the clients that are connecting, this gives us information where calls are coming from. It helps us debugging if we see weird behaviour.\n\n```go\nfunc tracing(nextRequestID func() string) func(http.Handler) http.Handler {\n   // Tracing the http requests so its easier to check if server is reached\n   return func(next http.Handler) http.Handler {\n       return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n           requestID := r.Header.Get(\"X-Request-Id\")\n           if requestID == \"\" {\n               requestID = nextRequestID()\n           }\n           ctx := context.WithValue(r.Context(), requestIDKey, requestID)\n           w.Header().Set(\"X-Request-Id\", requestID)\n           next.ServeHTTP(w, r.WithContext(ctx))\n       })\n   }\n}\n\nfunc logging(logToOut *log.Logger) func(http.Handler) http.Handler {\n   // Creating logging entry tracing the http requests\n   return func(next http.Handler) http.Handler {\n       return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n           defer func() {\n               requestID, ok := r.Context().Value(requestIDKey).(string)\n               if !ok {\n                   requestID = \"unknown\"\n               }\n               logToOut.Println(requestID, r.Method, r.URL.Path, r.RemoteAddr, r.UserAgent())\n           }()\n           next.ServeHTTP(w, r)\n       })\n   }\n}\n```\n\nIPVS needs to be executed with root privileges, to ensure this is correct we get the userid and print it when starting the server.\n\n```go\n// getProcessOwner function is to see who is running the process. It needs to be a sudo / root user\nfunc getProcessOwner() string {\n   stdout, err := exec.Command(\"ps\", \"-o\", \"user=\", \"-p\", strconv.Itoa(os.Getpid())).Output()\n   if err != nil {\n       logToErr.Printf(\"ERROR: -- getProcessOwner -- %v\\n\", err)\n       os.Exit(1)\n   }\n   return string(stdout)\n}\n```\n\nWe added the init function to ensure we print results the moment the server starts up, if we would not do this it would take 10 seconds for the go channel to activate\n\n```go\nfunc init() {\n   // Placing the Save and val in the init, else we will need to wait for channel to perform its first run\n   listIpvs.Save()\n   ipvsString = fmt.Sprintln(listIpvs.Services)\n}\n```\n\nIn the main function, we set the configurable flags, such as debugging to show error messages. It proved useful during the creation of this tool to keep track and print output. If we would print the output at every call to our logs, our Elastic cluster would get thousands of logs that add little to no value.\n\nWe configure the listen port in the flags, we can use the portIndex from DCOS to assign a random port on the host to listen on. We also provided to print the version we are running. In our versioning, we use a constant to list the application semver version, we also provide the git-commit hash.\nWhen we begin the server we print the version information, the port we listen on and the user running the process. We then start the server process with the go channel, in setting up the go channel we ensure that when the server stops we try to gracefully stop the server within a 30-second timeframe. Since our ipvsadm timer is 10 seconds it should be able to cleanly shutdown within that period.\n\n### Docker build\n\nIn the repository, we have included a Dockerfile and a script to build the Dockerfile. In this Dockerfile, we pass the git commit hash to the go install. This way we always get the Git Hash from our GitHub repo and we can use this information in our version output.\n\n### DCOS service.json\n\nIn the repository, we have provided the service.json file, since it is opinionated on using Traefik you might need to change it. But in this service.json you see how we set up Traefik, the health check, and port index. Since the Mesos UCR container has fewer abstractions and has fewer limited capabilities. We can run the IPVS server inside a UCR container and get all the output as if we were running this directly as root on the host machine.\n\n## ipvs-client\n\nThe IPVS client is the component we use in the Kubernetes environment. The client connects to the server and gets the IPVS entries from the IPVS server inside our DCOS cluster. It then adds these IPVS entries to each node in the Kubernetes cluster. You, therefore, need to run each client per Kubernetes node.\n\nYou can find the code from the IPVS client in our repository.\n\n```go\nfunc httpGet(remoteURL string) []byte {\n   if debug != false {\n       _, err := url.ParseRequestURI(remoteURL)\n       if err != nil {\n           panic(err)\n       }\n   }\n\n   req, err := http.NewRequest(http.MethodGet, remoteURL, nil)\n   if err != nil {\n       logToErr.Fatalf(\"ERROR: -- new HTTP request -- %v\", err)\n   }\n\n   ipvsClient := http.Client{\n       Timeout: time.Second * 2, // Timeout after 2 seconds\n   }\n   req.Header.Set(\"User-Agent\", \"go-ipvs-get \\tversion: \"+version+\"\\t Git Commit: \"+gitCommit)\n   res, err := ipvsClient.Do(req)\n   if err != nil {\n       logToErr.Fatalf(\"ERROR: -- ipvsClient -- %v\\n\", err)\n   }\n\n   if res.Body != nil {\n       defer res.Body.Close()\n   }\n\n   body, readErr := ioutil.ReadAll(res.Body)\n   if readErr != nil {\n       logToErr.Fatalf(\"ERROR: -- body -- %v\\n\", readErr)\n   }\n\n   return body\n}\n```\n\nIn the httpGet function we can debug the URL and check if it is valid. Again we set the correct headers and retrieve the JSON body.\n\n```go\nfunc unmarshal(body []byte) []lvs.Service {\n\n   res := &responseObject{\n       Services: listIpvs.Services,\n   }\n\n   jsonErr := json.Unmarshal(body, &res)\n   if jsonErr != nil {\n       logToErr.Fatalf(\"ERROR: -- Unmarshal -- %v \\n\", jsonErr)\n   }\n\n   if debug != false {\n       logToOut.Fatalf(\"DEBUG: -- res -- %v \\n\", res.Services)\n   }\n\n   r := res.Services\n\n   return r\n}\n```\n\nIn the unmarshal function we unmarshal the JSON and turn it in a slice of lvs.Service.\n\n```go\nfunc addServers(remoteAddr string) {\n   body := httpGet(remoteAddr)\n   jsonData := unmarshal(body)\n\n   for i, v := range jsonData {\n       if debug != false {\n           logToOut.Printf(\"DEBUG: -- range jsonDATA --\\n\")\n           logToOut.Printf(\"ipvsCount=%v, value=%v\", i, v)\n       }\n\n       err := lvs.DefaultIpvs.AddService(v)\n       if err != nil {\n           logToErr.Printf(\"ERROR: -- AddService -- %v\", err)\n       }\n \n       i++\n       ipvsServerCount = float64(i)\n   }\n}\n```\n\nIn the addServers function we add the servers to IPVS.\n\n```go\nfunc clientChan(c chan bool) {\n   logToOut.Println(\"Starting time based IPVS Admin add\")\n\n   pollInterval := 10\n   timerCh := time.Tick(time.Duration(pollInterval) * time.Second)\n   // Time based loop to generate Global variable\n   for range timerCh {\n       select {\n       // when shutdown is received we break\n       case <-c:\n           logToOut.Println(\"Received shutdown, stopping timer\")\n           break\n       default:\n\n           logToOut.Println(\"Clearing & Adding servers...\")\n           // Before we add Servers we need to clear the existing list\n           lvs.Clear()\n           addServers(remoteAddr)\n           if debug != false {\n               logToOut.Printf(\"IPVS servers added:\\t%v\", ipvsServerCount)\n           }\n       }\n   }\n}\n```\n\nLike we did in the IPVS server we create a go channel to poll every 10 seconds the server endpoint. We perform this to get at a set interval the IPVS entries.\n\nSince we run the IPVS client as a binary directly on the Kubernetes hosts we build the binary with a few parameters we pass to the go build command. The binary we build with this command we host on an internal s3 bucket, we can download this binary with systemd unit files.\n\n```bash\nGOOS=linux\nGOARCH=amd64\nGIT_COMMIT=$(git rev-list -1 HEAD)\n\nexport GOOS\nexport GOARCH\nexport GIT_COMMIT\n\nenv GOOS=${GOOS} GOARCH=${GOARCH} go build -v -ldflags \"-X main.gitCommit=${GIT_COMMIT}\" .\n```\n\nWhen we run the IPVS client we can verify if the IPVS routes are added by running the `ipvsadm -L -n` command.\n\n### Unit files\n\nSince IPVS is part of the Linux kernel it is hard to deploy this in a docker container, the capabilities are more restricted in Kubernetes. We decided to deploy the IPVS client on each host machine through a systemd unit file, the main reason was that we ran into restrictions that slowed us down and this is not a permanent solution. By adding the IPVS client on the machines alone does not make it possible for containers to use the IPVS routes. We needed to add NET_ADMIN capabilities to all containers using the l4lb loadbalancer locations and configure `hostNetworking: true` in the Kubernetes pods.\n\nWe provided a deployment.yml file that runs a Ubuntu docker container with ipvsadm only installed extra. When the pods are deployed in this deployment you can use kubectl exec to get into the pod and run the `ipvsadm -L -n` command.\n\n## Vacancy at Vandebron\n\nWe are looking for a platform engineer in Vandebron. As you can understand this is not a typical scenario we daily run across, but it is part of the workloads that we will support when working on our platform. Within Vandebron we try to use the best technology available, when it is not available we build it. Due to this as platform engineers, we have many interesting challenges and offer engineers to support further than only a strict domain. We support all components of our entire platform, regardless if it is a Linux kernel issue like this, involves setting up and maintaining a NoSQL cluster, or helping the business with something like requesting a certificate.\n\nIf you are interested in learning more about this position, take a look at our Vacancy and get in contact with us.\n<https://werkenbij.vandebron.nl/>\n","meta":{"title":"Migrating from DCOS to Kubernetes, dealing with the l4lb loadbalancer","description":"When you want minimal downtime, you need to build your own tools","createdAt":"Fri Mar 05 2021 01:00:00 GMT+0100 (Central European Standard Time)","coverImage":"images/migrating-dcos-kubernetes-l4lb.jpg","imageSource":"https://pixabay.com/users/praesentator-4372890/","tags":"Kubernetes, k8s, mesos, l4lb, ipvs, ipvsadm","author":"Rogier Dikkes","slug":"blog/migrating-dcos-kubernetes-l4lb","formattedDate":"March 5, 2021","date":"Fri Mar 05 2021 01:00:00 GMT+0100 (Central European Standard Time)"}},{"content":"\nCypress is a game-changer in the automation testing world, the way that Cypress was built and its architecture allows us as testers to cover more scenarios.\n\nCypress is not Selenium; in fact, it is different. And the way to build and design a framework should be different as well.\n\nThe most famous design technique in Selenium is the Page Object Model, and many testers use the same design technique with Cypress. Even that Cypress on their official website [recommended](https://www.cypress.io/blog/2019/01/03/stop-using-page-objects-and-start-using-app-actions/) us not to go with that approach.\n\n## Page Object Model\n\nThe main benefit of using the page object model Is to make the automation framework maintenance-friendly. We can define a specific page's selectors in a separate file and then use these selectors in our test cases.\n\n```js\nclass SignInPage {\n  visit() {\n    cy.visit(\"/signin\");\n  }\n  getEmailError() {\n    return cy.get(`[data-testid=SignInEmailError]`);\n  }\n  getPasswordError() {\n    return cy.get(`[data-testid=SignInPasswordError]`);\n  }\n  fillEmail(value) {\n    const field = cy.get(`[data-testid=SignInEmailField]`);\n    field.clear();\n    field.type(value);\n    return this;\n  }\n  fillPassword(value) {\n    const field = cy.get(`[data-testid=SignInPasswordField]`);\n    field.clear();\n    field.type(value);\n    return this;\n  }\n  submit() {\n    const button = cy.get(`[data-testid=SignInSubmitButton]`);\n    button.click();\n  }\n}\nexport default SignInPage;\n```\n\nThe main two downsides using the typical page object model with cypress are:\n\n- Page objects introduce an additional state into the tests, separate from the application’s internal state. This makes understanding the tests and failures harder.\n- Page objects make tests slow because they force the tests to always go through the application user interface.\n\n## Component-Based Architecture\n\nOn the other hand, a React application is component-based, where a specific page will be built from a collection of components. And components in React can be used on different pages too. So if we want to use the Page Object Model, we may define the same locator twice on different pages.\n\nSo having these two facts, At Vandebron, we came up with a new way to design our Cypress Automation framework by creating a separate JavaScript file for every component in our application, inside a folder called `components` within our Cypress project as below:\n\n```js\n// Locators\nexport const getEmailError = () => cy.get(`[data-testid=SignInEmailError]`);\nexport const getPasswordError = () =>\n  cy.get(`[data-testid=SignInPasswordError]`);\nexport const emailField = () => cy.get(`[data-testid=SignInEmailField]`);\nexport const passwordField = () => cy.get(`[data-testid=SignInPasswordField]`);\nexport const submitButton = () => cy.get(`[data-testid=SignInSubmitButton]`);\n\n// Actions\nexport const visit = () => cy.visit(\"/signin\");\nexport const performLogin = (email, password) => {\n  emailField().clear().type(email);\n  passwordField().clear().type(password);\n  submitButton().click();\n};\n```\n\nHaving it built this way, we eliminated all the previous problems mentioned earlier; we are not adding any classes, and we are defining objects within our test cases. And the most important part is that we are following the way that Cypress recommends it.\n\nAnd after defining the component locators and actions, we can import them inside our test case and use them as below:\n\n```js\nimport LoginComponent from \"../components/loginComponent\";\nimport Menu from \"../components/Menu\";\n\ndescribe(\"Test Login Page\", () => {\n  it(\"should show an error message if the password in wrong\", () => {\n    LoginComponent.visit();\n    LoginComponent.performLogin(\"email@gmail.com\", \"wrongPassword\");\n    LoginComponent.getPasswordError().should(\"be.visible\");\n  });\n  it(\"should show the logout button if the user logged in succesfully\", () => {\n    LoginComponent.visit();\n    LoginComponent.performLogin(\"email@gmail.com\", \"correctPassword\");\n    Menu.LogoutButton().should(\"be.visible\");\n  });\n});\n```\n\nAnd as you can see, our test cases are readable for anyone! And if any locator changes in any of the components, we can easily fix it in one location and from the same file. And lastly, if a component will be used in different places, we can use the same code.\n\nIn the next article, I will talk about how we use Cypress in our manual testing during the sprint and how it saves us tons of time and effort.\n","meta":{"title":"Cypress.io Component Design Technique for React Applications","description":"Cypress is a game-changer in the automation testing world, the way that Cypress was built and its architecture allows us as testers to cover more scenarios.","createdAt":"Fri Feb 05 2021 01:00:00 GMT+0100 (Central European Standard Time)","coverImage":"images/cypress-component-design-technique-for-react-applications.png","tags":"Cypress, Testing, React","author":"Hatem Hatamleh","slug":"blog/cypress-component-design-technique-for-react-applications","formattedDate":"February 5, 2021","date":"Fri Feb 05 2021 01:00:00 GMT+0100 (Central European Standard Time)"}},{"content":"\nIn Vandebron we have been using container clusters to host our services since the foundation of our Big Data team. \nRecently our cluster of choice has declared End-Of-Life development stage, so we decided to take a step forward and get a ticket for the Kubernetes boat.\n\nA change in the OS that is used to run your services and applications can look quite challenging and not everyone is on the same experience level. To make everyone comfortable it is a good choice to give everyone the possibility to play with the new tools and learn what can be done and how: **you need a sandbox.**\n\nOur developers are provided with a Macbook and at the moment of writing there some options you can go for when deciding how to set up your playground:\n\n- **Docker CE Kubernetes**: This is the easiest solution since there is a handy button to run your containers into a Kubernetes environment.\n\n- **Vagrant and Virtualbox**: This solution is the one that can give you more control and you can easily create a cluster the size you want, but you need to be handy with VMs, the hypervisor of choice, and Vagrant. It's the old school way to do it but, while it's a chunk your platform engineers can bite, it can be a steep and frustrating process for people that are not used to handle VMs.\n\n- **Multipass + some bash magic glue**: Since Canonical created this tool for macOS, creating an Ubuntu VM became a breeze and you can have a single, easily manageable VM with its networking up and running in less than a minute, without having to handle disks, distros, and stuff. On top of it, the command line interface is straight forward and it has just the basic commands we will need, so wrapping the entire process into a bash script is a piece of cake.\n\nI have found this super cool in-depth [article](https://jyeee.medium.com/kubernetes-on-your-macos-laptop-with-multipass-k3s-and-rancher-2-4-6e9cbf013f58) from Jason Yee (kudos to you bruh) that guided me through the installation of my first single node Kubernetes cluster.\n\nThe process is not that long but it involves a lot of copy/pasting and, once learned the basics, I didn't want to go under the same process more times, plus it could be interesting for me as a Platform Engineer, but it may be boring and pointless for developers who just want to have a sandbox replica of what they are working on in the remote environment.\nMy automator (aka do-it-once-never-do-it-again) spirit kicked in and I decided to wrap every step in a small command-line tool with only 3 options:\n- **install**\n- **cleanup**\n- **help**\n\n\n### What is happening under the hood\n\nWhat the script does is substantially automating all the steps needed to:\n1. Create a new VM using Multipass (tool released by Canonical)\n2. Fetch the VM IP address and adding it to your local `/etc/hosts` file\n3. Install k3s (a lightweight distribution of Kubernetes) on top of the VM\n4. Install the Kubernetes command-line tools on your laptop\n5. Install Helm (the Kubernetes package manager) on your laptop\n6. Install cert-manager (certificate manager) package on top of your k3s cluster\n7. Install Rancher (a Kubernetes control plane) package on top of your k3s cluster\n\nIf you are looking for a more in-depth breakdown of the single steps you can download and inspect [the script](https://gist.githubusercontent.com/nikotrone/50b1a5f8d137411879eb2467e689bfbe/raw/090b4b4323d96ac28d96bbb346e2e657073722e6/bronernetes) (one of the many advantages of [OpenSource](https://en.wikipedia.org/wiki/Open_source) projects) or checkout and read the original [article](https://jyeee.medium.com/kubernetes-on-your-macos-laptop-with-multipass-k3s-and-rancher-2-4-6e9cbf013f58): it explains line by line what the specific commands are doing.\n\n#### 1. Multipass VM\n[Multipass](https://multipass.run/) is a tool from Canonical (the company developing and maintaining the Ubuntu Linux distribution) that leverages Hyperkit (macOS feature to handle virtualization) to create and handle a Virtual Machine directly on your Mac.\n\n#### 2. Edit /etc/hosts\nOnce we have our VM up and running we need to make it available with an easy url that is also gonna be used to generate the SSL certificate, in our case we picked up `rancher.localdev`.\nIt is important to have a name setup in the beginning since this one will need to match with the certificate so we can use it programmatically.\n\n#### 3. Install K3S\nThis step is pretty straightforward: just fetch a script that is publicly available on the [k3s official website](https://get.k3s.io) and feed it to your bash.\nK3s is a lightweight version of Kubernetes with all the needed dependencies and executable packaged in a convenient installation script. Because of its light nature, it is often used in embedded devices that have a limited amount of resources to offer.\n\n#### 4 & 5. Kubernetes and Helm cli\n**Kubernetes cli** (`kubectl`) is used to talk and interact with your Kubernetes cluster. It can be used to manage multiple clusters according to the content of your KUBECONFIG environment variable. \nThe variable itself contains just a path to where your cluster configuration is stored, so you can switch from a cluster to another by simply pointing to another file that contains the configuration of another cluster.\n\n**Helm** instead is the \"package manager\" of Kubernetes: you can use it to add repositories to specific `charts` which are the blueprint that contains a way to install a specific tool on your cluster.\nBoth of these tools have to be installed and run from your local laptop, either in the case you are managing a local VM or in the case you are interacting with a remote cluster.\n\n#### 6 & 7. cert-manager and Rancher\n\n**Rancher** is the control plane for our cluster: it provides a GUI and an overview of our single node cluster. It offers other goodies like management of multiple clusters, deployed on different locations like AWS Azure and GCP or even on your own hardware, plus certificate deployment and some other handy functionalities.\n\n**cert-manager** is installed via Helm chart and it is the tool used by Rancher to generate and deploy a certificate across the entire cluster.\n\n### How to use it\n\nAll the steps will involve the use of a Terminal window\n#### Installation\nThe first thing you need to do is download [this script](https://gist.github.com/nikotrone/50b1a5f8d137411879eb2467e689bfbe) and save it in a folder on your Mac (let's assume `~/bronernetes`) by executing\n```bash\n    mkdir ~/bronernetes\n    cd ~/bronernetes\n    curl https://gist.githubusercontent.com/nikotrone/50b1a5f8d137411879eb2467e689bfbe/raw/090b4b4323d96ac28d96bbb346e2e657073722e6/bronernetes > bronernetes\n    export PATH=$PATH:$(pwd)\n```\n\nNow we have the toolset and you can confirm it works by simply running `bronernetes help`.\n\n#### Spin up Kubernetes\nThe next step is to run the installation process with the command `bronernetes install`\n\n#### Clean up\nWhen you are done or you just want to hard reset your environment you can just type `bronernetes cleanup` and it will take care of cleaning up the VM you just used, leaving you with a pristine machine, as nothing ever happened :)\n\n### Conclusion\n\nHaving a sandbox is very useful to play around with the concepts of a new setup or service and it packs up a huge amount of positive sides. No matter what is the language or the nature of the system you are trying to replicate, it can be challenging and involve a long list of instructions or manual operations and, sometimes, even dedicated hardware. Although with some bash glue, it is possible to automate most of those processes and the investment cost can be enormously beneficial for yourself (less work the next time you do it) and for the other people working with you (they can use the tool, comment and suggest improvements). Most of all, in the case of infrastructure, it helps raise the knowledge of \"what's going on here\" and documents for the ones interested in taking a trip down the rabbit hole.\n","meta":{"title":"How to Spin Up A Kubernetes Cluster On Your Macbook","description":"It is can be useful to create a disposable Kubernetes sandbox to play with when you are exploring a new application and how it could work.","createdAt":"Mon Jan 25 2021 01:00:00 GMT+0100 (Central European Standard Time)","coverImage":"images/spin-up-kubernetes-on-macbook.jpg","imageSource":"https://pixabay.com/it/users/mari_sparrow-13090456/","tags":"Kubernetes, k8s, local","author":"Marco Nicotra","slug":"blog/spin-up-kubernetes-on-macbook","formattedDate":"January 25, 2021","date":"Mon Jan 25 2021 01:00:00 GMT+0100 (Central European Standard Time)"}},{"content":"\nAt Vandebron we're maintaining a component library called [Windmolen](https://windmolen.netlify.app/) (Dutch for \"wind turbine\"). And if you've ever built a component library, you probably dealt with optimizing and converting icons before. With SVGO and SVGR you can do this at scale, without compromising the quality or size of your icons.\n\n## The problem\n\nThe web is full of icons, and often these icons are rendered from SVG files to ensure you can increase (or decrease) the size of the icons depending on the use case. Designers often create these icons from design tools like Adobe Photoshop or Sketch. Although these icons might look pretty, exporting a SVG out of these tools is often difficult as [this article](https://medium.com/sketch-app-sources/the-best-way-to-export-an-svg-from-sketch-dd8c66bb6ef2) explains. Also, added lot of code in the form of metadata is added to the SVG file. Let's have a look at what a typical SVG file exported out of Sketch looks like:\n\n```svg\n<!-- something.svg -->\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<svg width=\"14px\" height=\"14px\" viewBox=\"0 0 14 14\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n    <!-- Generator: Sketch 46 (44423) - http://www.bohemiancoding.com/sketch -->\n    <title>last</title>\n    <desc>Created with Sketch.</desc>\n    <defs></defs>\n    <g id=\"Page-1\" stroke=\"none\" stroke-width=\"1\" fill=\"none\" fill-rule=\"evenodd\">\n        <g id=\"last\" transform=\"translate(2.000000, 0.000000)\" fill-rule=\"nonzero\" fill=\"#666666\">\n            <polygon id=\"Fill-2\" points=\"6.6902923 9.6812703 9.3700469 7.0005052 6.6902923 4.3187297 2.37257308 0 0 2.37358354 4.3177192 6.6902923 4.6279322 7.0005052 4.3177192 7.3107182 0 11.6274269 2.37257308 14\"></polygon>\n        </g>\n    </g>\n</svg>\n```\n\nThe SVG file above holds a lot of information about Sketch, such as the `title` of the icon and a `desc`ription. Next to that, there's a lot of elements that could be combined into one element to reduce the file size.\n\n## Optimizing SVGs\n\nWhat's cool about SVG files is that you can optimize and minify them, without affecting what the SVG looks like. This is something you can try out yourself using the website [SVGOMG](https://jakearchibald.github.io/svgomg/), which is powered by the library SVGO that you'll learn more about later.\n\n\nYou can optimize the SVG file above by following these steps:\n\n1. Go to [https://jakearchibald.github.io/svgomg/](https://jakearchibald.github.io/svgomg/)\n2. Click on `Paste markup` an paste the SVG code that you exported from Sketch (a.k.a. the SVG file above)\n3. You will see the icon rendered, now you have to either click at the `Copy as a text` or `Download` button to get the optimized SVG file\n\nWith these simple steps you've optimized the SVG from over 450 bytes, which is already small, to 173 bytes (a decrease of over 62%!). If you'd open this file in the editor of your choice, you can see a lot of the useless (meta)data from the original file has been deleted. Also, the different elements of the SVG are combined in a single `path` that renders the icon:\n\n```svg\n<!-- something.svg -->\n<svg width=\"14\" height=\"14\" xmlns=\"http://www.w3.org/2000/svg\">\n  <path d=\"M8.69 9.681l2.68-2.68-2.68-2.682L4.373 0 2 2.374 6.318 6.69l.31.31-.31.31L2 11.628 4.373 14z\" fill-rule=\"nonzero\" fill=\"#666\"/>\n</svg>\n```\n\nThis SVG can be even further optimized by checking the \"Prefer viewbox to width/height\" in SVGOMG, but let's save that for later when we use SVGO instead.\n\n## Using SVGO\n\nBy using SVGOMG you've already experienced what power [SVGO](https://github.com/svg/svgo) has, as SVGOMG is described by its creators as *\" SVGO's Missing GUI, aiming to expose the majority if not all the configuration options of SVGO\"*. Instead of using the GUI, you can also use SVGO directly from the command line as a CLI-tool or as a Node.js module. For the sake of this article, we'll be using it solely as CLI.\n\nSVGO can be installed globally on your machine, or locally in your project, from npm by running:\n\n```bash\nnpm i -g svgo\n\n# Yarn equivalent\nyarn add -G svgo\n```\n\nAfter doing this you can run `svgo` from the command line and optimize any SVG file instantly. But, you don't want to do this manually on your machine anytime you're adding a new icon to a project (or component library). Therefore, you can also add SVGO to a project locally and add a script to the `package.json` file to optimize all SVGs in a certain directory.\n\n```json\n// package.json\n{\n // ...\n \"scripts\": {\n     // ...\n    \"optimize-svg\": \"svgo --config=.svgo.yml -f ./src/assets/icons\"\n }\n}\n```\n\nThe `optimize-svg` script will run SVGO in the directory `src/assets/icons` and optimize all the SVG files based on the settings in `.svgo.yml`. This file is where you can configure the rules for SVGO, as the previously mentioned \"Prefer viewbox to width/height\":\n\n```yaml\n# .svgo.yml\nplugins:\n  - removeViewBox: false\n  - removeDimensions: true # this deletes width/height and adds it to the viewBox\n  - removeDoctype: true\n  - removeComments: true\n  - removeMetadata: true\n  - removeEditorsNSData: true\n  - cleanupIDs: true\n  - removeRasterImages: true\n  - removeUselessDefs: true\n  - removeUnknownsAndDefaults: true\n  - removeUselessStrokeAndFill: true\n  - removeHiddenElems: true\n  - removeEmptyText: true\n  - removeEmptyAttrs: true\n  - removeEmptyContainers: true\n  - removeUnusedNS: true\n  - removeDesc: true\n  - prefixIds: false\n  - prefixClassNames: false\n```\n   \nFrom the rules above you'll get an idea about all the redundant and useless lines of code that might be present in your SVG files. But luckily, they will all get removed when you run the command `npm run optimize-svg`.\n\n## Converting SVGs with SVGR\n\nYou've now learned how to optimize your SVG files, and are probably wondering how to use these files in a React application. To render an SVG in React, you need to either configure Webpack in a way that it knows how to deal with SVG files or use a library called SVGR. By default, any application created with `create-react-app` can render SVG files as a component, using the following `import` statement:\n\n```jsx\n// MyComponent.jsx\nimport React from 'react';\nimport { ReactComponent as MySVG } from './something.svg';\n\nconst MyComponent = () => {\n  return (\n    <div>\n      <MySVG />\n    </div>\n  );\n}\nexport default MyComponent;\n```\n\nMore information about how this is done can be found in [this article](https://blog.logrocket.com/how-to-use-svgs-in-react/), but let me show you how to solve that with SVGR.\n\nWith [SVGR](https://react-svgr.com/) you can convert SVG files into React Components, either by adding it to Webpack or by using the SVGR CLI or Node.js module. In the same way, as we optimized the SVGs from the command line with SVGO, we can also convert these icons from the command line with SVGR:\n\n```json\n// package.json\n{\n // ...\n \"scripts\": {\n     // ...\n    \"optimize-svg\": \"svgo --config=.svgo.yml -f ./src/assets/icons\",\n    \"convert-svg\": \"svgr -d ./src/components/Icon ./src/assets/icons\"\n }\n}\n```\n\nWhenever you run the command `npm run convert-svg` a JSX file will be created for every SVG file that's present in the directory `src/assets/icons`. These JSX files can be found in the directory `src/components/Icons`, together with an `index.js` file that exports all these components from this directory.\n\nAn example of such a converted SVG file is:\n\n\n```jsx\n// MySVG.jsx\nimport * as React from 'react';\n\nconst MySVG = (props) => (\n  <svg viewBox=\"0 0 14 14\" xmlns=\"http://www.w3.org/2000/svg\" {...props}>\n  <path d=\"M8.69 9.681l2.68-2.68-2.68-2.682L4.373 0 2 2.374 6.318 6.69l.31.31-.31.31L2 11.628 4.373 14z\" fill-rule=\"nonzero\" fill=\"#666\"/>\n  </svg>\n);\n\nexport default MySVG;\n```\n\nAnd, as we now have a directory filled with converted SVGs these can be imported into any React component like this:\n\n```jsx\n// MyComponent.jsx\nimport React from 'react';\nimport MySVG from './MySVG.jsx';\n\nconst MyComponent = () => {\n  return (\n    <div>\n      <MySVG />\n    </div>\n  );\n}\nexport default MyComponent;\n```\n\nOften SVGR is used alongside SVGO, so you can even automatically optimize all SVGS that will be converted by SVGR. This is done by adding the flag `--no-svgo true` and point it towards your SVGO configuration file:\n\n```json\n// package.json\n{\n // ...\n \"scripts\": {\n     // ...\n    \"convert-svg\": \"svgr -d ./src/components/Icon ./src/assets/icons --no-svgo true --svgo-config .svgo.yml\"\n }\n}\n```\n\nBy running the `convert-svg` script you both optimize and convert all the SVG files in `src/assets/icons` to React components based on optimized SVGs.\n\n## Reading further\n\nThe examples in this post are the tip of the metaphorical iceberg on what problems SVGO and SVGR can solve. There are many other features you can enable, such as using them as Node.js modules or enabling TypeScript support. To read further make sure to have a look at the SVGR [playground](https://react-svgr.com/playground/) or [documentation](https://react-svgr.com/docs/getting-started/).\n","meta":{"title":"Optimizing, Converting And Exporting SVG Icons In React","description":"If you've ever build a component library, you probably dealt with optimizing and converting icons before. With SVGO and SVGR you can do this at scale.","createdAt":"Thu Dec 10 2020 01:00:00 GMT+0100 (Central European Standard Time)","coverImage":"images/optimizing-converting-and-exporting-svg-icons-in-react.jpg","tags":"React, component library","author":"Roy Derks","slug":"blog/optimizing-converting-and-exporting-svg-icons-in-react","formattedDate":"December 10, 2020","date":"Thu Dec 10 2020 01:00:00 GMT+0100 (Central European Standard Time)"}},{"content":"\nHere at Vandebron, we have several projects which need to compute large amounts of data. To achieve acceptable results, we had to choose a computing tool that should have helped us to build such algorithms.\n\nAs you may have read in other articles our main backend language is Scala so the natural choice to build distributed parallel algorithms was indeed Spark.\n\n## What is Spark\n\nWe will briefly introduce Spark in the next few lines and then we will dive deep into some of its key concepts.\n\nSpark is an ETL distributed tool. ETL are three phases that describe a general procedure for moving data from a source to a destination.\n\n![ETL Diagram](/images/etlprocess.png \"ETL\")\n\n- **_Extract_** is the act of retrieving data from a data source which could be a database or a file system.\n- **_Transform_** is the core part of an algorithm. As you may know, functional programming is all about transformation. Whenever you write a block of code in Scala you go from an initial data structure to a resulting data structure, the same goes with Spark but the data structures you use are specific Spark structures we will describe later.\n- **_Load_** is the final part. Here you need to save (load) the resulting data structure from the transformation phase to a data source. This can either be the same as the extract phase or a different one.\n- **_Distributed_**: Spark is meant to be run in a cluster of nodes. Each node runs its own JVM and every Spark data structure can/should be distributed among all the nodes of the cluster (using serialization) to parallelize the computation.\n\n### Spark data structure: RDD, DataFrame, and Dataset\n\nThe core of Spark is its _distributed resilient dataset (RDD)_.\n\n![Spark API history](/images/sparkapihistory.png \"Spark API history\")\n\nAn **_RDD_** is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. _Extracting_ data from a source creates an RDD. Operating on the RDD allows us to _transform_ the data. Writing the RDD _loads_ the data into the end target like a database for example). They are made to be distributed over the cluster to parallelize the computation.\n\nA **_DataFrame_** is an abstraction on top of an RDD. It is the first attempt of Spark (2013) to organize the data inside and RDD with an SQL-like structure. With dataframe, you can actually make a transformation in an SQL fashion. Every element in a dataframe is a Row and you can actually transform a dataframe to another by adding or removing columns.\n\nA **_DataSet_** finally is a further abstraction on top of a dataframe to organize data in an OO fashion (2015). Every element in a dataset is a case class and you can operate transformation in a scala fashion from a case class to another.\n\n## Spark in action\n\nLet’s see now some code samples from our codebase to illustrate in more detail each of the ETL phases.\n\n### Extract\n\nThe extraction phase is the first step in which you gather the data from a datasource.\n\n```scala\nval allConnections = sparkSession\n.read\n.jdbc(connectionString, tableName, props)\n\nval selectedConnections = allConnections\n.select(ColumnNames.head, ColumnNames.tail: _*)\n\nval p4Connections = selectedConnections\n.filter(allConnections(\"HasP4Day activated\").equalTo(1))\n.filter(allConnections(\"HasP4INT activated\").equalTo(1))\n.as[Connection]\n\np4Connections.show()\n```\n\nFor most people the extraction phase is just the first line (the invocation to the read method), they are not wrong because extracting means reading data from a datasource (in this case an SQL server database). I decided to include in this phase also some filtering and projection operations because I think these are not really part of the algorithm, this is still the preparation phase before you actually process the data. We can ultimately say that _preparing the data_ is something in between extraction and transformation therefore it is up to you to decide which phase it belongs to.\n\n### Transform\n\nTransformation phase is the core of the algorithm. Here you actually process your data to reach your final result.\n\n```java scala\nusageDF\n.groupBy('ConnectionId, window('ReadingDate, \"1 day\"))\n.agg(\n    sum('Consumption).as(\"Consumption\"),\n    sum('OffPeak_consumption).as(\"OffPeak_consumption\"),\n    sum('Peak_consumption).as(\"Peak_consumption\"),\n    sum('Production).as(\"Production\"),\n    sum('OffPeak_production).as(\"OffPeak_production\"),\n    sum('Peak_production).as(\"Peak_production\"),\n    first('ReadingDate).as(\"ReadingDate\"),\n    first('marketsegment).as(\"marketsegment\"),\n    collect_set('Source).as(\"Sources\"),\n    collect_set('Tag).as(\"Tags\"),\n    max('Last_modified).as(\"Last_modified\")\n)\n.withColumn(\n    \"Tag\", when(array_contains('Tags, “Interpolated”),\nlit(Tag.Interpolated.toString)).otherwise(lit(“Measured”)))\n.withColumn(\"Source\",\nwhen(size('Sources) > 1,\nlit(Source.Multiple.toString)).otherwise(mkString('Sources)))\n.orderBy('ConnectionId, 'ReadingDate)\n.drop(\"window\", \"sources\", \"tags\")\n```\n\nIn this specific example, we are processing connection usage data by aggregating it daily. In the `usageDF` we have 15 minutes interval usage data, now we want to show to the user the same data but with a different aggregation interval (1 day). So we group the whole data by connection id and window the reading date by 1 day (A window function calculates a return value for every input row of a table based on a group of rows [Introducing Window Functions in Spark SQL - The Databricks Blog](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html).\n\nOnce the data is grouped we can aggregate it, using the `agg` method which allows us to call the aggregation functions over the dataframe (for example: `sum`, `first`,`max` or `collect_set`). Successively we transform the dataframe to suit our visualization needs, the methods used are self-explanatory and the documentation is very clear. [Getting Started - Spark 3.0.1 Documentation](https://spark.apache.org/docs/latest/sql-getting-started.html)\n\n### Load\n\nThe final phase is the one which `save`, `put`, `show` the transformed data into the target data source.\n\n```java scala\ndataFrame\n.select(columns.head, columns.tail: _*)\n.write\n.cassandraFormat(tableName, keySpace)\n.mode(saveMode)\n.save()\n```\n\nIn this specific case, we will save our dataframe into a Cassandra database. In Spark, methods used to achieve the load phase are called _actions_. It is very important to distinguish Spark actions from the rest because actions are the only ones that trigger Spark to actually perform the whole transformation chain you have defined previously.\n\nIf our transformation phase, as we described above, wasn’t followed by an action (for example `save`) nothing would have happened, the software would have simply terminated without doing anything.\n\n## One concept to rule them all\n\n```java scala\nval rdd1 = sc.parallelize(1 to 10)\nval rdd2 = sc.parallelize(11 to 20)\nval rdd2Count = rdd1.map(\nx => rdd2.values.count() * x //This will NEVER work!!!!\n)\n```\n\n_One does not simply use RDD inside another RDD_. (Same goes for Dataframes or Datasets).\n\nThis is a very simple concept that leads very often to lots of questions because many people just want to use Spark as a normal scala library. But this is not possible due to the inner distributed nature of Spark and its data structures. We have said that an RDD is a resilient distributed dataset, let’s focus on the word _distributed_, it means that the data inside it is spread across the nodes of the cluster. Every node has its own JVM and it is called _Executor_, except for the master node where your program starts which is called _Driver_:\n\n![Spark cluster overview](/images/spark-cluster-overview.png \"Spark cluster overview\")\n\nYour code starts from the Driver and a copy is distributed to all executors, this also means that each executor needs to have the same working environment of the Driver, for Scala it is not a problem since it just needs a JVM to run. (but we will see that if you use _pySpark_ you need to take extra care when you distribute your application.) Every Spark data structure you have defined in your code will also be distributed across the executors and every time you perform a transformation it will be performed to each chunk of data in each executor.\n\nNow let’s go back to our example, a `map` is a transformation on `rdd1` this means that block inside will be executed at the executor level, if we need `rdd2` to perform this block Spark should somehow serialize the whole `rdd2` and send it to each executor. You can understand now that _it is really not possible to serialize the whole RDD since it is by its nature already a distributed data structure_. So what can you do to actually perform such computation we showed in the example? The solution is “simple”: _prepare your data in such a way that it will be contained in one single RDD_. To do so you can take advantage of all the transformation functions Spark has to offer such `map` `join` `union` `reduce` etc.\n\n## Next step…\n\nWe have explained all the main concepts of Spark and we have shown some real snippets of our codebase. In the next article, I would like to show you a real-life problem we have solved in our company using [_pySpark_](https://spark.apache.org/docs/latest/api/python/index.html). I will show you how to customize Spark infrastructure to correctly parallelize the ETL algorithm you have built.\n","meta":{"title":"Fueling the Energy Transition With Spark - Part 1","description":"Our main backend language is Scala, and by using Spark we build distributed parallel algorithms to fuel the Energy Transition. But why is Spark the best choice for that job?","createdAt":"Wed Nov 04 2020 01:00:00 GMT+0100 (Central European Standard Time)","coverImage":"images/fueling-the-energy-transition-with-spark-part-1.jpg","imageSource":"https://www.pexels.com/photo/shallow-focus-photography-of-light-bulbs-2764942","tags":"spark, scala","author":"Rosario Renga","slug":"blog/fueling-the-energy-transition-with-spark-part-1","formattedDate":"November 4, 2020","date":"Wed Nov 04 2020 01:00:00 GMT+0100 (Central European Standard Time)"}},{"content":"\nAt Vandebron we organize a two-day long Hackathon every quarter, and a colleague and I took this chance to dig into the wonderful world of GraalVM.\n\nI've first heard of GraalVM around two years ago when Oleg Šelajev toured through Java User Groups in Germany and held talks about GraalVM. [Here](https://www.youtube.com/watch?v=GinNxS3OSi0) is one from 2019 (not Germany, but Spain this time).\n\nGraalVM promises a significant speedup in compile times and as I am working with Scala, which is notoriously known for its long compile times, this seems interesting. Furthermore, GraalVM provides functionality to build native executables. Meaning, an application can be run without a Java Virtual Machine (JVM).\n  \nThanks to the Hackathon I finally took the time to get to know GraalVM a bit better. With this blog post, I want to share our findings, experiences, and results, as they might be helpful for you too!\n\n## What is GraalVM?\n\nGraalVM is a high-performance JVM that supports efficient ahead-of-time (AOT) and just-in-time (JIT) compilation, but also allows non-JVM languages (e.g. Ruby, Python, C++) to run on the JVM. The ahead-of-time compilation feature is the base for creating native executable programs, meaning an application can be run independently from the JVM. Seeing the versatile features of GraalVM, it is worth looking a bit under its hood.\n\nActually, GraalVM is defined by three main technologies:\n\n- [Graal compiler](https://www.graalvm.org/reference-manual/jvm/), a high-performance JIT-compiler that can make JVM applications run faster from within the JVM\n- [SubstrateVM](https://www.graalvm.org/reference-manual/native-image/SubstrateVM/), includes the necessary components to run a JVM-app as a native executable ( Garbage Collector, Thread Scheduler, etc.)\n- [Truffle Language Implementation Framework](https://www.graalvm.org/graalvm-as-a-platform/language-implementation-framework/), the basis for the polyglot support from GraalVM\n\nOur motivation for trying out GraalVM was tackling the pain points of Scala, Java projects, and microservices. Shipping microservices written in Scala as Docker containers to your production system comes with the cost that startup can be a bit slow, having JVM and Docker overhead, and that those containers can be fairly large, as the application can only be run with a JVM. See [Building Docker images](#building-docker-images) for more information.\n\nDuring the hackathon, we were most interested in building native images for Scala applications. Hoping to reduce the size of our docker containers and reducing up the startup time.\n\n## Project setup\n\nThe project we worked on during the Hackathon is an API that should be used for applicants to submit their applications at Vandebron in the future. By exposing one endpoint through which a resume and contact information can be submitted.\n\nIt is also a good project to test out GraalVM, nothing too complex but also not as simple as \"Hello World\".\n\nThe full setup can be found [on Github](https://github.com/kgrunert/apply-at-vdb). But I'll summarise the used stack below. The project is built around the following libraries, no particular reason, simply because I like them.\n\n- _cats_ for working with effects, such as IO\n- _http4s_ for running the server\n- _tapir_ for defining the endpoints\n- _circe_ for JSON de/serialisation\n- _pureconfig_ for reading config-files\n- _logback_ for logging\n\nThe project can be run via `sbt run` and with Postman or similar a POST-request can be sent like so:\n\n```json\nPOST localhost:8080/api/v1/apply\n\n{\n\t\"email\": \"my@email.de\",\n\t\"name\": \"My Name\",\n\t\"phoneNumber\": \"+310123456789\",\n\t\"applicationBase64\": \"VGhpcyBjb3VsZCBiZSB5b3VyIGFwcGxpY2F0aW9uIQ==\"\n}\n\nResponse:\n\"*confetti* Thanks for handing in your application, we will get back to you within the next days! *confetti*\"\n```\n\n## Setup GraalVM with sbt\n\nWith this initial project setup in mind, GraalVM needs to be installed locally.\n\nFor the installation of GraalVM the [setup guide](https://www.graalvm.org/docs/getting-started-with-graalvm/#install-graalvm) can be followed.\n\nAfter the installation sbt needs to know that not the regular JDK/JVM is used. This can be done with the `java-home` option on sbt bootup.\nTo make the path to GraalVM a bit more accessible and easy to use it can be exported as an environment variable.\n\n```bash\nexport GRAAL_HOME=/Library/Java/JavaVirtualMachines/graalvm-ce-java8-20.1.0/Contents/Home\nsbt -java-home $GRAALHOME\n```\n\nThe path to GraalVM can vary depending on OS and installation. We followed the basic installation for macOS.\n\nNow sbt using GraalVM can be verified with:\n\n```bash\nsbt -java-home $GRAALHOME\nscala> eval System.getProperty(\"java.home\")\n[info] ans: String = /Library/Java/JavaVirtualMachines/graalvm-ce-java8-20.1.0/Contents/Home/jre\n```\n\nThat means everything running in this sbt instance is getting compiled by GraalVM. Awesome!\n\nThe next step is to become strong and independent and learn how to run without an underlying JVM with the help of building native images.\n\n## Building native images\n\nGraalVM ships with the [GraalVM Updater](https://www.graalvm.org/reference-manual/graalvm-updater/) (`gu`) to install the `native-image` on your machine.\n\n```bash\n$GRAALHOME/bin/gu install native-image\n```\n\n[sbt-native-packager](https://sbt-native-packager.readthedocs.io/en/latest/) provides functionality to build packages efficiently (e.g. building Docker images) and added to that, it also provides support for building native images.\nIn order to build native images with sbt commands this plugin has to be added to the project:\n\n```java scala\n// inside project/plugins.sbt\naddSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.7.3\")\n```\n\nAnd the `GraalVMNativeImagePlugin` needs to be enabled:\n\n```java scala\n// inside build.sbt\nenablePlugins(GraalVMNativeImagePlugin)\n```\n\nFrom within sbt it should be able to autocomplete and suggest graal-commands, e.g.:\n\n```java scala\nsbt:apply-at-vdb> graalvm\ngraalvm-native-image:       graalvmNativeImageOptions\n```\n\nWith that setup, native images are just a stone's throw away!\n\n---\n\n### Disclaimer\n\nThe next three sections are not a write-up but rather the main steps we had to take to make the project work. This includes failing images and troubleshooting.\nI want to keep this in because it might be interesting for others when they have to troubleshoot.\nFor the summary and happy path, you can jump directly to [Roundup](#roundup).\n\n---\n\n### First try building a native image\n\nNext up `graalvm-native-image:packageBin` can be run from within sbt. This might take a while (on our systems it took about a minute)\n\nSome warnings start to pop up:\n\n```\n[error] warning: unknown locality of class Lnl/vandebron/applyatvdb/Main$anon$exportedReader$macro$24$1;, assuming class is not local. To remove the warning report an issue to the library or language author. The issue is caused by Lnl/vandebron/applyatvdb/Main$anon$exportedReader$macro$24$1; which is not following the naming convention.\n\n[error] warning: unknown locality of class Lfs2/internal/Algebra$Done$2$;, assuming class is not local. To remove the warning report an issue to the library or language author. The issue is caused by Lfs2/internal/Algebra$Done$2$; which is not following the naming convention.\n```\n\nThe library-specific warnings can be ignored for now. Ultimately it fails with:\n\n```\nError: com.oracle.graal.pointsto.constraints.UnresolvedElementException:\nDiscovered unresolved type during parsing: org.slf4j.impl.StaticLoggerBinder.\nTo diagnose the issue you can use the --allow-incomplete-classpath option.\nThe missing type is then reported at run time when it is accessed the first time.\n```\nActually a good hint on where to start fine-tuning the GraalVM config:\n\n```java scala\n// inside build.sbt\ngraalVMNativeImageOptions ++= Seq(\n\t\"--allow-incomplete-classpath\",\n)\n```\n\nSome things like a `StaticLoggerBinder` only get resolved at runtime, meaning at build time the classpath needs to be allowed to be incomplete. This option allows resolution errors to be ignored at build time and only pop up during runtime.\n\nDuring the build of a native image, GraalVM tries to resolve those runtime dependencies already at compile-time, as it is part of the Ahead-Of-Time-compilation process. With this flag, GraalVM knows \"hey, don't worry about it now, we cross the bridge when we get there\" (or something like that).\n\n### Adding resource files\n\nA `reload` (or restart) of sbt is needed to activate these new options. And we can try to build the native image up new.\nThis time the build finished successfully and the executable file `target/graalvm-native-image/apply-at-vdb` has been created!\nThis is an executable that can be run without a JVM:\n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n```\n\nBut what's that? It actually cannot be started...\n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n*** An error occured! ***\nCannot convert configuration to a de.erewl.pricetracker.server.Config. Failures are:\nat the root:\n- Key not found: 'host'.\n- Key not found: 'port'.\n```\n\nThe first three lines relate to the error that occurred during the first build. It simply says that logging hasn't been set up correctly (maybe due to the absence of a `src/main/resources/logback.xml` or some other misconfiguration), triggering the default setting of not logging anything at all.\nThe second error states that a configuration file does not have the right keys or cannot be found at all.\nLooking into `src/main/resources`:\n\n```bash\nls src/main/resources/\napplication.conf logback.xml\n```\n\nand peeking into `application.conf`:\n\n```bash\ncat src/main/resources/application.conf\n\thost = \"localhost\"\n\tport = 8080\n```\n\nHm, so everything is actually in place. But somehow GraalVM can't find those files.\nIt still requires some more GraalVM fine-tuning here.\n\nBy default, GraalVM doesn't include any resource or configuration-files.\nThe option `-H:ResourceConfigurationFiles=path/to/resource-config.json` defines a path to a JSON configuration file. So inside the `resource-config.json` we can include our `application.conf` and our `logback.xml`.\n\nBut writing those config files can be tedious and it is difficult in larger projects to find all necessary classes that need to be included. GraalVM provides some support with writing those files and actually does all the work. In the project's root directory a configs-folder can be created which will contain all necessary config-files.\n\nFor writing the configuration files we will build a normal JAR-file with the help of the `sbt-assembly` plugin. Adding it to the project like so:\n\n```java scala sbt\n  // inside project/plugins.sbt\n  addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\")\n```\n\nThe JAR-file will be built with `sbt assembly`.\n\nWith that we can now start the application, providing the path to the JAR-file that just has been created:\n\n```bash\nmkdir configs\n$GRAALHOME/bin/java -agentlib:native-image-agent=config-output-dir=./configs -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n```\n\nWith the command above the JAR gets to run with GraalVM but adds [dynamic lookups](https://www.graalvm.org/reference-manual/native-image/Configuration/#assisted-configuration-of-native-image-builds) that are being intercepted during runtime and written to the files: `jni-config.json`, `proxy-config.json`, `reflect-config.json` and `resource-config.json`.\n\nThose generated files can be included in the GraalVMNativeImageOptions:\n\n```java scala\n// build.sbt\ngraalVMNativeImageOptions ++= Seq(\n\t\"--allow-incomplete-classpath\",\n\t\"-H:ResourceConfigurationFiles=../../configs/resource-config.json\",\n\t\"-H:ReflectionConfigurationFiles=../../configs/reflect-config.json\",\n\t\"-H:JNIConfigurationFiles=../../configs/jni-config.json\",\n\t\"-H:DynamicProxyConfigurationFiles=../../configs/proxy-config.json\"\n)\n```\n\nThe build with those updated options should succeed and the app can be run once again: \n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n```\n\nStill no logging, sadly. But the server is actually running and responds to POST requests via its exposed endpoint:\n\n```json\nPOST localhost:8080/api/v1/apply\n\n{\n\t\"email\": \"my@email.de\",\n\t\"name\": \"My Name\",\n\t\"phoneNumber\": \"+310123456789\",\n\t\"applicationBase64\": \"VGhpcyBjb3VsZCBiZSB5b3VyIGFwcGxpY2F0aW9uIQ==\"\n}\n\nResponse:\n\"*confetti* Thanks for handing in your application, we will get back to you within the next days! *confetti*\"\n```\n\nThe next and last step will investigate why logging is not picked up by GraalVM.\n\n### Investigating the missing logging\n\nSo first I wanted to have a look if it was an overall issue with logging. I stepped back from using logging-framework and tried the most basic logging with the java-integrated `java.util.Logging`. GraalVM's [docs](https://www.graalvm.org/docs/Native-Image/user/LOGGING) stated that GraalVM supports any logging that depends on that.\n\nBuilding and running the native-image with `java.util.Logging` instead of `logback` succeeded and everything is logged properly.\n\nSo it must be something with the dependencies?\n\nFor further investigation, I added the [sbt-dependency-graph](https://github.com/jrudolph/sbt-dependency-graph) plugin and checked out the dependency-tree with `sbt dependencyBrowserTree`. The library `logback` wasn't included in the dependency tree.\nWhich is odd, since `logback` is clearly present in the project's library-dependencies.\n\n```java scala\n// inside build.sbt\nlibraryDependencies ++= Seq(\n\t...\n\t\"ch.qos.logback\" % \"logback-classic\" % \"1.2.3\" % Runtime,\n\t\"ch.qos.logback\" % \"logback-core\" % \"1.2.3\" % Runtime,\n\t...\n)\n```\n\nHaving a closer look, the appendix `% Runtime` on logback's dependency is present.\n\nNot sure where this was coming from but it is most probably blindly copy-pasted from somewhere when gathering the dependencies for this project.\n\n[sbt reference manual](https://www.scala-sbt.org/1.x/docs/Scopes.html#Scoping+by+the+configuration+axis) states that the appendix `Runtime` defines that this dependency will be only included in the runtime classpath.\n\nSo this explains probably why logging was only working when the server was run from inside sbt.\n\nWith removing this and building the native-image, `logback` appears in the dependency-tree, and logging works when the native image is executed!\n\nThis \"bug\" was interesting as it emphasized what GraalVM can NOT do for you. Dynamic class loading/linking can not be supported by GraalVM as classes and dependencies have to be present during compile time to make a fully functional application. \n\n### Roundup\n\nA successful setup of sbt and GraalVM to build native-images requires to:\n\n- install GraalVM's native-image functionality via it's graal-updater: \n  ```bash\n  gu install native-image\n  ```\n- add sbt-native-packager and sbt-assembly to sbt:\n  ```java scala sbt\n  // inside project/plugins.sbt\n  addSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.7.3\")\n  addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\")\n  ```\n- enable the GraalVM-Plugin:\n  ```java scala sbt\n  // inside build.sbt\n  enablePlugins(GraalVMNativeImagePlugin)\n  ```\n- create a fat JAR and define which resource and configuration files should be intergated by intercepting look up calls during its execution:\n  ```bash\n  sbt assembly\n  mkdir configs\n  $GRAALHOME/bin/java -agentlib:native-image-agent=config-output-dir=./configs -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n  ```\n- fine-tune GraalVM with the following options and include the files that have been created in the previous step:\n  ```java scala\n  // build.sbt\n  graalVMNativeImageOptions ++= Seq(\n    \"--allow-incomplete-classpath\",\n    \"-H:ResourceConfigurationFiles=../../configs/resource-config.json\",\n    \"-H:ReflectionConfigurationFiles=../../configs/reflect-config.json\",\n    \"-H:JNIConfigurationFiles=../../configs/jni-config.json\",\n    \"-H:DynamicProxyConfigurationFiles=../../configs/proxy-config.json\"\n  )\n  ```\n- build the native image with:\n  ```bash\n  sbt graalvm-native-image:packageBin\n  ```\n- run the executable file without the need of java\n  ```\n  ./target/graalvm-native-image/apply-at-vdb\n  ```\n\nEven without benchmarking, you notice that the startup time is way faster than with a traditional JAR-file and the application is up and running almost instantly.\n\nIt is worth noting that the creation of a native image is a quite time-consuming process. For this project, it took between 1 and 2 minutes. This is, of course, something a CI/CD-Server like Jenkins would take care of but it has to be kept in mind. \n\nWith a working native-image, it is time to dockerize.\n\n## Building Docker images\n\nIn this section two Docker containers will be built. One, following the \"normal\"-java way and the other will be using the native-image to build a Docker-container without Java.\n\nBefore getting started with native images, a regular JAR-file and Docker image for comparison can be built.\n\nWith the [sbt-assembly](https://github.com/sbt/sbt-assembly) plugin you can create JAR-files with all of its dependencies (fat JARs).\n`sbt assembly` creates this `target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar` which has a size of around 42MB:\n\n```shell\n sbt assembly \n ls -lh target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n\n  ...  ...   42M   target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n```\n\nThis application can be run locally via `java -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar` with the prerequisite that Java is installed on that machine.\n\nCreating the Docker image for this JAR-file can be done manually, but luckily `sbt-native-package` supports building regular Docker images out of the box, only the `DockerPlugin` needs to be enabled:\n\n```java scala\n// build.sbt\nenablePlugins(DockerPlugin)\n```\n\n`sbt docker:publishLocal` creates the Docker image `apply-at-vdb`.\n \n```shell\ndocker images | grep apply-at-vdb\n  apply-at-vdb \t0.1.0-SNAPSHOT \t\tf488d4c06f28 \t555MB\n```\n\nA whopping 555MB for a tiny app exposing one endpoint which JAR-file was only 42MB. But to run this JAR-file in a container, this container needs to ship with a JVM, and that's where the overhead lies.\n\nWith that Docker image and JAR-file as a reference, we can now look into how the native-image operates together with Docker.\n\nGraalVM does not support cross-building, meaning an application cannot be expected to be built in a MacOS environment and run in a Linux environment. It has to be built and run on the same platform. With the help of Docker, the desired built environment can be provided.\nThe `Dockerfile` looks as follows:\n```docker\nFROM oracle/graalvm-ce AS builder\nWORKDIR /app/vdb\nRUN gu install native-image\nRUN curl https://bintray.com/sbt/rpm/rpm > bintray-sbt-rpm.repo \\\n\t&& mv bintray-sbt-rpm.repo /etc/yum.repos.d/ \\\n\t&& yum install -y sbt\nCOPY . /app/vdb\nWORKDIR /app/vdb\nRUN sbt \"graalvm-native-image:packageBin\"\n\nFROM oraclelinux:7-slim\nCOPY --from=builder /app/vdb/target/graalvm-native-image/apply-at-vdb ./app/\nCMD ./app/apply-at-vdb\n\n```\n\nAnd can be run with:\n```bash\ndocker build -t native-apply-at-vdb .\n```\nThe Dockerfile describes to do the following:\nThe first docker container, as the name implies, is the builder. As a base image the official [GraalVM image](https://hub.docker.com/r/oracle/graalvm-ce) is used. \n\nThis image needs two more things, GraalVM's native-image command, and sbt, and this is what the two follow-up rows are providing. Once that's done, the project is copied into this container and the native image is built from within sbt.\n\nThe next steps bring the native executable into its own docker container.\nAs a base image, we use an Oracle Linux image and from our builder-container, we copy the native executable to this new container. The last step is that the app gets run on container startup.\n\n`docker run -p 8080:8080 -it native-apply-at-vdb` starts the container and shows that everything is working just as before.\n\nBut what about the image size? Let's have a look.\n```\ndocker images | grep apply-at-vdb\n  native-apply-at-vdb\t\tlatest              17b559e78645\t\t199MB\n  apply-at-vdb\t\t\t0.1.0-SNAPSHOT      f488d4c06f28\t\t555MB\n```\nThat is impressive! We created an app that is approx. 2.8 times smaller than our original app.\n\n## Summary\n\nWe learned how to set up a Scala project with GraalVM, what steps have to be taken to build a native image with GraalVM, and let it run inside a Docker container. We also received a good overview of what's possible with GraalVM and what's not.\n\nThe initial start and setup of GraalVM with sbt is pretty easy and straightforward. Getting GraalVM to compile an sbt project is nice and simple. \n\nThis Hackathon showed us that it is difficult and requires a lot of fine-tuning to integrate GraalVM into an existing project or product. At Vandebron we work with a complex stack of technologies including Spark, Kafka, and Akka which made it difficult to port the findings from this small toy service to one of our existing microservices. This made extensive troubleshooting in the Hackathon not possible.\n\nAll in all, GraalVM allows you to give up some Java overhead and create significant smaller Docker images. Sadly, this comes at the cost of giving up dynamic linking and class loading. \nA silver lining is, that inside Scala's ecosystem this rarely a problem. Scala relies heavily on compile-time mechanisms for detecting bugs early and creating type-safe applications (read [here](https://blog.softwaremill.com/small-fast-docker-images-using-graalvms-native-image-99c0bc92e70b) but also see e.g. [Scala's compiler phases](https://typelevel.org/scala/docs/phases.html)).\n\n* * *\n\n## Sources and Reading\n- [Building Serverless Scala Services with GraalVM](https://www.inner-product.com/posts/serverless-scala-services-with-graalvm/) by Noel Welsh\n- [Small & fast Docker images using GraalVM’s native-image](https://blog.softwaremill.com/small-fast-docker-images-using-graalvms-native-image-99c0bc92e70b) by Adam Warski\n- [Run Scala applications with GraalVM and Docker](https://medium.com/rahasak/run-scala-applications-with-graalvm-and-docker-a1e67701e935) by @itseranga\n- [Getting Started with GraalVM and Scala](https://medium.com/graalvm/getting-started-with-graalvm-for-scala-d0a006dec1d1) by Oleg Šelajev\n- [Updates on Class Initialization in GraalVM Native Image Generation](https://medium.com/graalvm/updates-on-class-initialization-in-graalvm-native-image-generation-c61faca461f7) by \nChristian Wimmer\n- [GraalVM's Reference Manuals](https://www.graalvm.org/reference-manual/)\n","meta":{"title":"Building native images and compiling with GraalVM and sbt","description":"At Vandebron we organized a two-day long Hackathon, a colleague and I took the chance to dig into the wonderful world of GraalVM.","createdAt":"Tue Oct 06 2020 02:00:00 GMT+0200 (Central European Summer Time)","coverImage":"images/building-native-images-and-compiling-with-graalvm-and-sbt.jpg","imageSource":"https://pixabay.com/users/lumix2004-3890388/","tags":"graalvm, scala","author":"Katrin Grunert","slug":"blog/building-native-images-and-compiling-with-graalvm-and-sbt copy","formattedDate":"October 6, 2020","date":"Tue Oct 06 2020 02:00:00 GMT+0200 (Central European Summer Time)"}},{"content":"\nAt Vandebron we organize a two-day long Hackathon every quarter, and a colleague and I took this chance to dig into the wonderful world of GraalVM.\n\nI've first heard of GraalVM around two years ago when Oleg Šelajev toured through Java User Groups in Germany and held talks about GraalVM. [Here](https://www.youtube.com/watch?v=GinNxS3OSi0) is one from 2019 (not Germany, but Spain this time).\n\nGraalVM promises a significant speedup in compile times and as I am working with Scala, which is notoriously known for its long compile times, this seems interesting. Furthermore, GraalVM provides functionality to build native executables. Meaning, an application can be run without a Java Virtual Machine (JVM).\n  \nThanks to the Hackathon I finally took the time to get to know GraalVM a bit better. With this blog post, I want to share our findings, experiences, and results, as they might be helpful for you too!\n\n## What is GraalVM?\n\nGraalVM is a high-performance JVM that supports efficient ahead-of-time (AOT) and just-in-time (JIT) compilation, but also allows non-JVM languages (e.g. Ruby, Python, C++) to run on the JVM. The ahead-of-time compilation feature is the base for creating native executable programs, meaning an application can be run independently from the JVM. Seeing the versatile features of GraalVM, it is worth looking a bit under its hood.\n\nActually, GraalVM is defined by three main technologies:\n\n- [Graal compiler](https://www.graalvm.org/reference-manual/jvm/), a high-performance JIT-compiler that can make JVM applications run faster from within the JVM\n- [SubstrateVM](https://www.graalvm.org/reference-manual/native-image/SubstrateVM/), includes the necessary components to run a JVM-app as a native executable ( Garbage Collector, Thread Scheduler, etc.)\n- [Truffle Language Implementation Framework](https://www.graalvm.org/graalvm-as-a-platform/language-implementation-framework/), the basis for the polyglot support from GraalVM\n\nOur motivation for trying out GraalVM was tackling the pain points of Scala, Java projects, and microservices. Shipping microservices written in Scala as Docker containers to your production system comes with the cost that startup can be a bit slow, having JVM and Docker overhead, and that those containers can be fairly large, as the application can only be run with a JVM. See [Building Docker images](#building-docker-images) for more information.\n\nDuring the hackathon, we were most interested in building native images for Scala applications. Hoping to reduce the size of our docker containers and reducing up the startup time.\n\n## Project setup\n\nThe project we worked on during the Hackathon is an API that should be used for applicants to submit their applications at Vandebron in the future. By exposing one endpoint through which a resume and contact information can be submitted.\n\nIt is also a good project to test out GraalVM, nothing too complex but also not as simple as \"Hello World\".\n\nThe full setup can be found [on Github](https://github.com/kgrunert/apply-at-vdb). But I'll summarise the used stack below. The project is built around the following libraries, no particular reason, simply because I like them.\n\n- _cats_ for working with effects, such as IO\n- _http4s_ for running the server\n- _tapir_ for defining the endpoints\n- _circe_ for JSON de/serialisation\n- _pureconfig_ for reading config-files\n- _logback_ for logging\n\nThe project can be run via `sbt run` and with Postman or similar a POST-request can be sent like so:\n\n```json\nPOST localhost:8080/api/v1/apply\n\n{\n\t\"email\": \"my@email.de\",\n\t\"name\": \"My Name\",\n\t\"phoneNumber\": \"+310123456789\",\n\t\"applicationBase64\": \"VGhpcyBjb3VsZCBiZSB5b3VyIGFwcGxpY2F0aW9uIQ==\"\n}\n\nResponse:\n\"*confetti* Thanks for handing in your application, we will get back to you within the next days! *confetti*\"\n```\n\n## Setup GraalVM with sbt\n\nWith this initial project setup in mind, GraalVM needs to be installed locally.\n\nFor the installation of GraalVM the [setup guide](https://www.graalvm.org/docs/getting-started-with-graalvm/#install-graalvm) can be followed.\n\nAfter the installation sbt needs to know that not the regular JDK/JVM is used. This can be done with the `java-home` option on sbt bootup.\nTo make the path to GraalVM a bit more accessible and easy to use it can be exported as an environment variable.\n\n```bash\nexport GRAAL_HOME=/Library/Java/JavaVirtualMachines/graalvm-ce-java8-20.1.0/Contents/Home\nsbt -java-home $GRAALHOME\n```\n\nThe path to GraalVM can vary depending on OS and installation. We followed the basic installation for macOS.\n\nNow sbt using GraalVM can be verified with:\n\n```bash\nsbt -java-home $GRAALHOME\nscala> eval System.getProperty(\"java.home\")\n[info] ans: String = /Library/Java/JavaVirtualMachines/graalvm-ce-java8-20.1.0/Contents/Home/jre\n```\n\nThat means everything running in this sbt instance is getting compiled by GraalVM. Awesome!\n\nThe next step is to become strong and independent and learn how to run without an underlying JVM with the help of building native images.\n\n## Building native images\n\nGraalVM ships with the [GraalVM Updater](https://www.graalvm.org/reference-manual/graalvm-updater/) (`gu`) to install the `native-image` on your machine.\n\n```bash\n$GRAALHOME/bin/gu install native-image\n```\n\n[sbt-native-packager](https://sbt-native-packager.readthedocs.io/en/latest/) provides functionality to build packages efficiently (e.g. building Docker images) and added to that, it also provides support for building native images.\nIn order to build native images with sbt commands this plugin has to be added to the project:\n\n```java scala\n// inside project/plugins.sbt\naddSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.7.3\")\n```\n\nAnd the `GraalVMNativeImagePlugin` needs to be enabled:\n\n```java scala\n// inside build.sbt\nenablePlugins(GraalVMNativeImagePlugin)\n```\n\nFrom within sbt it should be able to autocomplete and suggest graal-commands, e.g.:\n\n```java scala\nsbt:apply-at-vdb> graalvm\ngraalvm-native-image:       graalvmNativeImageOptions\n```\n\nWith that setup, native images are just a stone's throw away!\n\n---\n\n### Disclaimer\n\nThe next three sections are not a write-up but rather the main steps we had to take to make the project work. This includes failing images and troubleshooting.\nI want to keep this in because it might be interesting for others when they have to troubleshoot.\nFor the summary and happy path, you can jump directly to [Roundup](#roundup).\n\n---\n\n### First try building a native image\n\nNext up `graalvm-native-image:packageBin` can be run from within sbt. This might take a while (on our systems it took about a minute)\n\nSome warnings start to pop up:\n\n```\n[error] warning: unknown locality of class Lnl/vandebron/applyatvdb/Main$anon$exportedReader$macro$24$1;, assuming class is not local. To remove the warning report an issue to the library or language author. The issue is caused by Lnl/vandebron/applyatvdb/Main$anon$exportedReader$macro$24$1; which is not following the naming convention.\n\n[error] warning: unknown locality of class Lfs2/internal/Algebra$Done$2$;, assuming class is not local. To remove the warning report an issue to the library or language author. The issue is caused by Lfs2/internal/Algebra$Done$2$; which is not following the naming convention.\n```\n\nThe library-specific warnings can be ignored for now. Ultimately it fails with:\n\n```\nError: com.oracle.graal.pointsto.constraints.UnresolvedElementException:\nDiscovered unresolved type during parsing: org.slf4j.impl.StaticLoggerBinder.\nTo diagnose the issue you can use the --allow-incomplete-classpath option.\nThe missing type is then reported at run time when it is accessed the first time.\n```\nActually a good hint on where to start fine-tuning the GraalVM config:\n\n```java scala\n// inside build.sbt\ngraalVMNativeImageOptions ++= Seq(\n\t\"--allow-incomplete-classpath\",\n)\n```\n\nSome things like a `StaticLoggerBinder` only get resolved at runtime, meaning at build time the classpath needs to be allowed to be incomplete. This option allows resolution errors to be ignored at build time and only pop up during runtime.\n\nDuring the build of a native image, GraalVM tries to resolve those runtime dependencies already at compile-time, as it is part of the Ahead-Of-Time-compilation process. With this flag, GraalVM knows \"hey, don't worry about it now, we cross the bridge when we get there\" (or something like that).\n\n### Adding resource files\n\nA `reload` (or restart) of sbt is needed to activate these new options. And we can try to build the native image up new.\nThis time the build finished successfully and the executable file `target/graalvm-native-image/apply-at-vdb` has been created!\nThis is an executable that can be run without a JVM:\n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n```\n\nBut what's that? It actually cannot be started...\n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n*** An error occured! ***\nCannot convert configuration to a de.erewl.pricetracker.server.Config. Failures are:\nat the root:\n- Key not found: 'host'.\n- Key not found: 'port'.\n```\n\nThe first three lines relate to the error that occurred during the first build. It simply says that logging hasn't been set up correctly (maybe due to the absence of a `src/main/resources/logback.xml` or some other misconfiguration), triggering the default setting of not logging anything at all.\nThe second error states that a configuration file does not have the right keys or cannot be found at all.\nLooking into `src/main/resources`:\n\n```bash\nls src/main/resources/\napplication.conf logback.xml\n```\n\nand peeking into `application.conf`:\n\n```bash\ncat src/main/resources/application.conf\n\thost = \"localhost\"\n\tport = 8080\n```\n\nHm, so everything is actually in place. But somehow GraalVM can't find those files.\nIt still requires some more GraalVM fine-tuning here.\n\nBy default, GraalVM doesn't include any resource or configuration-files.\nThe option `-H:ResourceConfigurationFiles=path/to/resource-config.json` defines a path to a JSON configuration file. So inside the `resource-config.json` we can include our `application.conf` and our `logback.xml`.\n\nBut writing those config files can be tedious and it is difficult in larger projects to find all necessary classes that need to be included. GraalVM provides some support with writing those files and actually does all the work. In the project's root directory a configs-folder can be created which will contain all necessary config-files.\n\nFor writing the configuration files we will build a normal JAR-file with the help of the `sbt-assembly` plugin. Adding it to the project like so:\n\n```java scala sbt\n  // inside project/plugins.sbt\n  addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\")\n```\n\nThe JAR-file will be built with `sbt assembly`.\n\nWith that we can now start the application, providing the path to the JAR-file that just has been created:\n\n```bash\nmkdir configs\n$GRAALHOME/bin/java -agentlib:native-image-agent=config-output-dir=./configs -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n```\n\nWith the command above the JAR gets to run with GraalVM but adds [dynamic lookups](https://www.graalvm.org/reference-manual/native-image/Configuration/#assisted-configuration-of-native-image-builds) that are being intercepted during runtime and written to the files: `jni-config.json`, `proxy-config.json`, `reflect-config.json` and `resource-config.json`.\n\nThose generated files can be included in the GraalVMNativeImageOptions:\n\n```java scala\n// build.sbt\ngraalVMNativeImageOptions ++= Seq(\n\t\"--allow-incomplete-classpath\",\n\t\"-H:ResourceConfigurationFiles=../../configs/resource-config.json\",\n\t\"-H:ReflectionConfigurationFiles=../../configs/reflect-config.json\",\n\t\"-H:JNIConfigurationFiles=../../configs/jni-config.json\",\n\t\"-H:DynamicProxyConfigurationFiles=../../configs/proxy-config.json\"\n)\n```\n\nThe build with those updated options should succeed and the app can be run once again: \n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n```\n\nStill no logging, sadly. But the server is actually running and responds to POST requests via its exposed endpoint:\n\n```json\nPOST localhost:8080/api/v1/apply\n\n{\n\t\"email\": \"my@email.de\",\n\t\"name\": \"My Name\",\n\t\"phoneNumber\": \"+310123456789\",\n\t\"applicationBase64\": \"VGhpcyBjb3VsZCBiZSB5b3VyIGFwcGxpY2F0aW9uIQ==\"\n}\n\nResponse:\n\"*confetti* Thanks for handing in your application, we will get back to you within the next days! *confetti*\"\n```\n\nThe next and last step will investigate why logging is not picked up by GraalVM.\n\n### Investigating the missing logging\n\nSo first I wanted to have a look if it was an overall issue with logging. I stepped back from using logging-framework and tried the most basic logging with the java-integrated `java.util.Logging`. GraalVM's [docs](https://www.graalvm.org/docs/Native-Image/user/LOGGING) stated that GraalVM supports any logging that depends on that.\n\nBuilding and running the native-image with `java.util.Logging` instead of `logback` succeeded and everything is logged properly.\n\nSo it must be something with the dependencies?\n\nFor further investigation, I added the [sbt-dependency-graph](https://github.com/jrudolph/sbt-dependency-graph) plugin and checked out the dependency-tree with `sbt dependencyBrowserTree`. The library `logback` wasn't included in the dependency tree.\nWhich is odd, since `logback` is clearly present in the project's library-dependencies.\n\n```java scala\n// inside build.sbt\nlibraryDependencies ++= Seq(\n\t...\n\t\"ch.qos.logback\" % \"logback-classic\" % \"1.2.3\" % Runtime,\n\t\"ch.qos.logback\" % \"logback-core\" % \"1.2.3\" % Runtime,\n\t...\n)\n```\n\nHaving a closer look, the appendix `% Runtime` on logback's dependency is present.\n\nNot sure where this was coming from but it is most probably blindly copy-pasted from somewhere when gathering the dependencies for this project.\n\n[sbt reference manual](https://www.scala-sbt.org/1.x/docs/Scopes.html#Scoping+by+the+configuration+axis) states that the appendix `Runtime` defines that this dependency will be only included in the runtime classpath.\n\nSo this explains probably why logging was only working when the server was run from inside sbt.\n\nWith removing this and building the native-image, `logback` appears in the dependency-tree, and logging works when the native image is executed!\n\nThis \"bug\" was interesting as it emphasized what GraalVM can NOT do for you. Dynamic class loading/linking can not be supported by GraalVM as classes and dependencies have to be present during compile time to make a fully functional application. \n\n### Roundup\n\nA successful setup of sbt and GraalVM to build native-images requires to:\n\n- install GraalVM's native-image functionality via it's graal-updater: \n  ```bash\n  gu install native-image\n  ```\n- add sbt-native-packager and sbt-assembly to sbt:\n  ```java scala sbt\n  // inside project/plugins.sbt\n  addSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.7.3\")\n  addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\")\n  ```\n- enable the GraalVM-Plugin:\n  ```java scala sbt\n  // inside build.sbt\n  enablePlugins(GraalVMNativeImagePlugin)\n  ```\n- create a fat JAR and define which resource and configuration files should be intergated by intercepting look up calls during its execution:\n  ```bash\n  sbt assembly\n  mkdir configs\n  $GRAALHOME/bin/java -agentlib:native-image-agent=config-output-dir=./configs -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n  ```\n- fine-tune GraalVM with the following options and include the files that have been created in the previous step:\n  ```java scala\n  // build.sbt\n  graalVMNativeImageOptions ++= Seq(\n    \"--allow-incomplete-classpath\",\n    \"-H:ResourceConfigurationFiles=../../configs/resource-config.json\",\n    \"-H:ReflectionConfigurationFiles=../../configs/reflect-config.json\",\n    \"-H:JNIConfigurationFiles=../../configs/jni-config.json\",\n    \"-H:DynamicProxyConfigurationFiles=../../configs/proxy-config.json\"\n  )\n  ```\n- build the native image with:\n  ```bash\n  sbt graalvm-native-image:packageBin\n  ```\n- run the executable file without the need of java\n  ```\n  ./target/graalvm-native-image/apply-at-vdb\n  ```\n\nEven without benchmarking, you notice that the startup time is way faster than with a traditional JAR-file and the application is up and running almost instantly.\n\nIt is worth noting that the creation of a native image is a quite time-consuming process. For this project, it took between 1 and 2 minutes. This is, of course, something a CI/CD-Server like Jenkins would take care of but it has to be kept in mind. \n\nWith a working native-image, it is time to dockerize.\n\n## Building Docker images\n\nIn this section two Docker containers will be built. One, following the \"normal\"-java way and the other will be using the native-image to build a Docker-container without Java.\n\nBefore getting started with native images, a regular JAR-file and Docker image for comparison can be built.\n\nWith the [sbt-assembly](https://github.com/sbt/sbt-assembly) plugin you can create JAR-files with all of its dependencies (fat JARs).\n`sbt assembly` creates this `target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar` which has a size of around 42MB:\n\n```shell\n sbt assembly \n ls -lh target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n\n  ...  ...   42M   target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n```\n\nThis application can be run locally via `java -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar` with the prerequisite that Java is installed on that machine.\n\nCreating the Docker image for this JAR-file can be done manually, but luckily `sbt-native-package` supports building regular Docker images out of the box, only the `DockerPlugin` needs to be enabled:\n\n```java scala\n// build.sbt\nenablePlugins(DockerPlugin)\n```\n\n`sbt docker:publishLocal` creates the Docker image `apply-at-vdb`.\n \n```shell\ndocker images | grep apply-at-vdb\n  apply-at-vdb \t0.1.0-SNAPSHOT \t\tf488d4c06f28 \t555MB\n```\n\nA whopping 555MB for a tiny app exposing one endpoint which JAR-file was only 42MB. But to run this JAR-file in a container, this container needs to ship with a JVM, and that's where the overhead lies.\n\nWith that Docker image and JAR-file as a reference, we can now look into how the native-image operates together with Docker.\n\nGraalVM does not support cross-building, meaning an application cannot be expected to be built in a MacOS environment and run in a Linux environment. It has to be built and run on the same platform. With the help of Docker, the desired built environment can be provided.\nThe `Dockerfile` looks as follows:\n```docker\nFROM oracle/graalvm-ce AS builder\nWORKDIR /app/vdb\nRUN gu install native-image\nRUN curl https://bintray.com/sbt/rpm/rpm > bintray-sbt-rpm.repo \\\n\t&& mv bintray-sbt-rpm.repo /etc/yum.repos.d/ \\\n\t&& yum install -y sbt\nCOPY . /app/vdb\nWORKDIR /app/vdb\nRUN sbt \"graalvm-native-image:packageBin\"\n\nFROM oraclelinux:7-slim\nCOPY --from=builder /app/vdb/target/graalvm-native-image/apply-at-vdb ./app/\nCMD ./app/apply-at-vdb\n\n```\n\nAnd can be run with:\n```bash\ndocker build -t native-apply-at-vdb .\n```\nThe Dockerfile describes to do the following:\nThe first docker container, as the name implies, is the builder. As a base image the official [GraalVM image](https://hub.docker.com/r/oracle/graalvm-ce) is used. \n\nThis image needs two more things, GraalVM's native-image command, and sbt, and this is what the two follow-up rows are providing. Once that's done, the project is copied into this container and the native image is built from within sbt.\n\nThe next steps bring the native executable into its own docker container.\nAs a base image, we use an Oracle Linux image and from our builder-container, we copy the native executable to this new container. The last step is that the app gets run on container startup.\n\n`docker run -p 8080:8080 -it native-apply-at-vdb` starts the container and shows that everything is working just as before.\n\nBut what about the image size? Let's have a look.\n```\ndocker images | grep apply-at-vdb\n  native-apply-at-vdb\t\tlatest              17b559e78645\t\t199MB\n  apply-at-vdb\t\t\t0.1.0-SNAPSHOT      f488d4c06f28\t\t555MB\n```\nThat is impressive! We created an app that is approx. 2.8 times smaller than our original app.\n\n## Summary\n\nWe learned how to set up a Scala project with GraalVM, what steps have to be taken to build a native image with GraalVM, and let it run inside a Docker container. We also received a good overview of what's possible with GraalVM and what's not.\n\nThe initial start and setup of GraalVM with sbt is pretty easy and straightforward. Getting GraalVM to compile an sbt project is nice and simple. \n\nThis Hackathon showed us that it is difficult and requires a lot of fine-tuning to integrate GraalVM into an existing project or product. At Vandebron we work with a complex stack of technologies including Spark, Kafka, and Akka which made it difficult to port the findings from this small toy service to one of our existing microservices. This made extensive troubleshooting in the Hackathon not possible.\n\nAll in all, GraalVM allows you to give up some Java overhead and create significant smaller Docker images. Sadly, this comes at the cost of giving up dynamic linking and class loading. \nA silver lining is, that inside Scala's ecosystem this rarely a problem. Scala relies heavily on compile-time mechanisms for detecting bugs early and creating type-safe applications (read [here](https://blog.softwaremill.com/small-fast-docker-images-using-graalvms-native-image-99c0bc92e70b) but also see e.g. [Scala's compiler phases](https://typelevel.org/scala/docs/phases.html)).\n\n* * *\n\n## Sources and Reading\n- [Building Serverless Scala Services with GraalVM](https://www.inner-product.com/posts/serverless-scala-services-with-graalvm/) by Noel Welsh\n- [Small & fast Docker images using GraalVM’s native-image](https://blog.softwaremill.com/small-fast-docker-images-using-graalvms-native-image-99c0bc92e70b) by Adam Warski\n- [Run Scala applications with GraalVM and Docker](https://medium.com/rahasak/run-scala-applications-with-graalvm-and-docker-a1e67701e935) by @itseranga\n- [Getting Started with GraalVM and Scala](https://medium.com/graalvm/getting-started-with-graalvm-for-scala-d0a006dec1d1) by Oleg Šelajev\n- [Updates on Class Initialization in GraalVM Native Image Generation](https://medium.com/graalvm/updates-on-class-initialization-in-graalvm-native-image-generation-c61faca461f7) by \nChristian Wimmer\n- [GraalVM's Reference Manuals](https://www.graalvm.org/reference-manual/)\n","meta":{"title":"Building native images and compiling with GraalVM and sbt","description":"At Vandebron we organized a two-day long Hackathon, a colleague and I took the chance to dig into the wonderful world of GraalVM.","createdAt":"Tue Oct 06 2020 02:00:00 GMT+0200 (Central European Summer Time)","coverImage":"images/building-native-images-and-compiling-with-graalvm-and-sbt.jpg","imageSource":"https://pixabay.com/users/lumix2004-3890388/","tags":"graalvm, scala","author":"Katrin Grunert","slug":"blog/building-native-images-and-compiling-with-graalvm-and-sbt","formattedDate":"October 6, 2020","date":"Tue Oct 06 2020 02:00:00 GMT+0200 (Central European Summer Time)"}},{"content":"\nTwo months ago, I started my journey at Vandebron. One of the projects I first dove into was their efforts to build a [component library](https://windmolen.netlify.app/). Something I was already familiar with from previous companies I worked at. \n\nOn the internet, you can find many articles that describe why a reusable component library is a good investment for your development team(s). Although there's much to say about the advantages of component libraries, most articles don't state the (obvious) disadvantages such projects can have. In this post, I'll point out some of our learnings and why you might not need such a reusable component library.\n\n## About component libraries\n\nOften you find yourself repeating the same lines of code to make, for example, a button or the layout of a page look nice, especially when you're working on multiple projects. Or as a designer, you get frustrated every time the styling for a part of the application is off when a new page or project is created. Many companies have already found multiple solutions to preventing themselves from repeating styling, which is the main reason for design inconsistencies. And therefore component libraries were created.\n\nA component library is a collection of all the styled parts (or components) of a website or multiple websites that make it easier for developers to reuse these parts. Also, designers will know for sure that all components in the component library adhere to their designs, and therefore all projects that use these components will conform. Often these libraries consist of different layers of components, for example, offering atoms, molecules, and organisms when an [Atomic Design](https://bradfrost.com/blog/post/atomic-web-design/) pattern is applied. Following this pattern, developers can use the parts to style their templates and pages consistently.\n\nComponent libraries are becoming more and more popular with the rise of JavaScript libraries and frameworks like React and Vue. These technologies are very suitable for quickly building interactive components that you can use in your application, and can easily be exposed as a library on NPM or Github Packages. At Vandebron, we're building all our web and mobile applications with React and React Native and are using [Storybook](https://storybook.js.org/) to develop our components in a shared library between the engineering and design teams. This can potentially create a lot of advantages for both the developers and designers, as you can read below.\n\n## Why you *might* need a component library\n\nBefore deciding to create a component library for your team or company, you probably want to hear about the advantages such a project can lead to. The main advantages of component libraries are briefly mentioned in the first section above and are often defined as:\n\n- **Reducing code duplication**: With a component library, you can create components that can be shared across multiple websites or applications. This way you no longer have to duplicate styling in different projects. This can seriously decrease the amount of code duplication that you have in your projects, also reducing the number of bugs or design inconsistencies.\n\n- **Preventing design inconsistencies**: By adding all your components and styled parts to the component library you're certain that these will look the same on all the places they're used. Not only will all the components look the same on every page, when designers make a change to one of these components they can be easily updated on all the places they're used.\n\n- **Easier collaborating**: Component libraries make it easier for developers and designers to collaborate on applications and designs, with the component library as the common \"playground\". By using a tool, like Storybook, you can also make this playground visible to non-technical people and show what components are already available to use for new features.\n\nBut these advantages come at a certain price, as I'll explain in the next section.\n\n## Disadvantages of component libraries\n\nBesides the obvious advantages of a component library, it can also have serious disadvantages that are listed below. Whether or not these disadvantages apply to you depends on numerous things that are discussed later on in this article.\n\n- **Increasing complexity**: With all attempts to make code more generic,  an increased level of complexity also comes to play. Reusable components should be easy to extend or customize, which requires you to think about the different use cases beforehand or force you to add many different variations to a component. With every new project that starts to use the component library, you get the risk of increasing the complexity of the library even more.\n\n- **Time-consuming**: Every time you want to add a component to your project, you need to create that component in the component library first and import it locally in the project to test it. Therefore you need to be working in multiple projects at the same time, which requires you to set up a more time-consuming workflow. Also, when you want to use this new component from the library, you have to publish a new version of the library to make the component available.\n\n- **Conflicting dependencies**: When you're using different versions of dependencies across your projects and the component library, you're forced to sync those with each other. Imagine having, for example, an older version of React running in one of your projects that doesn't use a recent React API that you want to use in your component library. In this scenario, you either have to update that project or are unable to keep your component library on par with the latest release of your dependency on React. Both solutions have pros and cons, and would rather be avoided.\n\nAs mentioned before, there are reasons why these disadvantages might apply to you that are the team size, the number of teams and projects at the company, development or release lifecycles, and how your source code is organized. It clearly doesn't make sense to invest in a component library if you have just a small amount of people work on just one project, or a sole team is working on all the different projects making it easier to manage code duplication or design inconsistencies.\n\n## Considerations before starting\n\nThere are two main alternatives that you need to take into consideration before building a reusable component library, which is (obviously) using or extending an existing component library or sourcing your code in a monorepo. \n\n- **Existing component libraries:** Using an existing component library is an efficient way to create consistently (web) pages and reduce the amount of complexity of your own project, while also taking advantage of best practices of large open-source projects. Popular examples of component libraries are [Ant Design For React](https://ant.design/docs/react/introduce) or [various implementations](https://material.io/develop) for Google's Material Design. These libraries allow you to move quickly without having all the overhead of creating complex components but limit you to the design guidelines of these component libraries.\n\n- **Monorepo:** If you don't want to take advantage of existing libraries or are very keen to apply your own styling to components across multiple applications without having to copy-paste the code, you can host the source code of applications in a monorepo. With the monorepo approach, you can create a shared folder that includes all the components used by your applications. This makes it possible to apply changes with a simple pull request and import these components from every project in that repository.\n\nBesides these two alternatives, you also need to have proper design guidelines set by your designer(s). When the design guidelines are flexible and fluctuating, you could be structuring components incorrectly with the risk of doing a lot of work that will be omitted once the project evolves.\n\n## To summarize\n\nComponent libraries are a great way to reduce the amount of code duplication in your applications, prevent design inconsistencies, and increase collaborations between developers, designers, and different teams. But this comes with increased complexity, slower development cycles, and possible code conflicts between projects. Therefore you should consider if using an existing component library or having a monorepo for your source code is a workable solution. At Vandebron we decided to build our own component library (called [windmolen](https://windmolen.netlify.app/)) and if you'd decide the same, then be sure that your design guidelines are properly structured and mature enough.\n","meta":{"title":"When (Not) To Build A Reusable Component Library","description":"You can find much information on why a reusable component library is a good investment, but most articles don't state the (obvious) disadvantages..","createdAt":"Mon Oct 05 2020 02:00:00 GMT+0200 (Central European Summer Time)","coverImage":"images/when-not-to-build-a-reusable-component-library.jpg","imageSource":"https://pixabay.com/users/stevepb-282134/","tags":"React, component library","author":"Roy Derks","slug":"blog/when-not-to-build-a-reusable-component-library","formattedDate":"October 5, 2020","date":"Mon Oct 05 2020 02:00:00 GMT+0200 (Central European Summer Time)"}}]},"__N_SSG":true}