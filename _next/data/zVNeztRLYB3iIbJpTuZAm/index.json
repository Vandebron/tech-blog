{"pageProps":{"posts":[{"content":"\nAt Vandebron we're maintaining a component library called [Windmolen](https://windmolen.netlify.app/) (Dutch for \"wind turbine\"). And if you've ever built a component library, you probably dealt with optimizing and converting icons before. With SVGO and SVGR you can do this at scale, without compromising the quality or size of your icons.\n\n## The problem\n\nThe web is full of icons, and often these icons are rendered from SVG files to ensure you can increase (or decrease) the size of the icons depending on the use case. Designers often create these icons from design tools like Adobe Photoshop or Sketch. Although these icons might look pretty, exporting a SVG out of these tools is often difficult as [this article](https://medium.com/sketch-app-sources/the-best-way-to-export-an-svg-from-sketch-dd8c66bb6ef2) explains. Also, added lot of code in the form of metadata is added to the SVG file. Let's have a look at what a typical SVG file exported out of Sketch looks like:\n\n```svg\n<!-- something.svg -->\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<svg width=\"14px\" height=\"14px\" viewBox=\"0 0 14 14\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n    <!-- Generator: Sketch 46 (44423) - http://www.bohemiancoding.com/sketch -->\n    <title>last</title>\n    <desc>Created with Sketch.</desc>\n    <defs></defs>\n    <g id=\"Page-1\" stroke=\"none\" stroke-width=\"1\" fill=\"none\" fill-rule=\"evenodd\">\n        <g id=\"last\" transform=\"translate(2.000000, 0.000000)\" fill-rule=\"nonzero\" fill=\"#666666\">\n            <polygon id=\"Fill-2\" points=\"6.6902923 9.6812703 9.3700469 7.0005052 6.6902923 4.3187297 2.37257308 0 0 2.37358354 4.3177192 6.6902923 4.6279322 7.0005052 4.3177192 7.3107182 0 11.6274269 2.37257308 14\"></polygon>\n        </g>\n    </g>\n</svg>\n```\n\nThe SVG file above holds a lot of information about Sketch, such as the `title` of the icon and a `desc`ription. Next to that, there's a lot of elements that could be combined into one element to reduce the file size.\n\n## Optimizing SVGs\n\nWhat's cool about SVG files is that you can optimize and minify them, without affecting what the SVG looks like. This is something you can try out yourself using the website [SVGOMG](https://jakearchibald.github.io/svgomg/), which is powered by the library SVGO that you'll learn more about later.\n\n\nYou can optimize the SVG file above by following these steps:\n\n1. Go to [https://jakearchibald.github.io/svgomg/](https://jakearchibald.github.io/svgomg/)\n2. Click on `Paste markup` an paste the SVG code that you exported from Sketch (a.k.a. the SVG file above)\n3. You will see the icon rendered, now you have to either click at the `Copy as a text` or `Download` button to get the optimized SVG file\n\nWith these simple steps you've optimized the SVG from over 450 bytes, which is already small, to 173 bytes (a decrease of over 62%!). If you'd open this file in the editor of your choice, you can see a lot of the useless (meta)data from the original file has been deleted. Also, the different elements of the SVG are combined in a single `path` that renders the icon:\n\n```svg\n<!-- something.svg -->\n<svg width=\"14\" height=\"14\" xmlns=\"http://www.w3.org/2000/svg\">\n  <path d=\"M8.69 9.681l2.68-2.68-2.68-2.682L4.373 0 2 2.374 6.318 6.69l.31.31-.31.31L2 11.628 4.373 14z\" fill-rule=\"nonzero\" fill=\"#666\"/>\n</svg>\n```\n\nThis SVG can be even further optimized by checking the \"Prefer viewbox to width/height\" in SVGOMG, but let's save that for later when we use SVGO instead.\n\n## Using SVGO\n\nBy using SVGOMG you've already experienced what power [SVGO](https://github.com/svg/svgo) has, as SVGOMG is described by its creators as *\" SVGO's Missing GUI, aiming to expose the majority if not all the configuration options of SVGO\"*. Instead of using the GUI, you can also use SVGO directly from the command line as a CLI-tool or as a Node.js module. For the sake of this article, we'll be using it solely as CLI.\n\nSVGO can be installed globally on your machine, or locally in your project, from npm by running:\n\n```bash\nnpm i -g svgo\n\n# Yarn equivalent\nyarn add -G svgo\n```\n\nAfter doing this you can run `svgo` from the command line and optimize any SVG file instantly. But, you don't want to do this manually on your machine anytime you're adding a new icon to a project (or component library). Therefore, you can also add SVGO to a project locally and add a script to the `package.json` file to optimize all SVGs in a certain directory.\n\n```json\n// package.json\n{\n // ...\n \"scripts\": {\n     // ...\n    \"optimize-svg\": \"svgo --config=.svgo.yml -f ./src/assets/icons\"\n }\n}\n```\n\nThe `optimize-svg` script will run SVGO in the directory `src/assets/icons` and optimize all the SVG files based on the settings in `.svgo.yml`. This file is where you can configure the rules for SVGO, as the previously mentioned \"Prefer viewbox to width/height\":\n\n```yaml\n# .svgo.yml\nplugins:\n  - removeViewBox: false\n  - removeDimensions: true # this deletes width/height and adds it to the viewBox\n  - removeDoctype: true\n  - removeComments: true\n  - removeMetadata: true\n  - removeEditorsNSData: true\n  - cleanupIDs: true\n  - removeRasterImages: true\n  - removeUselessDefs: true\n  - removeUnknownsAndDefaults: true\n  - removeUselessStrokeAndFill: true\n  - removeHiddenElems: true\n  - removeEmptyText: true\n  - removeEmptyAttrs: true\n  - removeEmptyContainers: true\n  - removeUnusedNS: true\n  - removeDesc: true\n  - prefixIds: false\n  - prefixClassNames: false\n```\n   \nFrom the rules above you'll get an idea about all the redundant and useless lines of code that might be present in your SVG files. But luckily, they will all get removed when you run the command `npm run optimize-svg`.\n\n## Converting SVGs with SVGR\n\nYou've now learned how to optimize your SVG files, and are probably wondering how to use these files in a React application. To render an SVG in React, you need to either configure Webpack in a way that it knows how to deal with SVG files or use a library called SVGR. By default, any application created with `create-react-app` can render SVG files as a component, using the following `import` statement:\n\n```jsx\n// MyComponent.jsx\nimport React from 'react';\nimport { ReactComponent as MySVG } from './something.svg';\n\nconst MyComponent = () => {\n  return (\n    <div>\n      <MySVG />\n    </div>\n  );\n}\nexport default MyComponent;\n```\n\nMore information about how this is done can be found in [this article](https://blog.logrocket.com/how-to-use-svgs-in-react/), but let me show you how to solve that with SVGR.\n\nWith [SVGR](https://react-svgr.com/) you can convert SVG files into React Components, either by adding it to Webpack or by using the SVGR CLI or Node.js module. In the same way, as we optimized the SVGs from the command line with SVGO, we can also convert these icons from the command line with SVGR:\n\n```json\n// package.json\n{\n // ...\n \"scripts\": {\n     // ...\n    \"optimize-svg\": \"svgo --config=.svgo.yml -f ./src/assets/icons\",\n    \"convert-svg\": \"svgr -d ./src/components/Icon ./src/assets/icons\"\n }\n}\n```\n\nWhenever you run the command `npm run convert-svg` a JSX file will be created for every SVG file that's present in the directory `src/assets/icons`. These JSX files can be found in the directory `src/components/Icons`, together with an `index.js` file that exports all these components from this directory.\n\nAn example of such a converted SVG file is:\n\n\n```jsx\n// MySVG.jsx\nimport * as React from 'react';\n\nconst MySVG = (props) => (\n  <svg viewBox=\"0 0 14 14\" xmlns=\"http://www.w3.org/2000/svg\" {...props}>\n  <path d=\"M8.69 9.681l2.68-2.68-2.68-2.682L4.373 0 2 2.374 6.318 6.69l.31.31-.31.31L2 11.628 4.373 14z\" fill-rule=\"nonzero\" fill=\"#666\"/>\n  </svg>\n);\n\nexport default MySVG;\n```\n\nAnd, as we now have a directory filled with converted SVGs these can be imported into any React component like this:\n\n```jsx\n// MyComponent.jsx\nimport React from 'react';\nimport MySVG from './MySVG.jsx';\n\nconst MyComponent = () => {\n  return (\n    <div>\n      <MySVG />\n    </div>\n  );\n}\nexport default MyComponent;\n```\n\nOften SVGR is used alongside SVGO, so you can even automatically optimize all SVGS that will be converted by SVGR. This is done by adding the flag `--no-svgo true` and point it towards your SVGO configuration file:\n\n```json\n// package.json\n{\n // ...\n \"scripts\": {\n     // ...\n    \"convert-svg\": \"svgr -d ./src/components/Icon ./src/assets/icons --no-svgo true --svgo-config .svgo.yml\"\n }\n}\n```\n\nBy running the `convert-svg` script you both optimize and convert all the SVG files in `src/assets/icons` to React components based on optimized SVGs.\n\n## Reading further\n\nThe examples in this post are the tip of the metaphorical iceberg on what problems SVGO and SVGR can solve. There are many other features you can enable, such as using them as Node.js modules or enabling TypeScript support. To read further make sure to have a look at the SVGR [playground](https://react-svgr.com/playground/) or [documentation](https://react-svgr.com/docs/getting-started/).\n","meta":{"title":"Optimizing, Converting And Exporting SVG Icons In React","description":"If you've ever build a component library, you probably dealt with optimizing and converting icons before. With SVGO and SVGR you can do this at scale.","createdAt":"Thu Dec 10 2020 01:00:00 GMT+0100 (Central European Standard Time)","coverImage":"images/optimizing-converting-and-exporting-svg-icons-in-react.jpg","tags":"React, component library","author":"Roy Derks","slug":"blog/optimizing-converting-and-exporting-svg-icons-in-react","formattedDate":"December 10, 2020","date":"Thu Dec 10 2020 01:00:00 GMT+0100 (Central European Standard Time)"}},{"content":"\nHere at Vandebron, we have several projects which need to compute large amounts of data. To achieve acceptable results, we had to choose a computing tool that should have helped us to build such algorithms.\n\nAs you may have read in other articles our main backend language is Scala so the natural choice to build distributed parallel algorithms was indeed Spark.\n\n## What is Spark\n\nWe will briefly introduce Spark in the next few lines and then we will dive deep into some of its key concepts.\n\nSpark is an ETL distributed tool. ETL are three phases that describe a general procedure for moving data from a source to a destination.\n\n![ETL Diagram](/images/etlprocess.png \"ETL\")\n\n- **_Extract_** is the act of retrieving data from a data source which could be a database or a file system.\n- **_Transform_** is the core part of an algorithm. As you may know, functional programming is all about transformation. Whenever you write a block of code in Scala you go from an initial data structure to a resulting data structure, the same goes with Spark but the data structures you use are specific Spark structures we will describe later.\n- **_Load_** is the final part. Here you need to save (load) the resulting data structure from the transformation phase to a data source. This can either be the same as the extract phase or a different one.\n- **_Distributed_**: Spark is meant to be run in a cluster of nodes. Each node runs its own JVM and every Spark data structure can/should be distributed among all the nodes of the cluster (using serialization) to parallelize the computation.\n\n### Spark data structure: RDD, DataFrame, and Dataset\n\nThe core of Spark is its _distributed resilient dataset (RDD)_.\n\n![Spark API history](/images/sparkapihistory.png \"Spark API history\")\n\nAn **_RDD_** is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. _Extracting_ data from a source creates an RDD. Operating on the RDD allows us to _transform_ the data. Writing the RDD _loads_ the data into the end target like a database for example). They are made to be distributed over the cluster to parallelize the computation.\n\nA **_DataFrame_** is an abstraction on top of an RDD. It is the first attempt of Spark (2013) to organize the data inside and RDD with an SQL-like structure. With dataframe, you can actually make a transformation in an SQL fashion. Every element in a dataframe is a Row and you can actually transform a dataframe to another by adding or removing columns.\n\nA **_DataSet_** finally is a further abstraction on top of a dataframe to organize data in an OO fashion (2015). Every element in a dataset is a case class and you can operate transformation in a scala fashion from a case class to another.\n\n## Spark in action\n\nLet’s see now some code samples from our codebase to illustrate in more detail each of the ETL phases.\n\n### Extract\n\nThe extraction phase is the first step in which you gather the data from a datasource.\n\n```scala\nval allConnections = sparkSession\n.read\n.jdbc(connectionString, tableName, props)\n\nval selectedConnections = allConnections\n.select(ColumnNames.head, ColumnNames.tail: _*)\n\nval p4Connections = selectedConnections\n.filter(allConnections(\"HasP4Day activated\").equalTo(1))\n.filter(allConnections(\"HasP4INT activated\").equalTo(1))\n.as[Connection]\n\np4Connections.show()\n```\n\nFor most people the extraction phase is just the first line (the invocation to the read method), they are not wrong because extracting means reading data from a datasource (in this case an SQL server database). I decided to include in this phase also some filtering and projection operations because I think these are not really part of the algorithm, this is still the preparation phase before you actually process the data. We can ultimately say that _preparing the data_ is something in between extraction and transformation therefore it is up to you to decide which phase it belongs to.\n\n### Transform\n\nTransformation phase is the core of the algorithm. Here you actually process your data to reach your final result.\n\n```java scala\nusageDF\n.groupBy('ConnectionId, window('ReadingDate, \"1 day\"))\n.agg(\n    sum('Consumption).as(\"Consumption\"),\n    sum('OffPeak_consumption).as(\"OffPeak_consumption\"),\n    sum('Peak_consumption).as(\"Peak_consumption\"),\n    sum('Production).as(\"Production\"),\n    sum('OffPeak_production).as(\"OffPeak_production\"),\n    sum('Peak_production).as(\"Peak_production\"),\n    first('ReadingDate).as(\"ReadingDate\"),\n    first('marketsegment).as(\"marketsegment\"),\n    collect_set('Source).as(\"Sources\"),\n    collect_set('Tag).as(\"Tags\"),\n    max('Last_modified).as(\"Last_modified\")\n)\n.withColumn(\n    \"Tag\", when(array_contains('Tags, “Interpolated”),\nlit(Tag.Interpolated.toString)).otherwise(lit(“Measured”)))\n.withColumn(\"Source\",\nwhen(size('Sources) > 1,\nlit(Source.Multiple.toString)).otherwise(mkString('Sources)))\n.orderBy('ConnectionId, 'ReadingDate)\n.drop(\"window\", \"sources\", \"tags\")\n```\n\nIn this specific example, we are processing connection usage data by aggregating it daily. In the `usageDF` we have 15 minutes interval usage data, now we want to show to the user the same data but with a different aggregation interval (1 day). So we group the whole data by connection id and window the reading date by 1 day (A window function calculates a return value for every input row of a table based on a group of rows [Introducing Window Functions in Spark SQL - The Databricks Blog](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html).\n\nOnce the data is grouped we can aggregate it, using the `agg` method which allows us to call the aggregation functions over the dataframe (for example: `sum`, `first`,`max` or `collect_set`). Successively we transform the dataframe to suit our visualization needs, the methods used are self-explanatory and the documentation is very clear. [Getting Started - Spark 3.0.1 Documentation](https://spark.apache.org/docs/latest/sql-getting-started.html)\n\n### Load\n\nThe final phase is the one which `save`, `put`, `show` the transformed data into the target data source.\n\n```java scala\ndataFrame\n.select(columns.head, columns.tail: _*)\n.write\n.cassandraFormat(tableName, keySpace)\n.mode(saveMode)\n.save()\n```\n\nIn this specific case, we will save our dataframe into a Cassandra database. In Spark, methods used to achieve the load phase are called _actions_. It is very important to distinguish Spark actions from the rest because actions are the only ones that trigger Spark to actually perform the whole transformation chain you have defined previously.\n\nIf our transformation phase, as we described above, wasn’t followed by an action (for example `save`) nothing would have happened, the software would have simply terminated without doing anything.\n\n## One concept to rule them all\n\n```java scala\nval rdd1 = sc.parallelize(1 to 10)\nval rdd2 = sc.parallelize(11 to 20)\nval rdd2Count = rdd1.map(\nx => rdd2.values.count() * x //This will NEVER work!!!!\n)\n```\n\n_One does not simply use RDD inside another RDD_. (Same goes for Dataframes or Datasets).\n\nThis is a very simple concept that leads very often to lots of questions because many people just want to use Spark as a normal scala library. But this is not possible due to the inner distributed nature of Spark and its data structures. We have said that an RDD is a resilient distributed dataset, let’s focus on the word _distributed_, it means that the data inside it is spread across the nodes of the cluster. Every node has its own JVM and it is called _Executor_, except for the master node where your program starts which is called _Driver_:\n\n![Spark cluster overview](/images/spark-cluster-overview.png \"Spark cluster overview\")\n\nYour code starts from the Driver and a copy is distributed to all executors, this also means that each executor needs to have the same working environment of the Driver, for Scala it is not a problem since it just needs a JVM to run. (but we will see that if you use _pySpark_ you need to take extra care when you distribute your application.) Every Spark data structure you have defined in your code will also be distributed across the executors and every time you perform a transformation it will be performed to each chunk of data in each executor.\n\nNow let’s go back to our example, a `map` is a transformation on `rdd1` this means that block inside will be executed at the executor level, if we need `rdd2` to perform this block Spark should somehow serialize the whole `rdd2` and send it to each executor. You can understand now that _it is really not possible to serialize the whole RDD since it is by its nature already a distributed data structure_. So what can you do to actually perform such computation we showed in the example? The solution is “simple”: _prepare your data in such a way that it will be contained in one single RDD_. To do so you can take advantage of all the transformation functions Spark has to offer such `map` `join` `union` `reduce` etc.\n\n## Next step…\n\nWe have explained all the main concepts of Spark and we have shown some real snippets of our codebase. In the next article, I would like to show you a real-life problem we have solved in our company using [_pySpark_](https://spark.apache.org/docs/latest/api/python/index.html). I will show you how to customize Spark infrastructure to correctly parallelize the ETL algorithm you have built.\n","meta":{"title":"Fueling the Energy Transition With Spark - Part 1","description":"Our main backend language is Scala, and by using Spark we build distributed parallel algorithms to fuel the Energy Transition. But why is Spark the best choice for that job?","createdAt":"Wed Nov 04 2020 01:00:00 GMT+0100 (Central European Standard Time)","coverImage":"images/fueling-the-energy-transition-with-spark-part-1.jpg","imageSource":"https://www.pexels.com/photo/shallow-focus-photography-of-light-bulbs-2764942","tags":"spark, scala","author":"Rosario Renga","slug":"blog/fueling-the-energy-transition-with-spark-part-1","formattedDate":"November 4, 2020","date":"Wed Nov 04 2020 01:00:00 GMT+0100 (Central European Standard Time)"}},{"content":"\nAt Vandebron we organize a two-day long Hackathon every quarter, and a colleague and I took this chance to dig into the wonderful world of GraalVM.\n\nI've first heard of GraalVM around two years ago when Oleg Šelajev toured through Java User Groups in Germany and held talks about GraalVM. [Here](https://www.youtube.com/watch?v=GinNxS3OSi0) is one from 2019 (not Germany, but Spain this time).\n\nGraalVM promises a significant speedup in compile times and as I am working with Scala, which is notoriously known for its long compile times, this seems interesting. Furthermore, GraalVM provides functionality to build native executables. Meaning, an application can be run without a Java Virtual Machine (JVM).\n  \nThanks to the Hackathon I finally took the time to get to know GraalVM a bit better. With this blog post, I want to share our findings, experiences, and results, as they might be helpful for you too!\n\n## What is GraalVM?\n\nGraalVM is a high-performance JVM that supports efficient ahead-of-time (AOT) and just-in-time (JIT) compilation, but also allows non-JVM languages (e.g. Ruby, Python, C++) to run on the JVM. The ahead-of-time compilation feature is the base for creating native executable programs, meaning an application can be run independently from the JVM. Seeing the versatile features of GraalVM, it is worth looking a bit under its hood.\n\nActually, GraalVM is defined by three main technologies:\n\n- [Graal compiler](https://www.graalvm.org/reference-manual/jvm/), a high-performance JIT-compiler that can make JVM applications run faster from within the JVM\n- [SubstrateVM](https://www.graalvm.org/reference-manual/native-image/SubstrateVM/), includes the necessary components to run a JVM-app as a native executable ( Garbage Collector, Thread Scheduler, etc.)\n- [Truffle Language Implementation Framework](https://www.graalvm.org/graalvm-as-a-platform/language-implementation-framework/), the basis for the polyglot support from GraalVM\n\nOur motivation for trying out GraalVM was tackling the pain points of Scala, Java projects, and microservices. Shipping microservices written in Scala as Docker containers to your production system comes with the cost that startup can be a bit slow, having JVM and Docker overhead, and that those containers can be fairly large, as the application can only be run with a JVM. See [Building Docker images](#building-docker-images) for more information.\n\nDuring the hackathon, we were most interested in building native images for Scala applications. Hoping to reduce the size of our docker containers and reducing up the startup time.\n\n## Project setup\n\nThe project we worked on during the Hackathon is an API that should be used for applicants to submit their applications at Vandebron in the future. By exposing one endpoint through which a resume and contact information can be submitted.\n\nIt is also a good project to test out GraalVM, nothing too complex but also not as simple as \"Hello World\".\n\nThe full setup can be found [on Github](https://github.com/kgrunert/apply-at-vdb). But I'll summarise the used stack below. The project is built around the following libraries, no particular reason, simply because I like them.\n\n- _cats_ for working with effects, such as IO\n- _http4s_ for running the server\n- _tapir_ for defining the endpoints\n- _circe_ for JSON de/serialisation\n- _pureconfig_ for reading config-files\n- _logback_ for logging\n\nThe project can be run via `sbt run` and with Postman or similar a POST-request can be sent like so:\n\n```json\nPOST localhost:8080/api/v1/apply\n\n{\n\t\"email\": \"my@email.de\",\n\t\"name\": \"My Name\",\n\t\"phoneNumber\": \"+310123456789\",\n\t\"applicationBase64\": \"VGhpcyBjb3VsZCBiZSB5b3VyIGFwcGxpY2F0aW9uIQ==\"\n}\n\nResponse:\n\"*confetti* Thanks for handing in your application, we will get back to you within the next days! *confetti*\"\n```\n\n## Setup GraalVM with sbt\n\nWith this initial project setup in mind, GraalVM needs to be installed locally.\n\nFor the installation of GraalVM the [setup guide](https://www.graalvm.org/docs/getting-started-with-graalvm/#install-graalvm) can be followed.\n\nAfter the installation sbt needs to know that not the regular JDK/JVM is used. This can be done with the `java-home` option on sbt bootup.\nTo make the path to GraalVM a bit more accessible and easy to use it can be exported as an environment variable.\n\n```bash\nexport GRAAL_HOME=/Library/Java/JavaVirtualMachines/graalvm-ce-java8-20.1.0/Contents/Home\nsbt -java-home $GRAALHOME\n```\n\nThe path to GraalVM can vary depending on OS and installation. We followed the basic installation for macOS.\n\nNow sbt using GraalVM can be verified with:\n\n```bash\nsbt -java-home $GRAALHOME\nscala> eval System.getProperty(\"java.home\")\n[info] ans: String = /Library/Java/JavaVirtualMachines/graalvm-ce-java8-20.1.0/Contents/Home/jre\n```\n\nThat means everything running in this sbt instance is getting compiled by GraalVM. Awesome!\n\nThe next step is to become strong and independent and learn how to run without an underlying JVM with the help of building native images.\n\n## Building native images\n\nGraalVM ships with the [GraalVM Updater](https://www.graalvm.org/reference-manual/graalvm-updater/) (`gu`) to install the `native-image` on your machine.\n\n```bash\n$GRAALHOME/bin/gu install native-image\n```\n\n[sbt-native-packager](https://sbt-native-packager.readthedocs.io/en/latest/) provides functionality to build packages efficiently (e.g. building Docker images) and added to that, it also provides support for building native images.\nIn order to build native images with sbt commands this plugin has to be added to the project:\n\n```java scala\n// inside project/plugins.sbt\naddSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.7.3\")\n```\n\nAnd the `GraalVMNativeImagePlugin` needs to be enabled:\n\n```java scala\n// inside build.sbt\nenablePlugins(GraalVMNativeImagePlugin)\n```\n\nFrom within sbt it should be able to autocomplete and suggest graal-commands, e.g.:\n\n```java scala\nsbt:apply-at-vdb> graalvm\ngraalvm-native-image:       graalvmNativeImageOptions\n```\n\nWith that setup, native images are just a stone's throw away!\n\n---\n\n### Disclaimer\n\nThe next three sections are not a write-up but rather the main steps we had to take to make the project work. This includes failing images and troubleshooting.\nI want to keep this in because it might be interesting for others when they have to troubleshoot.\nFor the summary and happy path, you can jump directly to [Roundup](#roundup).\n\n---\n\n### First try building a native image\n\nNext up `graalvm-native-image:packageBin` can be run from within sbt. This might take a while (on our systems it took about a minute)\n\nSome warnings start to pop up:\n\n```\n[error] warning: unknown locality of class Lnl/vandebron/applyatvdb/Main$anon$exportedReader$macro$24$1;, assuming class is not local. To remove the warning report an issue to the library or language author. The issue is caused by Lnl/vandebron/applyatvdb/Main$anon$exportedReader$macro$24$1; which is not following the naming convention.\n\n[error] warning: unknown locality of class Lfs2/internal/Algebra$Done$2$;, assuming class is not local. To remove the warning report an issue to the library or language author. The issue is caused by Lfs2/internal/Algebra$Done$2$; which is not following the naming convention.\n```\n\nThe library-specific warnings can be ignored for now. Ultimately it fails with:\n\n```\nError: com.oracle.graal.pointsto.constraints.UnresolvedElementException:\nDiscovered unresolved type during parsing: org.slf4j.impl.StaticLoggerBinder.\nTo diagnose the issue you can use the --allow-incomplete-classpath option.\nThe missing type is then reported at run time when it is accessed the first time.\n```\nActually a good hint on where to start fine-tuning the GraalVM config:\n\n```java scala\n// inside build.sbt\ngraalVMNativeImageOptions ++= Seq(\n\t\"--allow-incomplete-classpath\",\n)\n```\n\nSome things like a `StaticLoggerBinder` only get resolved at runtime, meaning at build time the classpath needs to be allowed to be incomplete. This option allows resolution errors to be ignored at build time and only pop up during runtime.\n\nDuring the build of a native image, GraalVM tries to resolve those runtime dependencies already at compile-time, as it is part of the Ahead-Of-Time-compilation process. With this flag, GraalVM knows \"hey, don't worry about it now, we cross the bridge when we get there\" (or something like that).\n\n### Adding resource files\n\nA `reload` (or restart) of sbt is needed to activate these new options. And we can try to build the native image up new.\nThis time the build finished successfully and the executable file `target/graalvm-native-image/apply-at-vdb` has been created!\nThis is an executable that can be run without a JVM:\n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n```\n\nBut what's that? It actually cannot be started...\n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n*** An error occured! ***\nCannot convert configuration to a de.erewl.pricetracker.server.Config. Failures are:\nat the root:\n- Key not found: 'host'.\n- Key not found: 'port'.\n```\n\nThe first three lines relate to the error that occurred during the first build. It simply says that logging hasn't been set up correctly (maybe due to the absence of a `src/main/resources/logback.xml` or some other misconfiguration), triggering the default setting of not logging anything at all.\nThe second error states that a configuration file does not have the right keys or cannot be found at all.\nLooking into `src/main/resources`:\n\n```bash\nls src/main/resources/\napplication.conf logback.xml\n```\n\nand peeking into `application.conf`:\n\n```bash\ncat src/main/resources/application.conf\n\thost = \"localhost\"\n\tport = 8080\n```\n\nHm, so everything is actually in place. But somehow GraalVM can't find those files.\nIt still requires some more GraalVM fine-tuning here.\n\nBy default, GraalVM doesn't include any resource or configuration-files.\nThe option `-H:ResourceConfigurationFiles=path/to/resource-config.json` defines a path to a JSON configuration file. So inside the `resource-config.json` we can include our `application.conf` and our `logback.xml`.\n\nBut writing those config files can be tedious and it is difficult in larger projects to find all necessary classes that need to be included. GraalVM provides some support with writing those files and actually does all the work. In the project's root directory a configs-folder can be created which will contain all necessary config-files.\n\nFor writing the configuration files we will build a normal JAR-file with the help of the `sbt-assembly` plugin. Adding it to the project like so:\n\n```java scala sbt\n  // inside project/plugins.sbt\n  addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\")\n```\n\nThe JAR-file will be built with `sbt assembly`.\n\nWith that we can now start the application, providing the path to the JAR-file that just has been created:\n\n```bash\nmkdir configs\n$GRAALHOME/bin/java -agentlib:native-image-agent=config-output-dir=./configs -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n```\n\nWith the command above the JAR gets to run with GraalVM but adds [dynamic lookups](https://www.graalvm.org/reference-manual/native-image/Configuration/#assisted-configuration-of-native-image-builds) that are being intercepted during runtime and written to the files: `jni-config.json`, `proxy-config.json`, `reflect-config.json` and `resource-config.json`.\n\nThose generated files can be included in the GraalVMNativeImageOptions:\n\n```java scala\n// build.sbt\ngraalVMNativeImageOptions ++= Seq(\n\t\"--allow-incomplete-classpath\",\n\t\"-H:ResourceConfigurationFiles=../../configs/resource-config.json\",\n\t\"-H:ReflectionConfigurationFiles=../../configs/reflect-config.json\",\n\t\"-H:JNIConfigurationFiles=../../configs/jni-config.json\",\n\t\"-H:DynamicProxyConfigurationFiles=../../configs/proxy-config.json\"\n)\n```\n\nThe build with those updated options should succeed and the app can be run once again: \n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n```\n\nStill no logging, sadly. But the server is actually running and responds to POST requests via its exposed endpoint:\n\n```json\nPOST localhost:8080/api/v1/apply\n\n{\n\t\"email\": \"my@email.de\",\n\t\"name\": \"My Name\",\n\t\"phoneNumber\": \"+310123456789\",\n\t\"applicationBase64\": \"VGhpcyBjb3VsZCBiZSB5b3VyIGFwcGxpY2F0aW9uIQ==\"\n}\n\nResponse:\n\"*confetti* Thanks for handing in your application, we will get back to you within the next days! *confetti*\"\n```\n\nThe next and last step will investigate why logging is not picked up by GraalVM.\n\n### Investigating the missing logging\n\nSo first I wanted to have a look if it was an overall issue with logging. I stepped back from using logging-framework and tried the most basic logging with the java-integrated `java.util.Logging`. GraalVM's [docs](https://www.graalvm.org/docs/Native-Image/user/LOGGING) stated that GraalVM supports any logging that depends on that.\n\nBuilding and running the native-image with `java.util.Logging` instead of `logback` succeeded and everything is logged properly.\n\nSo it must be something with the dependencies?\n\nFor further investigation, I added the [sbt-dependency-graph](https://github.com/jrudolph/sbt-dependency-graph) plugin and checked out the dependency-tree with `sbt dependencyBrowserTree`. The library `logback` wasn't included in the dependency tree.\nWhich is odd, since `logback` is clearly present in the project's library-dependencies.\n\n```java scala\n// inside build.sbt\nlibraryDependencies ++= Seq(\n\t...\n\t\"ch.qos.logback\" % \"logback-classic\" % \"1.2.3\" % Runtime,\n\t\"ch.qos.logback\" % \"logback-core\" % \"1.2.3\" % Runtime,\n\t...\n)\n```\n\nHaving a closer look, the appendix `% Runtime` on logback's dependency is present.\n\nNot sure where this was coming from but it is most probably blindly copy-pasted from somewhere when gathering the dependencies for this project.\n\n[sbt reference manual](https://www.scala-sbt.org/1.x/docs/Scopes.html#Scoping+by+the+configuration+axis) states that the appendix `Runtime` defines that this dependency will be only included in the runtime classpath.\n\nSo this explains probably why logging was only working when the server was run from inside sbt.\n\nWith removing this and building the native-image, `logback` appears in the dependency-tree, and logging works when the native image is executed!\n\nThis \"bug\" was interesting as it emphasized what GraalVM can NOT do for you. Dynamic class loading/linking can not be supported by GraalVM as classes and dependencies have to be present during compile time to make a fully functional application. \n\n### Roundup\n\nA successful setup of sbt and GraalVM to build native-images requires to:\n\n- install GraalVM's native-image functionality via it's graal-updater: \n  ```bash\n  gu install native-image\n  ```\n- add sbt-native-packager and sbt-assembly to sbt:\n  ```java scala sbt\n  // inside project/plugins.sbt\n  addSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.7.3\")\n  addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\")\n  ```\n- enable the GraalVM-Plugin:\n  ```java scala sbt\n  // inside build.sbt\n  enablePlugins(GraalVMNativeImagePlugin)\n  ```\n- create a fat JAR and define which resource and configuration files should be intergated by intercepting look up calls during its execution:\n  ```bash\n  sbt assembly\n  mkdir configs\n  $GRAALHOME/bin/java -agentlib:native-image-agent=config-output-dir=./configs -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n  ```\n- fine-tune GraalVM with the following options and include the files that have been created in the previous step:\n  ```java scala\n  // build.sbt\n  graalVMNativeImageOptions ++= Seq(\n    \"--allow-incomplete-classpath\",\n    \"-H:ResourceConfigurationFiles=../../configs/resource-config.json\",\n    \"-H:ReflectionConfigurationFiles=../../configs/reflect-config.json\",\n    \"-H:JNIConfigurationFiles=../../configs/jni-config.json\",\n    \"-H:DynamicProxyConfigurationFiles=../../configs/proxy-config.json\"\n  )\n  ```\n- build the native image with:\n  ```bash\n  sbt graalvm-native-image:packageBin\n  ```\n- run the executable file without the need of java\n  ```\n  ./target/graalvm-native-image/apply-at-vdb\n  ```\n\nEven without benchmarking, you notice that the startup time is way faster than with a traditional JAR-file and the application is up and running almost instantly.\n\nIt is worth noting that the creation of a native image is a quite time-consuming process. For this project, it took between 1 and 2 minutes. This is, of course, something a CI/CD-Server like Jenkins would take care of but it has to be kept in mind. \n\nWith a working native-image, it is time to dockerize.\n\n## Building Docker images\n\nIn this section two Docker containers will be built. One, following the \"normal\"-java way and the other will be using the native-image to build a Docker-container without Java.\n\nBefore getting started with native images, a regular JAR-file and Docker image for comparison can be built.\n\nWith the [sbt-assembly](https://github.com/sbt/sbt-assembly) plugin you can create JAR-files with all of its dependencies (fat JARs).\n`sbt assembly` creates this `target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar` which has a size of around 42MB:\n\n```shell\n sbt assembly \n ls -lh target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n\n  ...  ...   42M   target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n```\n\nThis application can be run locally via `java -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar` with the prerequisite that Java is installed on that machine.\n\nCreating the Docker image for this JAR-file can be done manually, but luckily `sbt-native-package` supports building regular Docker images out of the box, only the `DockerPlugin` needs to be enabled:\n\n```java scala\n// build.sbt\nenablePlugins(DockerPlugin)\n```\n\n`sbt docker:publishLocal` creates the Docker image `apply-at-vdb`.\n \n```shell\ndocker images | grep apply-at-vdb\n  apply-at-vdb \t0.1.0-SNAPSHOT \t\tf488d4c06f28 \t555MB\n```\n\nA whopping 555MB for a tiny app exposing one endpoint which JAR-file was only 42MB. But to run this JAR-file in a container, this container needs to ship with a JVM, and that's where the overhead lies.\n\nWith that Docker image and JAR-file as a reference, we can now look into how the native-image operates together with Docker.\n\nGraalVM does not support cross-building, meaning an application cannot be expected to be built in a MacOS environment and run in a Linux environment. It has to be built and run on the same platform. With the help of Docker, the desired built environment can be provided.\nThe `Dockerfile` looks as follows:\n```docker\nFROM oracle/graalvm-ce AS builder\nWORKDIR /app/vdb\nRUN gu install native-image\nRUN curl https://bintray.com/sbt/rpm/rpm > bintray-sbt-rpm.repo \\\n\t&& mv bintray-sbt-rpm.repo /etc/yum.repos.d/ \\\n\t&& yum install -y sbt\nCOPY . /app/vdb\nWORKDIR /app/vdb\nRUN sbt \"graalvm-native-image:packageBin\"\n\nFROM oraclelinux:7-slim\nCOPY --from=builder /app/vdb/target/graalvm-native-image/apply-at-vdb ./app/\nCMD ./app/apply-at-vdb\n\n```\n\nAnd can be run with:\n```bash\ndocker build -t native-apply-at-vdb .\n```\nThe Dockerfile describes to do the following:\nThe first docker container, as the name implies, is the builder. As a base image the official [GraalVM image](https://hub.docker.com/r/oracle/graalvm-ce) is used. \n\nThis image needs two more things, GraalVM's native-image command, and sbt, and this is what the two follow-up rows are providing. Once that's done, the project is copied into this container and the native image is built from within sbt.\n\nThe next steps bring the native executable into its own docker container.\nAs a base image, we use an Oracle Linux image and from our builder-container, we copy the native executable to this new container. The last step is that the app gets run on container startup.\n\n`docker run -p 8080:8080 -it native-apply-at-vdb` starts the container and shows that everything is working just as before.\n\nBut what about the image size? Let's have a look.\n```\ndocker images | grep apply-at-vdb\n  native-apply-at-vdb\t\tlatest              17b559e78645\t\t199MB\n  apply-at-vdb\t\t\t0.1.0-SNAPSHOT      f488d4c06f28\t\t555MB\n```\nThat is impressive! We created an app that is approx. 2.8 times smaller than our original app.\n\n## Summary\n\nWe learned how to set up a Scala project with GraalVM, what steps have to be taken to build a native image with GraalVM, and let it run inside a Docker container. We also received a good overview of what's possible with GraalVM and what's not.\n\nThe initial start and setup of GraalVM with sbt is pretty easy and straightforward. Getting GraalVM to compile an sbt project is nice and simple. \n\nThis Hackathon showed us that it is difficult and requires a lot of fine-tuning to integrate GraalVM into an existing project or product. At Vandebron we work with a complex stack of technologies including Spark, Kafka, and Akka which made it difficult to port the findings from this small toy service to one of our existing microservices. This made extensive troubleshooting in the Hackathon not possible.\n\nAll in all, GraalVM allows you to give up some Java overhead and create significant smaller Docker images. Sadly, this comes at the cost of giving up dynamic linking and class loading. \nA silver lining is, that inside Scala's ecosystem this rarely a problem. Scala relies heavily on compile-time mechanisms for detecting bugs early and creating type-safe applications (read [here](https://blog.softwaremill.com/small-fast-docker-images-using-graalvms-native-image-99c0bc92e70b) but also see e.g. [Scala's compiler phases](https://typelevel.org/scala/docs/phases.html)).\n\n* * *\n\n## Sources and Reading\n- [Building Serverless Scala Services with GraalVM](https://www.inner-product.com/posts/serverless-scala-services-with-graalvm/) by Noel Welsh\n- [Small & fast Docker images using GraalVM’s native-image](https://blog.softwaremill.com/small-fast-docker-images-using-graalvms-native-image-99c0bc92e70b) by Adam Warski\n- [Run Scala applications with GraalVM and Docker](https://medium.com/rahasak/run-scala-applications-with-graalvm-and-docker-a1e67701e935) by @itseranga\n- [Getting Started with GraalVM and Scala](https://medium.com/graalvm/getting-started-with-graalvm-for-scala-d0a006dec1d1) by Oleg Šelajev\n- [Updates on Class Initialization in GraalVM Native Image Generation](https://medium.com/graalvm/updates-on-class-initialization-in-graalvm-native-image-generation-c61faca461f7) by \nChristian Wimmer\n- [GraalVM's Reference Manuals](https://www.graalvm.org/reference-manual/)\n","meta":{"title":"Building native images and compiling with GraalVM and sbt","description":"At Vandebron we organized a two-day long Hackathon, a colleague and I took the chance to dig into the wonderful world of GraalVM.","createdAt":"Tue Oct 06 2020 02:00:00 GMT+0200 (Central European Summer Time)","coverImage":"images/building-native-images-and-compiling-with-graalvm-and-sbt.jpg","imageSource":"https://pixabay.com/users/lumix2004-3890388/","tags":"graalvm, scala","author":"Katrin Grunert","slug":"blog/building-native-images-and-compiling-with-graalvm-and-sbt","formattedDate":"October 6, 2020","date":"Tue Oct 06 2020 02:00:00 GMT+0200 (Central European Summer Time)"}},{"content":"\nTwo months ago, I started my journey at Vandebron. One of the projects I first dove into was their efforts to build a [component library](https://windmolen.netlify.app/). Something I was already familiar with from previous companies I worked at. \n\nOn the internet, you can find many articles that describe why a reusable component library is a good investment for your development team(s). Although there's much to say about the advantages of component libraries, most articles don't state the (obvious) disadvantages such projects can have. In this post, I'll point out some of our learnings and why you might not need such a reusable component library.\n\n## About component libraries\n\nOften you find yourself repeating the same lines of code to make, for example, a button or the layout of a page look nice, especially when you're working on multiple projects. Or as a designer, you get frustrated every time the styling for a part of the application is off when a new page or project is created. Many companies have already found multiple solutions to preventing themselves from repeating styling, which is the main reason for design inconsistencies. And therefore component libraries were created.\n\nA component library is a collection of all the styled parts (or components) of a website or multiple websites that make it easier for developers to reuse these parts. Also, designers will know for sure that all components in the component library adhere to their designs, and therefore all projects that use these components will conform. Often these libraries consist of different layers of components, for example, offering atoms, molecules, and organisms when an [Atomic Design](https://bradfrost.com/blog/post/atomic-web-design/) pattern is applied. Following this pattern, developers can use the parts to style their templates and pages consistently.\n\nComponent libraries are becoming more and more popular with the rise of JavaScript libraries and frameworks like React and Vue. These technologies are very suitable for quickly building interactive components that you can use in your application, and can easily be exposed as a library on NPM or Github Packages. At Vandebron, we're building all our web and mobile applications with React and React Native and are using [Storybook](https://storybook.js.org/) to develop our components in a shared library between the engineering and design teams. This can potentially create a lot of advantages for both the developers and designers, as you can read below.\n\n## Why you *might* need a component library\n\nBefore deciding to create a component library for your team or company, you probably want to hear about the advantages such a project can lead to. The main advantages of component libraries are briefly mentioned in the first section above and are often defined as:\n\n- **Reducing code duplication**: With a component library, you can create components that can be shared across multiple websites or applications. This way you no longer have to duplicate styling in different projects. This can seriously decrease the amount of code duplication that you have in your projects, also reducing the number of bugs or design inconsistencies.\n\n- **Preventing design inconsistencies**: By adding all your components and styled parts to the component library you're certain that these will look the same on all the places they're used. Not only will all the components look the same on every page, when designers make a change to one of these components they can be easily updated on all the places they're used.\n\n- **Easier collaborating**: Component libraries make it easier for developers and designers to collaborate on applications and designs, with the component library as the common \"playground\". By using a tool, like Storybook, you can also make this playground visible to non-technical people and show what components are already available to use for new features.\n\nBut these advantages come at a certain price, as I'll explain in the next section.\n\n## Disadvantages of component libraries\n\nBesides the obvious advantages of a component library, it can also have serious disadvantages that are listed below. Whether or not these disadvantages apply to you depends on numerous things that are discussed later on in this article.\n\n- **Increasing complexity**: With all attempts to make code more generic,  an increased level of complexity also comes to play. Reusable components should be easy to extend or customize, which requires you to think about the different use cases beforehand or force you to add many different variations to a component. With every new project that starts to use the component library, you get the risk of increasing the complexity of the library even more.\n\n- **Time-consuming**: Every time you want to add a component to your project, you need to create that component in the component library first and import it locally in the project to test it. Therefore you need to be working in multiple projects at the same time, which requires you to set up a more time-consuming workflow. Also, when you want to use this new component from the library, you have to publish a new version of the library to make the component available.\n\n- **Conflicting dependencies**: When you're using different versions of dependencies across your projects and the component library, you're forced to sync those with each other. Imagine having, for example, an older version of React running in one of your projects that doesn't use a recent React API that you want to use in your component library. In this scenario, you either have to update that project or are unable to keep your component library on par with the latest release of your dependency on React. Both solutions have pros and cons, and would rather be avoided.\n\nAs mentioned before, there are reasons why these disadvantages might apply to you that are the team size, the number of teams and projects at the company, development or release lifecycles, and how your source code is organized. It clearly doesn't make sense to invest in a component library if you have just a small amount of people work on just one project, or a sole team is working on all the different projects making it easier to manage code duplication or design inconsistencies.\n\n## Considerations before starting\n\nThere are two main alternatives that you need to take into consideration before building a reusable component library, which is (obviously) using or extending an existing component library or sourcing your code in a monorepo. \n\n- **Existing component libraries:** Using an existing component library is an efficient way to create consistently (web) pages and reduce the amount of complexity of your own project, while also taking advantage of best practices of large open-source projects. Popular examples of component libraries are [Ant Design For React](https://ant.design/docs/react/introduce) or [various implementations](https://material.io/develop) for Google's Material Design. These libraries allow you to move quickly without having all the overhead of creating complex components but limit you to the design guidelines of these component libraries.\n\n- **Monorepo:** If you don't want to take advantage of existing libraries or are very keen to apply your own styling to components across multiple applications without having to copy-paste the code, you can host the source code of applications in a monorepo. With the monorepo approach, you can create a shared folder that includes all the components used by your applications. This makes it possible to apply changes with a simple pull request and import these components from every project in that repository.\n\nBesides these two alternatives, you also need to have proper design guidelines set by your designer(s). When the design guidelines are flexible and fluctuating, you could be structuring components incorrectly with the risk of doing a lot of work that will be omitted once the project evolves.\n\n## To summarize\n\nComponent libraries are a great way to reduce the amount of code duplication in your applications, prevent design inconsistencies, and increase collaborations between developers, designers, and different teams. But this comes with increased complexity, slower development cycles, and possible code conflicts between projects. Therefore you should consider if using an existing component library or having a monorepo for your source code is a workable solution. At Vandebron we decided to build our own component library (called [windmolen](https://windmolen.netlify.app/)) and if you'd decide the same, then be sure that your design guidelines are properly structured and mature enough.\n","meta":{"title":"When (Not) To Build A Reusable Component Library","description":"You can find much information on why a reusable component library is a good investment, but most articles don't state the (obvious) disadvantages..","createdAt":"Mon Oct 05 2020 02:00:00 GMT+0200 (Central European Summer Time)","coverImage":"images/when-not-to-build-a-reusable-component-library.jpg","imageSource":"https://pixabay.com/users/stevepb-282134/","tags":"React, component library","author":"Roy Derks","slug":"blog/when-not-to-build-a-reusable-component-library","formattedDate":"October 5, 2020","date":"Mon Oct 05 2020 02:00:00 GMT+0200 (Central European Summer Time)"}}]},"__N_SSG":true}