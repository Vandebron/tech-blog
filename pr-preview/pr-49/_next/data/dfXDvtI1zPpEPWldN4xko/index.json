{"pageProps":{"posts":[{"content":"\n# Grid City: A Hackathon Journey into the Energy Transition by Vandebron\n\nDuring the last couple of hackathons, we set out to gamify one of the most pressing challenges of our time, the transition to green energy.\nA problem our team, VPP [Virtual Power Plant], deals with on a daily basis.\nWhat emerged was an interactive game designed to educate players about the complexities of energy grid management in a fun and manageable way, \nthough debatable if it is fun or well-balanced. It was a hackathon project after all.\n\n<video autoplay muted loop playsinline controls style=\"max-width: 100%; height: auto;\">\n  <source src=\"../images/game-start-h264_v2.mp4\" type=\"video/mp4;\">\n  Your browser does not support the video tag.\n</video>\n\nLet’s dive into the vision, mechanics, and what we hope players take away from this experience.\n\n### Vision: Educate Through Play\nOur goal was to create more than just a game. We envisioned a tool that serves as an onboarding experience, \nnot just for our colleagues, but also anyone interested in the energy transition.\n\nTo realize this vision we chose to go with Godot as our game engine of choice. \nAn open-source game engine that is a super fun and fantastic tool that allowed us to quickly prototype and iterate on our ideas and feedback.\n\n### The Core Gameplay Loop: Keeping the Grid Balanced\nIn this game, your primary goal is to keep the energy grid frequency within the \"goldilocks zone\", i.e. not too much power production, not too little. \nThe longer you can maintain this balance, the better!\n\n#### Key Concepts:\n- **Keeping within the goldilocks zone:** energy production must match energy demand. Fall outside the goldilocks zone, and the grid becomes unstable. If it is unstable for too long, then it's game over.\n- **Keeping up with demand:** Houses are automatically built over time, mimicking real-life construction. It’s up to the player to meet this increasing demand by manually adding renewable energy assets, like wind turbines, solar panels, and batteries.\n- **Forecasting:** Players can use weather forecasts to anticipate and plan for energy production challenges. Too much wind? Better disable some wind turbines for the time being.\n- **Curtailment:** Too much energy? Players must decide when to turn off assets to avoid overproduction. This introduces the concept of curtailment and highlights the work often required in grid management, due to not being able to store the excess energy.\n- **The Main Goal:** A 100% Green Energy Grid, 100% of the time.\n\nWhat does success look like in the game? Maintaining the grid with 100% green power for as long as possible, via strategically managing energy production and curtailment to ensure sustainability.\nPlayers experience the dual challenge of meeting growing energy demands and avoiding excess production, a dilemma central to real-world energy grids.\n\n### Final Thoughts\n\nWhile we did meet most of these goals, we definitely see room for improvement. \nFor instance, the city building aspect of the game does not exactly match what we want to teach the player. A Virtual Power Plant mainly balances the grid through smart curtailment. \nIt doesn't involve building new renewables. So we considered removing the building mechanic from the game, but we were already to deep into development.\n\nTo work around this design issue, we toyed with the idea of having a coal plant in the game, amongst others. \nThe goal was to shut down the coal plant before it fully pollutes the planet. \nAfter shutdown, players face the challenge of balancing the grid reliably using just renewable energy. \nUnfortunately, we ran out of time before we could fine-tune this part of the game.\n\n<video autoplay muted loop playsinline controls style=\"max-width: 100%; height: auto;\">\n  <source src=\"../images/coal-plant-prototype.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n</video>\n\n*Here's an early prototype of the coal plant that didn't make it in to the game.*\n\nAt some point, though, you have to wrap up the hackathon. So we polished what we had and declared the game \"done\". \nWhether you’re a gamer, an energy enthusiast, or someone entirely new to the topic, we hope this game sparks your interest in the energy transition :)\n\nYou can play the game right from your browser [here](https://djvisser.itch.io/grid-city)!\nIf you want to take a look at the (hackathon-quality) code, check out our [public repo](https://github.com/Vandebron/vandebron_game).\n\n","meta":{"title":"Grid City: A Hackathon Journey into the Energy Transition by Vandebron","description":"A journey into the energy transition through the lens of a video game","createdAt":"Tue Jan 21 2025 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/game-start-screen.png","tags":["godot","energy transition","gaming"],"author":"Dick Visser & Tomás Phelan","slug":"blog/vandebron-the-video-game","formattedDate":"21 januari 2025","date":"Tue Jan 21 2025 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\n## The Background\n\nWe at Vandebron have a mission to get the news out about [our good work](https://vandebron.nl/missie), and we understand that [Server Side Rendering (SSR)](https://web.dev/articles/rendering-on-the-web#server-side) can really help with that. Among other things, it provides an easy way for search engines to discover our pages, so you, our (future?!) customer, can find them more easily. That means more people choosing green energy, and ultimately, a cleaner environment!  🎉\n\n## We rolled our own\n\nThe year was 2017, Covid was still a word that sounded more like a bird than anything else... The world was heating up and Vandebron was 4 years into its mission to bring 100% renewable energy throughout all of the Netherlands.\n\nAs far as web technologies are concerned, 4 years was ages ago. It was a time when NextJS was less than a year old, and Remix was still several years from coming out. But we needed a way to deliver that high-quality content to all of you. So, the innovators that we were, we decided to build our own SSR framework. In short, we wanted pizza, but there were no pizza shops in town... So we made our own!\n\nIt's been great but not without issue...\n\n<table>\n  <tr style=\"border: none;\">\n    <td><img src=\"../images/remix-migration-ugly-window-mock.png\" alt=\"ugly-window-mock\" width=\"500\"/></td>\n    <td><img src=\"../images/remix-migration-mocking-a-window.png\" alt=\"remix-migration-mocking-a-window\" width=\"500\"/></td>\n  </tr>\n</table>\n\n\n\n## A Short Note: Why Server Side Rendering\n\nYou might not be satisfied with the short explanation of why we picked an SSR framework in the first place. This article isn't really about that - if you're interested in more analysis on when and where to choose an SSR framework, check out these excellent articles from Splunk:\n* [The User Experience (UX) Benefits of SSR](https://www.splunk.com/en_us/blog/learn/server-side-rendering-ssr.html)\n* [The SEO Benefits of SSR](https://www.splunk.com/en_us/blog/learn/server-side-rendering-ssr.html)\n\n## Decisions Made the Right Way - A Comparison\n\nNowadays, there are better, industry standard technologies available! I.e. pizza shops have opened nearby!! Let's find a good one. Of course, you don't want to just go to any spot. Especially if there's more than one shop in town - you'd be silly not to check which one is closest, and look at the menu. Which one has better reviews, is that one very angry customer just upset that there wasn't any anchovies in the vegan pizza shop? What were they expecting anyway?\n<img src=\"../images/remix-migration-vegan-pizza-shop.png\" alt=\"vegan-pizza-shop\" width=\"600\"/>\n\nAt Vandebron we're a React shop, so we limited ourselves to just SSR frameworks supporting React. The choice of one framework over another is of crucial importance, so, as part of our analysis, we built a small part of our [vandebron.nl/blog](https://vandebron.nl/blog) page twice. Two of our engineers then presented these prototypes to our Front End Guild, and this discussion fed heavily into the Architecture Decision Record that we wrote comparing the results.\n\n\\* At Vandebron, Guilds are groups of engineers from disparate teams that are interested in a single domain: i.e. Backend, Frontend, IAM and Auth, etc. \n\nThe Background for the decision record states this:\n\n> _\"Our Frontend currently uses a custom-built, hard to maintain SSR solution, which we'd like to replace with a modern and standard library. Possible candidates are NextJS and Remix. The goal is to investigate which one suits our needs best.\"_\n\nYes, there are other options we could have considered but we wanted to stay with a tried-and-tested framework and one that was compatible with our existing React setup.\n![remix-migration-adr-options-considered.png](../images/remix-migration-adr-options-considered.png)\n\nAs you can see, the comparison between the two frameworks was very similar. In the end we favoured the simple, opinionated direction of Remix over that of the more full-featured but potentially complex setup of NextJS. Even though Remix has a smaller community, we attributed this mostly to the age of the framework and not the quality  of the framework itself. Though the Positivity has gone down a bit (as listed in [the 2023 StateOfJS survey](https://2023.stateofjs.com/en-US/libraries/meta-frameworks/),) the decrease has been relatively minor and in line with most-other frameworks (notable exceptions for Astro and SvelteKit which have both seen big upticks in both Usage and Positivity)\n![State of JS Positivity](../images/remix-migration-sojs-framework-positivity.png)\nFinally, we noted that NextJS is tightly coupled with Vercel. At Vandebron we value platform independence and not getting tied to specific hosting providers or platforms. Remix gives us the independence we're looking for by providing a SSR framework without a potential to be tied into other solutions/platforms in the future.\n\nOutcome\n> _\"Most members favoured Remix’s focus on web standards and usage of standard libraries and were put off (a little) by NextJS’s uncertainty in development direction.\"_\n\n## So, How's it Going?\n\nThe migration effort is still underway but already we can report that it's going quite well - developers are excited to work on the new tech stack because it's seen as a developer-friendly platform and one of the two leading frameworks in the industry. In the words of one engineer: \"Dev experience has improved massively, it's fun, it's easy to work with\"\nHere are some of the things we still need to work on:\n- Our Docker image is quite large as it includes all the `node_modules`. We think we can clean this up a bit by using Yarn's Plug'n'Play (PnP) feature which should lead to faster image-build times and faster container startup times.\n- With our custom SSR solution, we use Redux Toolkit (RTK) and RTKQuery on the server... This is of course an anti pattern on the server, since server-side logic should be stateless. The Remix framework does already tries to be smart with it's loaders, so the benefits we might have gotten from RTK aren't needed there.\n- We feel the application we're migrating from is doing too much - it includes our marketing pages like the _Blog_ and _Mission_ pages we've been working on for the initial release, as well as the pages for our our signup and renewal process (become a Vandebron customer [here](https://vandebron.nl)!!!) This is a separate conversation, and ultimately one for the FE Guild, but the existing app's size and purpose is making the migration take longer than it should, and forcing us to put some routing rules in place to make sure the right parts of our old site are getting swapped out for the new.\n- Previously, many of the images and PDFs we used on our website were checked directly into the repo. Part of our migration to Remix made us realize we should be using a CMS for this. We are already integrated with a CMS, we just need to be making better use of it in some cases.\n- We haven't explored the Remix-specific linting rules yet. While we're confident in the existing React and TS lint rules we already have, it seems like configs like [@remix-run/eslint-config](https://www.npmjs.com/package/@remix-run/eslint-config) could be quite handy.\n","meta":{"title":"Choosing Remix as a Server-Side Rendering (SSR) Framework","description":"We had our own custom SSR framework. It was time to move on. Find out why we picked Remix over NextJS as the replacement!","createdAt":"Fri Oct 18 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/remix-migration-remix-vs-nextjs.png","tags":["remix","ssr","typescript","react","nextjs","ADR"],"author":"John Fisher","slug":"blog/choosing-remix-as-an-ssr-framework","formattedDate":"18 oktober 2024","date":"Fri Oct 18 2024 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\n\n## Salesforce + FlowRunner golden age\n\nSince 2015, Vandebron has been using Salesforce. At the time, Salesforce for Vandebron, was like a Swiss Army knife - versatile, multifunctional, and seemingly capable of addressing most of the business requirements. It quickly became the central hub for various operations - it became a workspace for agents, a CTI platform, a platform to send service emails and much more. Over time, Salesforce evolved beyond just a customer relationship management tool for Vandebron. It became a full-fledged platform that managed customer-related processes, such as the Signup process, Renewal process, Meter Reading process, etc. \nTo support this transition, Vandebron developed a custom mechanism known as FlowRunner, which was designed to automate and execute these processes within Salesforce.\nInitially, FlowRunner seemed like the perfect solution. It was tailor-made to handle the increasingly complex workflows that Vandebron needed to manage. While it successfully managed to support Vandebron’s operations for several years, this system was not without its flaws. These issues, which will be discussed in detail later, eventually led to the need for a more robust and scalable solution. But for a time, FlowRunner did its job, enabling Vandebron to leverage Salesforce far beyond its original purpose.\n\n\n## Salesforce + FlowRunner solution problems\n\n\nBroadly, the problems can be divided into two categories: technical and organizational.\n\nTechnical Problems: \n- Async Transactions Daily Limit. 250000 async transactions per 24 hours. For bulk processes, it is often not enough. We need to watch it carefully and adjust settings to avoid disaster.\n- Number of concurrent async jobs. Up to 5 async jobs simultaneously. \n- The FlowRunner mechanism in Salesforce creates lots of data. It uses ~ 25% of our storage. Data is expensive in Salesforce. \n- The Salesforce platform is not equipped for a custom BPM solution.This makes the Vandebron Salesforce codebase too large to be used with Salesforce DX (Salesforce CI/CD product). Furthermore, it forces us to maintain a lot of custom code that is available on the market.\n\nOrganizational Problems:\n- Centralization of Customer-Related Processes: With most customer-related processes embedded in Salesforce, any changes to these processes require intervention from the Salesforce team. This centralization creates a bottleneck, as all modifications, updates, and optimizations must pass through a single team, slowing down the overall pace of innovation and response.\n- Domain Overlap and Knowledge Dilution: The Salesforce team at Vandebron is responsible for managing approximately 50 different processes, each belonging to various business domains. This wide scope of responsibility leads to a dilution of expertise, as the team cannot maintain deep knowledge of every process. The result is a lower overall level of understanding and efficiency, making it difficult to ensure the smooth operation and timely updates of all processes.\n\n\n\n## Point of no return\n\nAt the beginning of 2022, Europe was hit by an unprecedented energy crisis. Gas and electricity prices skyrocketed, fluctuating unpredictably, and placing immense pressure on energy providers like Vandebron to adapt swiftly. In response, Vandebron introduced a solution designed to navigate this volatile market: the Flexible Variable Tariffs proposition.\nFrom a technical standpoint, implementing this new offering required the execution of a relatively complex process - Flow_VariableTariff  for approximately 50% of our customer base. However, it soon became clear that the FlowRunner mechanism and Salesforce in general were not sufficient to handle the demands of this new process. The total execution time for Flow_VariableTariff was projected to be enormous, spanning over 20 days, which was far too long for a business that needed to respond rapidly to market changes.\nRecognizing the urgency of the situation, we immediately sought ways to optimize the process. While we succeeded in significantly simplifying Flow_VariableTariff, these improvements alone were insufficient to meet our needs. It was at this critical juncture that we realized Salesforce and the FlowRunner were no longer adequate for Vandebron’s evolving requirements. The limitations of these tools became glaringly apparent, signaling the need for a more powerful and flexible solution to support our operations in the face of such a dynamic and challenging environment.\n\n\n## Why Camunda?\n\nChoosing the right process orchestration tool is a critical decision, especially for a company like Vandebron, where efficient workflow management is essential for operational success. To ensure we made the best choice, we began by establishing a set of criteria that the new tool needed to meet. These criteria were designed to address our current challenges and future-proof our operations. Here are some of the most crucial criteria:\n- Compliance with BPMN 2.0 Standard: We prioritized tools that adhered to the BPMN 2.0  standard. This would make any future migration to another tool less painful, ensuring a smoother transition if needed.\n- CI/CD Integration: The ability to seamlessly integrate the tool with Vandebron's CI/CD pipeline was crucial. This integration would allow us to automate deployments, streamline updates, and maintain a high level of consistency across our development processes.\n- Support for Multiple Programming Languages: Given our diverse technology stack, we needed a tool that allowed us to implement flowstep logic in multiple programming languages, with a particular emphasis on supporting Scala, which is heavily used within our systems.\n- Unit Testing: The tool had to enable us to unit-test individual steps and parts of flows. This capability was essential for ensuring the reliability and accuracy of our processes before they were deployed to production.\n\nOur market analysis of process orchestration tools led us to evaluate five potential solutions:\n- Camunda 8\n- IBM Business Automation Workflow (BAW)\n- Bonita\n- Kogito\n- Flowable\n\n\nEach vendor provided us with a demo and/or a trial version of their product. During this evaluation process, we rigorously tested each tool against our criteria. Although all five options met our hard requirements, it quickly became evident that Camunda is the true leader in the market.\n\nSeveral factors contributed to our decision to choose Camunda:\n\n- SaaS Offering: Camunda's SaaS version provided us with the flexibility and scalability we needed, reducing the burden on our infrastructure and allowing us to focus on process management rather than platform maintenance.\n- Comprehensive Documentation: Camunda's clear and well-organized documentation made it easier for our teams to learn and implement the tool effectively, reducing the learning curve and speeding up the integration process.\n- Out-of-the-Box Connectors: Camunda offers a wide range of connectors right out of the box, enabling quick integration with various systems and services. This saved us time and effort, allowing us to implement new workflows faster.\n- User-Friendly Interface: The tool's intuitive and clean UI made it accessible to both technical and non-technical users, facilitating collaboration across teams and improving overall efficiency.\n- Responsive Support: Camunda's quick and helpful support was another decisive factor. Their team was readily available to assist us with any issues or questions, ensuring a smooth onboarding experience.\n\nIn the end, Camunda stood out as the optimal choice for Vandebron’s process orchestration needs, offering the perfect balance of functionality, usability, and support.\n\n## First steps with Camunda\n\nBefore we could begin migrating our processes from Salesforce to Camunda, it was essential to establish a robust infrastructure that would allow Camunda to seamlessly integrate with the rest of Vandebron’s ecosystem, particularly Salesforce. Since Salesforce would continue to serve as the primary workspace for our agents, we needed to ensure smooth communication and data flow between the two platforms. To achieve this, we developed several key infrastructural applications:\n\n- CamundaGateway: Camunda API (Zeebe API) operates using the gRPC protocol, which is not natively supported by Salesforce. To bridge this gap, we created the CamundaGateway, a proxy application that translates HTTP calls into a format that Zeebe API can understand. This application acts as an intermediary, enabling effective communication between Salesforce and Camunda.\n- CamundaSync: Each Camunda process instance has a corresponding representation in Salesforce. To keep the status of these instances up to date across both platforms, we implemented CamundaSync. This job regularly pulls the status of process instances from Camunda and updates the relevant records in Salesforce, ensuring that agents always have access to the most current information.\n- CamundaJobWorker: Not all process steps can be handled by simple connectors like the RestConnector. Some steps are more complex and require custom logic to be executed. To manage these, we developed the CamundaJobWorker service, which contains handlers for these complex process steps. This service allows us to extend Camunda’s capabilities and handle sophisticated workflow requirements efficiently.\n- BPM app (React): Certain processes require input from users, particularly agents working within Salesforce. To facilitate this, we built the BPM app, which includes a set of forms necessary for running specific processes. This application ensures that agents can interact with and influence the workflow directly from their workspace, maintaining the user experience they are accustomed to.\n\n\n![A schematic overview of the camunda infrastructure](../images/camunda_infrastructure.png \"A schematic overview of the camunda infrastructure\")\n\nAs of September 2024, we have successfully implemented the basic infrastructure needed for Camunda integration, and three customer-related processes have been migrated from Salesforce to Camunda, with several more in progress. \nIt's important to highlight that the migration process involved a comprehensive analysis of the existing process, including the removal of legacy components, identification of common errors, and targeted optimization efforts. As a result, we achieved a substantial reduction in errors. Specifically, the Flow_Renewal process, which previously had a 2% failure rate, now experiences only a 0.62% dropout rate post-migration, reflecting a 69% decrease in errors.\n\n\n## Future plans\n\nBy the end of the year, we aim to migrate up to 10 processes to Camunda, further reducing our reliance on Salesforce for process orchestration. In parallel, we plan to enhance our infrastructure applications—CamundaGateway, CamundaSync, CamundaJobWorker, and the BPM frontend app - to improve their performance, scalability, and ease of use. These enhancements will ensure that our systems remain robust and efficient as we expand our use of Camunda across more of Vandebron's operations.\nMoving forward, We will continue to leverage Camunda's capabilities to automate and optimize more processes, ultimately driving greater efficiencies and innovations across Vandebron.","meta":{"title":"Camunda BPM migration","description":"Migration from Salesforce Flow_Runner to Camunda BPM","createdAt":"Wed Sep 04 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/camunda-rising.png","tags":["salesforce","camunda","bpm","process_orchestration"],"author":"Andrei Karabovich","slug":"blog/salesforce-camunda-bpm-migration","formattedDate":"4 september 2024","date":"Wed Sep 04 2024 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\n# Cassandra, it’s not you, it’s us\n\nI want you to know that you are probably an amazing product that has so much to offer to the world. However, it sadly just isn’t working out for us anymore.\n\nWe've encountered challenges such as escalating costs, inconsistent write and read performances, and setup misalignments that have constrained our growth. Of course, this is not entirely your fault, we set you up to fail with our infrastructure and use cases.\n\nI hope we can part on good terms, with mutual respect and appreciation for the time we shared.  \nI wish you all the happiness, success, and fulfilment in the world, and I hope you find a company that complements your life in the way you deserve.\n\nThank you for understanding, and I truly wish you the best.\n\nYours truly, Vandebron\n\n## Our Data Storage Prince Charming\n![data-prince-charming.jpg](../images/data-prince-charming.jpg \"Data Prince\")\n\nA list of some of the qualities we are looking for:\n- Kindness and compassion. \n- High availability.\n- Low Maintenance.\n- Ability to store large volumes of data (currently around 10TB), though not everything has to be queried fast.\n- Capable of ingesting real-time energy usage data (every 15 minutes per customer, possibly higher frequency in the future).\n- Ideally, we can use our current tech stack as much as possible (flyway migrations, roundtrip tests, spark).\n- Ideally, use as few different database technologies as possible.\n- It does not need to be horizontally scalable, due to moving from 1 central data storage to a separate data storage per service.\n\nWith enough work, time and commitment, Cassandra could have fulfilled most of these requirements. However, love is a two-way street, and we didn't put in the time and effort to make it work.\n\n## Speed Dating Round\nSome potential suitors we considered for replacing Cassandra:\n\n#### ScyllaDB\nScyllaDB is very similar to Cassandra. It should have better performance but still have (almost) all the same functionality as Cassandra.\n\n#### PostgreSQL\nPostgreSQL is a relational database. We already use it extensively in our services.\n\n#### Cockroach\nIt is similar to PostgreSQL but with some limitations: [Known Limitations in CockroachDB v23.2](https://www.cockroachlabs.com/docs/stable/known-limitations.html)\n\nIt is horizontally scalable, which is an advantage over PostgreSQL when it comes to high availability and fault tolerance. We are also currently using it in some of our services.\n\n#### Timescale\nTimescale is a PostgreSQL extension that uses the same query layer, but a different storage layer, to have efficient time series-related features.\n\nIt can also distribute data, but this is still in early access and is not recommended.\n\n#### Yugabyte\n\nYugabyte is a PostgreSQL extension to make PostgreSQL into a distributed database.\n\n## Comparisons\n\nTo determine the most suitable match, we did some quick performance tests. One where we inserted 2 million + weather data records as fast as possible via recurring inserts, to see how easy it would be to migrate over to. And another test to determine general query speed.\n\n### Write Speed Results\n![insert-perf-database.jpg](../images/insert-perf-database.jpg \"Insert Graph\")\n\nNote that the test results are not 100% fair, because Timescale and Postgres don’t have to distribute the data over multiple nodes (though Timescale does have 2 replicas), and Cassandra already contained a lot of data (though with some testing timescale didn’t seem to become slower when it already had more data). For Yugabyte and Cockroach, we gave up after 1 hour. Also, the tests were done with the existing setup for Cassandra.\n\n### Query Speed Results\n![query-perf-database.jpg](../images/query-perf-database.jpg \"Query Graph\")\n\nWe also did some tests to query aggregated data from streaming data.\n-   For this, we copied over 2.6M rows to each database.\n-   For this data we need to aggregate (sum) some values per 15-minute time block).\n-   For Cassandra/Scylla, this is done via spark.\n-   For timescale use buckets based on timestamps.\n-   For Postgres by grouping on floor(extract(epoch from timestamp) / 60 / 15)\n\n## Our Happily Ever After\n\nTimescale emerged as the clear winner here, not just for its performance in the above tests, but also for its seamless integration with our existing PostgreSQL setup. This compatibility allows us to maintain our current libraries and reduce code complexity, making Timescale an ideal choice for our time series data. At the same time, we continue to rely on Postgres for our other needs.\n\nCassandra, you won’t be missed.","meta":{"title":"Cassandra, it’s not you, it’s us","description":"Our Journey to find the perfect data storage solution for us","createdAt":"Fri Feb 16 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/heart-breaking.jpg","tags":["cassandra","timescale","postgresql"],"author":"Tomás Phelan","slug":"blog/cassandra-its-not-you-its-us","formattedDate":"16 februari 2024","date":"Fri Feb 16 2024 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nThe title of this article could have also been \"*Getting Rid of an Unmanageable Legacy Data Model*\", but after a year-long migration project the current title does more justice to the progress made. \n\n#### Compiled Legacy as a Data Model\n\nOur former data model was a series of parallel custom python jobs all covering every step of the *Extract-Transform-Load* (ETL) process from sources into report. Specific transformation got performed a numerous amount of times in multiple different jobs, daily. This made us prone to bugs, slow on development and maxing out on compute. \n\nThe situation became so pressing that keeping alive simple reporting to the business became a daily burden on the Data Analytics team, limiting resources for advanced analytics and leveraging data sources for competitive insights.\n\nWe concluded the old set-up to be outdated and looked around for current best practices concerning data infrastructure. Trying not to reinvent the wheel and staying away from designing custom solutions that had bitten us in the past, we decided to adopt a combination of *Snowflake*, *dbt* and *Lightdash* to start forming a new data landscape.\n\nThis revamping of the set-up gave us the opportunity to start over, using the power of *dbt* to create a modular data model where you could leverage different stages of data, while creating shared definitions, a single source of truth and documentation.\n\n#### What We Came Up With?\n\nWe went for a pretty classic *dbt* data model design, introducing 5 layers of data: staging, entity, intermediate, mart and api. Each layer serving a specific purpose.\n\n##### Staging\n\nWith all data coming in from different sources, this is where we ensure the data all adheres to the same conventions and formatting. This introduces a nice developer experience for the next layers, by introducing consistency across different sources. It also serves as the go to place for advanced or deep dive analysis that do not get answered by the downstream layers, which could potentially spark data modelling developments.\n\n##### Entity\n\nAfter uniforming the data, we create entities that form the building blocks of the downstream layers and analyses of our business analysts. We built entities along the core aspects of our product, capturing shared definitions in data and bringing together relevant features using the *One-Big-Table* (OBT) principle. We try to refrain from long queries or extensive use of CTE's, resulting in simplistic models. These models serve our business analysts by reducing the complexity of their queries with all joins and filters taken care of, denormalizing the database structure. This has shifted the place where ad-hoc BI requests are fulfilled from the central data team to the domain business teams, applying principles of a data mesh.\n\n##### Intermediate\n\nWith some models rising in complexity and computation, we use the intermediate layer to split this complexity and computation across multiple models. These intermediate models are rarely queried because they serve no reporting or analytical purpose. Think of incremental date spine explosions or highly complex business logic broken down into multiple models.\n\n##### Mart\n\nThis is the main layer where we provide self-service to less technical employees within the organization, creating ready-made tables. We aggregate along date spines and dimensions to create readable models. It is where we leverage *Lightdash* metrics to create dynamic tables to provide business teams with a certain amount of freedom in terms of the granularity and dimensions they want to report on in their dashboarding. The use of entities as building blocks has aligned reporting across domain business teams, creating a single and centralized source of truth and relieving the data team from explaining distinctions. So while the dimensions can be tweaked for specific use cases, the definitions of the metrics are set in code.\n\n##### API\n\nWith some dependencies outside of the data model, we use an API layer on top of our mart to record exposures towards different services and provide views which explicitly contain only the necessary datapoints.\n\n![A schematic overview of the data model structure](/images/schematic_data_layers.jpg \"A schematic overview of the data model structure\")\n\n#### The Process\n\nWe decided to take advantage of the chaos created by the old situation: no real single source of truth gave us the opportunity to create a truth. Investigating business teams' needs, we created data definitions in entities. We kept a pragmatic approach to these definitions, being flexible towards business teams' specific needs but also limiting the allowed complexity or number of exceptions. The new data model should answer everyone's questions, but should also be understood by everyone.\n\nWe forced ourselves to have descriptions for all data from the entity layer onwards, because only defining and describing the entities in code is not enough. We leveraged the embedded business analysts' knowledge to form the descriptions, noting that the best description is the one the user understands (because they wrote it).\n\nWith the ready-made marts in place, we decided to give away most of the dashboarding responsibility to the business teams. The embedded analysts are very apt at defining and designing their relevant insights into dashboards. The central data team only took ownership of company wide dashboards and provided support on the dashboarding where necessary.\n\nAfter the adoption of the new stack, we noticed that the more technical embedded analysts were very interested in learning a new tool and language. So, we started a data model group and onboarded multiple embedded business analysts as data model developers. This has massively increased the speed of development of the data model. Primarily, because of specific business domain knowledge not needed to be transferred to the developers in the central data team first, but the knowledge holders developed models themselves. The central data team took on a different role: providing infrastructural support, improving on efficiency, monitoring costs and creating a vision and strategy for organic but structured growth.\n\n![A schematic overview of the final self-servicing data model product](/images/schematic_data_product.jpg \"A schematic overview of the final self-servicing data model product\")\n\n#### What Did We Learn?\n\nFew key takeaways:\n\n- Some business teams have more requirements in terms of definitions than other teams, so if other teams allow, be pragmatic and just go for the stricter requirements.\n- Enabling self-service analysis means giving away control, take this into account in your data descriptions. They should be clear and concise.\n- Educate users on the designed structure of the data model, explain what layer serves which purpose and what questions can be answered how and where.\n- Create clear communication and support channels for the business to kickstart the adoption, you are not the only one learning a new tool.\n- Data is not only for the data team, so encourage those passionate and enthusiastic analysts to co-create! (Just keep an eye on the project.) ","meta":{"title":"Creating a Self-Service Data Model","description":"How we migrated to a modern data stack to enable self-servicing across the business","createdAt":"Wed Feb 07 2024 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/self_service.jpg","tags":["dbt","snowflake","lightdash","datamodel","self-service"],"author":"Mats Stijlaart","slug":"blog/creating_a_self-service_data_model","formattedDate":"7 februari 2024","date":"Wed Feb 07 2024 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\n\n# Authenticate Snowflake rest api via Keycloak\n\nHere in Vandebron  we use Keycloak as our identity and access management (IAM) solution and Snowflake as our data warehousing platform. \nKeycloak is a powerful and extensible solution for managing user identities and access control, making it a popular choice for organizations seeking a comprehensive and open-source IAM platform.\nSnowflake is designed to handle and analyze large volumes of data with speed and efficiency. It is known for its scalability, flexibility, and ease of use in managing and analyzing diverse and massive datasets.\n\n## Accessing Snowflake data via Rest API\n\nThere are several ways to access data in Snowflake one of these are the Snowflake rest api, they are a comprehensive set of REST APIs for managing and interacting with various aspects of the Snowflake Data Cloud, including account management, data loading, querying, and more.\nThese REST APIs allow developers to programmatically perform tasks such as executing SQL queries, managing virtual warehouses, and administering user roles. They are designed to enable automation and integration with other applications and services.\n\n## Why via Rest Api?\n\nThe Snowflake SQL API is a REST API that you can use to access and update data in a Snowflake database. You can use this API to develop custom applications and integrations that can perform most of the queries you need. More info here: [Snowflake rest api](https://docs.snowflake.com/en/developer-guide/sql-api/index)\n\nWe decided to connect our microservices to snowflake via rest api mainly because we consider this mechanism the best way to decouple database processing with backend processing in fact the queries issued via the endpoint are processed inside Snowflake ecosystem asynchronously.\n\nThe service can poll snowflake to monitor the request until it is completed. See [Sql api response](https://docs.snowflake.com/en/developer-guide/sql-api/handling-responses) .\n\nUsing api communication has other very good benefits:\n\n- No additional library dependency\n- No Additional spark connectors\n- Since there is no way to run snowflake on a local machine unit test a snowflake connection would have been very hard ( impossible ). With Rest api communication we can unit test snowflake api client using contract test. ( one way contract test is better than nothing )\n\n## Snowflake Authentication\n\nSnowflake provides a convenient way to authenticate to it using “any” OAuth authentication server. Our authentication server is Keycloak so in the following sections you will learn how to integrate Keycloak with Snowflake.\nResources to this topic can be found here [auth-ext-overview ](https://docs.snowflake.com/en/user-guide/oauth-ext-overview)  and here: [oauth-ext-custom](https://docs.snowflake.com/en/user-guide/oauth-ext-custom)\n\n\n## Keycloak side\n\nYou need to configure your client to return in the JWT access token the following claims:\n\n```json\n{\n    \"aud\": \"<audience_url>\",\n    \"iat\": 1576705500,\n    \"exp\": 1576709100,\n    \"iss\": \"<issuer_url>\",\n    \"scope\": [\n        \"session:role-any\"\n    ]\n}\n```\n\nmost of them are returned by default. Aud claims is the only one you should add\nTo add `aud` claim you can add a new mapper to your client with type Audience see image:\n\n![keycloak_aud.png](../images/keycloak_aud.png \"Keycloak aud mapper\")\n\n**Note**: You need to add a custom audience with the value **equal** to the login_name attribute value in snowflake. The audience value will be used to look up to the right user in snowflake integration\n\nThen you need to add the snowflake scope to your scope list: session:role-any\nFinally you can check that your token is correct:\n\n```json\n{\n    .....\n    \"iss\": \"https://test.vdbinfra.nl/auth/realms/vandebron\",\n    \"scope\": \"session:role-any\",\n    \"aud\": \"energy-trading-test\",\n    ....\n}\n```\n\nThe `aud` must contain only the snowflake login_name. For instance, a token such as the following will not work (multiple audiences):\"aud\": [     \"batterypack-services-test\",     \"account\"   ],\n\n## Snowflake side\n\nHow to find keycloak public key: [stackoverflow](https://stackoverflow.com/a/57457227)\nRequired: `ACCOUNTADMIN` rights in Snowflake.\nExample integration command:\n\n```sql\ncreate or replace security integration external_oauth_keycloak_test\ntype = external_oauth\nenabled = true\nexternal_oauth_type = custom\nexternal_oauth_issuer = 'https://test.vdbinfra.nl/auth/realms/vandebron'\nexternal_oauth_rsa_public_key = '<public_key>'\nexternal_oauth_audience_list = ('energy-trading-test')\nexternal_oauth_scope_mapping_attribute = 'scope'\nexternal_oauth_token_user_mapping_claim = 'aud'\nexternal_oauth_any_role_mode = 'ENABLE'\nexternal_oauth_scope_delimiter = ' '\nexternal_oauth_snowflake_user_mapping_attribute = 'login_name';\n```\n\nNote: the external_oauth_scope_delimiter setting must be enabled separately by Snowflake support.\nNext, you need to set the login name for the user you want associate with the integration:\n\n![snowflake_auth_conf.png](../images/snowflake_auth_conf.png \"Snowflake auth configuration\")\n\n### Example\n\nLet’s authenticate with keycloak as we do normally:\n\n```curl\ncurl --location --request POST 'https://keycloak.test-backend.vdbinfra.nl/auth/realms/vandebron/protocol/openid-connect/token/' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data-urlencode 'grant_type=client_credentials' \\\n--data-urlencode 'client_id=energy-trading' \\\n--data-urlencode 'client_secret=<secret>'\n```\n\nNow you should get the token. Optional: van verify the token directly in snowflake with SQL:\n\n```sql\nSELECT SYSTEM$VERIFY_EXTERNAL_OAUTH_TOKEN( '<token>' )\n```\n\nUse it in the snowflake statement endpoint. For example:\n\n```curl\ncurl --location --request POST 'https://<my_snowflake_identifier>.eu-central-1.snowflakecomputing.com/api/v2/statements?async=true' \\\n--header 'Authorization: Bearer <yourtoken> \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n\"statement\": \"select \\\"data\\\" as prediction, to_number(\\\"lats\\\", 10, 4) as lats, to_number(\\\"lons\\\", 10, 4) as lons, \\\"scaledValueOfFirstFixedSurface\\\" as scaled_value_of_first_fixed_surface, to_timestamp_tz( concat(\\\"dataDate\\\", lpad(\\\"dataTime\\\", 4, 0)) || '\\''+0'\\'', '\\''yyyymmddhh24mi+tzh'\\'') as model_datetime, to_timestamp_tz( concat(\\\"validityDate\\\", lpad(\\\"validityTime\\\", 4, 0)) || '\\''+0'\\'', '\\''yyyymmddhh24mi+tzh'\\'') as predicted_datetime, insert_date_snowflake, current_timestamp()::timestamp_tz(9) as insert_date_staging from raw.icon_eu.alhfl_s;\"\n}'\n```\n\nNB: It is important to use the proper snowflake base url. In my case I am using https://<my_snowflake_identifier>.eu-central-1.snowflakecomputing.com/ where <my_snowflake_identifier> is my account identifier which was authorised during configuration phase the snowflake user the token is referring to in the clientId claim.\nYou should get a response such as:\n\n```json\n{\n    \"code\": \"333334\",\n    \"message\": \"Asynchronous execution in progress. Use provided query id to perform query monitoring and management.\",\n    \"statementHandle\": \"01aafc80-3201-abed-0001-4a0e00e52816\",\n    \"statementStatusUrl\": \"/api/v2/statements/01aafc80-3201-abed-0001-4a0e00e52816\"\n}\n```\n\nNow you can follow the async operation to the following get endpoint:\n\n```http\nhttps://<my_snowflake_identifier>.eu-central-1.snowflakecomputing.com/api/v2/statements/01aafc80-3201-abed-0001-4a0e00e52816\n```\n\nIt will return 202 if the processing is still ongoing. It will return 200 and the actual result when processing ends.\n\nHappy coding!","meta":{"title":"Authenticate Snowflake via Keycloak","description":"How to use Keycloak to authenticate against Snowflake rest api","createdAt":"Tue Dec 19 2023 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/snowflake_keycloak.jpg","tags":["keycloak","snowflake","rest","oauth","bearer token","authentication","security"],"author":"Rosario Renga","slug":"blog/authenticate-snowflake-rest-api-using-keycloak","formattedDate":"19 december 2023","date":"Tue Dec 19 2023 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nIn our sustainable journey at Vandebron, we are not only striving to revolutionize the renewable energy sector, but we are also rethinking how we interact with technology ourselves. As a part of this initiative, we have been looking at how to reduce our digital footprint. One of our most recent projects involves transforming our ageing fleet of iMacs into revitalized, lightweight machines. \n\nWe proudly introduce the 'flexMac'.\n\n### Regained speed, sustainability and enhanced security\nOur customer contact department, the core of our operation, was equipped with older iMacs running on slower HDD drives. While replacing these machines with newer models might have been the easier route, it didn't align with our commitment to sustainability. \n\nInstead, we decided to be creative and look for ways to upcycle our older iMacs. Our choice of tool? Google's ChromeOS Flex. As the slogan suggests, [‘don’t bin it, just flex it’](https://www.linkedin.com/feed/update/urn:li:activity:7066377989831233536/) we figured this could very well meet our wishes. By installing this onto our iMacs, we have given birth to our new line of workstations, naming them 'flexMacs'.\n\n[ChromeOS Flex](https://chromeenterprise.google/os/chromeosflex/) is a free, open-source operating system by Google that breathes [new life into older PCs and Macs](https://cloud.google.com/blog/products/chrome-enterprise/chromeos-flex-ready-to-scale-to-pcs-and-macs). It's lightweight, fast, and ideal for the web-centric applications and services our customer contact department uses every day. Once ChromeOS Flex was installed, the transformation was remarkable. The old machines metamorphosed from very slow to production-ready again in a breath, adept at handling all our workflows at the Customer Contact department.\n\nThese workflows at Customer Contact are fully web-based. It enables us multichannel support, integration capabilities, and data-driven insights. These help our support agents to provide personalized and efficient service across various communication channels. By using these technologies and insights, we optimize our customer service strategies, leading (hopefully) to higher customer satisfaction.\n\nAnother important benefit of this transformation was an added layer of security. ChromeOS Flex allows our users to log in using their Google identity, ensuring a personalized and secure workspace for every team member. This means each user experiences a secure, tailored environment, whilst bringing an additional level of security and control to our IT operations.\n\n### The importance of circularity\nBesides the operational benefits, the broader environmental impact of this initiative is important to us. By extending the life of our technology, we contribute directly to reducing e-waste, one of [the fastest-growing waste streams in the EU](https://www.europarl.europa.eu/news/en/headlines/society/20201208STO93325/e-waste-in-the-eu-facts-and-figures-infographic). As a company, Vandebron is not only promoting sustainable innovations but striving to actively embody them. Our 'flexMacs project is a testament to this commitment.\n\nOur 'flexMacs' project demonstrates how we can repurpose and upgrade older hardware, which according to Emerce is a [hot thing to do](https://www.emerce.nl/achtergrond/circulaire-hardware-is-hot-dit-is-waarom). We hope this blogpost inspires you to consider similar sustainability initiatives. By choosing to upgrade rather than replace, we extend the life of existing hardware and contribute to a reduction in e-waste.\n\nStay tuned for more updates from our tech-driven sustainability journey.\n","meta":{"title":"Sustainable Tech-Hardware - Introducing the 'flexMac'","description":"Enhanced security, sustainability, and regained speed at Customer Contact revitalizing our old iMacs.","createdAt":"Mon Jul 03 2023 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/circular.jpeg","tags":"sustainable-tech, flexmac, circularity","author":"Gehdrio Lake & Sietse Bruinsma","slug":"blog/sustainable-tech-hardware","formattedDate":"3 juli 2023","date":"Mon Jul 03 2023 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\n\n### Amazon embraces the mighty monolith\n\n<img src=\"/images/step-functions.webp\" alt=\"image alt text\" style=\"width: 50%; float: left; padding: 5px;\" />\n\nIn March 2023, Amazon published a [blog post](https://www.primevideotech.com/video-streaming/scaling-up-the-prime-video-audio-video-monitoring-service-and-reducing-costs-by-90)\n, detailing how they had managed to reduce the cost of their audio-video monitoring service by 90%.\nThe _key_ to this reduction was migrating from a distributed, microservice architecture to a _monolith_.\nThe blog post went viral, prompting some software industry celebrities to \n[question](https://world.hey.com/dhh/even-amazon-can-t-make-sense-of-serverless-or-microservices-59625580) the entire concept of microservices.\n\n### What should we learn from this?\n\nSo, does this mean microservices are fundamentally flawed? Should we all migrate back to monoliths?\n_No_ and _definitely no_ I would say. Instead, my takeaways from this article are:\n\n1.  **Microservices aren't about scaling for performance.** At least not primarily. Although horizontally scalability for computationally intensive operations _can_ be very useful or even essential in some cases, it tends to be a rare benefit. Very often, performance bottlenecks are IO bound and caused by external systems beyond your control. Nevertheless, there _are_ other compelling reasons to consider microservices: they _force_ you to communicate via contracts, _encourage_ you to organize your functionality around domains, and _allow_ you to scale your organization. Of course, all this comes at considerable costs. There's no [free lunch 👇](#presentation).\n2.  <iframe width=\"372\" height=\"208\" src=\"https://www.youtube.com/embed/RC_FHNRI8Lg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen style=\"float: right; padding: 5px;\"></iframe> <b>Don't underestimate the power of a single CPU in 2023</b>. To judge whether a process is unreasonably slow or not, I tend to think of the fact that already in the 1990s, screens showed 65K pixels at any given time. Back then, multiple arithmetic calculations (additions, subtractions) could be performed for each pixel, fifty times per second. Nowadays, your screen probably displays more than 5 Million pixels at once. So, if the amount of datapoints you are dealing with in the order of millions, you should generally be able to process them in a matter of seconds on a single machine. If you can't, you may be doing something <i>very</i> inefficient.\n3.  **Software engineering is hard**. Mistakes are made all the time, everywhere. Even at the big 4 tech companies. Kudos to Amazon 👏 for openly sharing the mistake they made so that we may all learn.\nIn the next section I will share one of our own experiences, not entirely different from the Amazon example.\n\n### The 90% cost reduction case at Vandebron\n\n#### Microservices or just distributed computing?\nConsidering that all the functionality used in the Amazon case belongs to the same _domain_, it arguably does not even serve as \na case against improper use of microservices, but instead a case against misuse *distributed computing*. <br/>\nLet's look into an example of misuse of distributed computing at Vandebron now.\n\n#### Predicting the production of electricity\nFor utility companies, accurately predicting both electricity consumption and production is crucial.\nFailing to do so can result in blackouts or overproduction, both of which are [very costly](https://vandebron.nl/blog/hoe-houdt-onze-technologie-het-energienet-in-balans).\nVandebron is a unique utility company in that the electricity that our customers consume is produced by a [very large\namount](https://vandebron.nl/energiebronnen) of relatively small scale producers, who produce electricity using windmills or solar panels.\nThe large number and the weather dependent nature of these producers make it very hard to predict electricity generation accurately.\n\nTo do this, we use a machine learning model that is trained on historical production data \nand predictions from the national weather [institute](https://www.knmi.nl/). As you can imagine, this is a computationally intensive task, involving large amounts of data.\nFortunately, we have [tooling in place](https://www.vandebron.tech/blog/fueling-the-energy-transition-with-spark-part-1) that\nallows us to distribute computations of a cluster of machines if the task is too large for a single machine to handle.\n\nHowever, here's the catch: the fact that we _can_ distribute computations does not mean that we should. Initially it seemed that\nwe couldn't analyze the weather data quick enough for the estimation of our production to still be a _prediction_\nrather than a _postdiction_. We decided to distribute the computation of the weather data over a cluster of machines.\nThis worked, but it made our software more complex and Jeff Bezos even richer than he already was.\n\nUpon closer inspection, we found an extreme inefficiency in our code. It turned out that we were repeatedly reading the entire weather dataset\ninto memory, for _every_ single \"pixel\". After removing this performance bug, the entire analysis could _easily_ be done\non a single machine. \n\n### What more is there to say? <a id=\"presentation\"> </a>\n\n\nSo if microservices aren't about performance, what _are_ they about? If I had to sum it up in one sentence It would be:\n> _Microservices are a way to scale your organization_\n\nThere is a lot of detail hiding in that sentence, which I can't unpack in the scope of this article. If you're interested\nwhat microservices have meant for us, I would recommend you watch the presentation below.\n\n\n#### Microservices at Vandebron\nAt [Vandebron](https://vandebron.nl/), we jumped onto the \"microservice bandwagon\" circa 2019. This wasn't a decision\nmade on a whim. We had seen a few industry trends come and go, so we first [read up](https://samnewman.io/books/building_microservices_2nd_edition/)\nand did our own analysis. We found that the concept of microservices held promise, but also knew that they would come at a cost.\n\nThese are some of the dangers we identified and what we did to mitigate them.\n\n| **Danger**                                | **Mitigation**                                                                                                              |\n|-------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|\n| _A stagnating architecture_               | Compile and unit-test time detection of breaking changes                                                                    |\n| _Complicated and error prone deployments_ | Modular CI/CD [pipelines](https://github.com/Vandebron/mpyl)                                                                |\n| _Team siloization_                        | A single repository (AKA monorepo) for all microservices and a discussion platform for cross-domain and cross-team concerns |\n| _Duplication of code_                     | Shared in house libraries for common functionality                                                                          |\n\n\n\nThe following presentation to the students of [VU University, Amsterdam](https://vu.nl/) explains how we implemented\nsome of these mitigations and what we learned from them.\n\n[![Presentation about micro services to students of VU Amsterdam](/images/play_presentation.webp)](https://youtu.be/HDs-pCsEzKM)\n","meta":{"title":"So, back to the monolith it is then?","description":"A recent Amazon article explaining how they managed to save costs by merging some of their services has lead some to question the value of microservices. What is our take?","createdAt":"Sat May 20 2023 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/monolith.webp","tags":"dagster, cicd, ci-cd, orchestration, data pipeline, kubernetes, migration, helm, ansible","author":"Sam Theisens","slug":"blog/back-to-the-monolith","formattedDate":"20 mei 2023","date":"Sat May 20 2023 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nVandebron is a Dutch green-tech energy company on a mission to accelerate the transition to 100% renewable energy, 100% of the time. As part of [our mission and strategy](https://vandebron.nl/100procentgroen), we are constantly innovating and looking for ways to optimize energy operations and reduce negative impacts when it comes to energy production.\n\nOur new mission: [100% renewable energy, 100% of the time](https://youtu.be/_Yf8jk4gZbI)\n\n### The importance of curtailment and flexibility services\n\nOne area where we are currently focusing our efforts is the area of curtailment and flexibility of wind turbines, solar parks, industrial batteries and electric vehicles. [Curtailment](https://vandebron.nl/blog/curtailment-slimmer-omgaan-met-goeie-energie) refers to the practice of reducing the electricity inflow to balance the electricity grid. In other words, it involves adjusting the operation of, for example, a wind turbine in order to match the demand for electricity at any given time.\n\n[This is often necessary](https://vandebron.nl/blog/hoe-houdt-onze-technologie-het-energienet-in-balans) because the output of renewable energy sources can vary significantly due to changes in weather conditions. If the output of these sources exceeds the demand for electricity, it can lead to an excess of electricity on the grid, which can cause stability issues. On the other hand, if the output of wind turbines is too low, it can lead to a deficit of electricity on the grid, which can cause blackouts or other disruptions. To tackle this, we look at our customer’s batteries and electric vehicles offering flexibility capabilities.\n\n### Our journey to finding reliable, secure and energy-efficient hardware and software\n\nTo optimize these curtailment and flexibility efforts, we were in need of a gateway device that we could place at the installations of the producers on our platform. To keep it close to our mission, we preferred an ARM-based CPU for its [energy efficiency](https://www.redhat.com/en/topics/linux/ARM-vs-x86) compared to an x86-based CPU. After all, we don’t want to consume all of the produced energy to power an actively cooled NUC… 😉\n\nWhile gathering our hardware requirements, we concluded there was really only one competitor. Therefore, we partnered up with OnLogic! We chose their [Factor 201 device](https://www.onlogic.com/fr201/), which boasts the ARM-based Raspberry Pi CM4 module packed in a small and beautiful orange industrial chassis. The model also enables a lot of custom configurations. For example, we are able to configure multiple (wireless) networks, add extra SSD storage or optionally mount on DIN rails.\n\n![OnLogic Factor 201](/images/flex-onlogic-factor-201.jpg \"OnLogic Factor 201\")\n\nTo ensure our gateway devices are secure and agile (like us, developers, 😛) we needed them to integrate well into our existing technology landscape based on Kubernetes. After struggling for some time to harden several (lightweight) operating systems and bootstrapping lightweight Kubernetes clusters our eyes fell on a new kid in town: ‘Talos Linux, the Kubernetes Operating system’ built by [Sidero Labs](https://www.siderolabs.com/). Again our predetermined wishlist was covered (even more), and what we got is a minimal OS tailored for Kubernetes, hardened, immutable and ephemeral out-of-the-box. Can you survive even more buzzwords than that? \n\nUntil the present day though, they have fulfilled every promise made on [their website](https://www.talos.dev/). It initially didn’t work on our ARM CM4-based device from OnLogic. But after testing a lot together with their team (thank you!) the [latest release (v1.3.0)](https://www.talos.dev/v1.3/introduction/what-is-new/#raspberry-generic-images) officially supports our ARM devices. Ready for action! Right after the stable release the first batches were shipped and connected to the installations of our producers on the platform.\n\nOverall, Vandebron's use of OnLogic's fabricated gateway devices running Talos Linux demonstrates the potential of IoT computing to drive innovation and sustainability in the renewable energy industry. By leveraging the power of these technologies combined, we are one step closer to achieving our goal of 100% renewable energy, 100% of the time. Care to join our mission? Look for [open positions](https://werkenbij.vandebron.nl/).\n\n","meta":{"title":"How Vandebron helps balancing the Dutch energy grid together with OnLogic & Talos Linux","description":"Our journey to find the best fitting hardware and operating system to use for our flex services","createdAt":"Wed Jan 11 2023 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/flex-wallpaper.webp","tags":"iot, flexibility services, curtailment, onlogic, talos linux, kubernetes, arm64, raspberry pi","author":"Sietse Bruinsma & Tim van Druenen","slug":"blog/balancing-dutch-energy-grid-with-flex-services","formattedDate":"11 januari 2023","date":"Wed Jan 11 2023 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\n### TL;DR\nIf you want to deploy new Dagster user code respositories, you need to modify and redeploy the whole Dagster system (while they are [presented as separate](https://docs.dagster.io/deployment/guides/kubernetes/customizing-your-deployment#separately-deploying-dagster-infrastructure-and-user-code) in the docs). This is undesirable for many reasons, most notably because it slows down a migration or the regular development process. This post presents a way to avoid this and build a fully automated CI/CD-pipeline for (new) user code.\n\nThis article assumes that:\n* you (plan to) host Dagster on Kubernetes and manage its deployment with Helm and Ansible;\n* you want to automate the deployment of new Dagster user code repositories with a CI/CD pipeline automation tool of choice;\n* and you want to be able to (re)deploy the whole Dagster system and user code from scratch.\n\n### Why Dagster?\n\nIn short Dagster is a tool to build and orchestrate complex data applications in Python. For us, in the end, Dagster improved the development cycle for things like simple cron jobs as well as for complex ML pipelines. Testing the flows locally was never so easy, for instance. And with features like [asset materialization](https://docs.dagster.io/concepts/assets/asset-materializations) and [sensors](https://docs.dagster.io/concepts/partitions-schedules-sensors/sensors), we can trigger downstream jobs based on the change of an external state that an upstream job caused, without these jobs having to know of each other's existence.\n\nHowever, deployment of new [user code respositories](https://docs.dagster.io/concepts/repositories-workspaces/repositories) caused us some CI/CD related headaches...\n\n### System and user code are separated\n\nDagster separates the system deployment - the Dagit (UI) web server and the daemons that coordinate the runs - from the user code deployment - the actual data pipeline. In other words: the user code servers run in complete isolation from the system and each other. \n\nThis is a great feature of which the advantages are obvious: user code repositories have their own Python environment, teams can manage these separately, and if a user code server breaks down the system is not impacted. In fact, it even doesn't require a restart when user code is updated!\n\n![Schematic of the Dagster architecture. The user code repositories (green) are separate from the rest of the system (yellow and blue). The right side — irrelevant for now — shows the job runs. Source: https://docs.dagster.io/deployment/overview.](/images/dagster-architecture.png)\n\nIn Helm terms: there are 2 charts, namely the _system_: `dagster/dagster` ([values.yaml](https://github.com/dagster-io/dagster/blob/master/helm/dagster/values.yaml)), and the _user code_: `dagster/dagster-user-deployments` ([values.yaml](https://github.com/dagster-io/dagster/blob/master/helm/dagster/charts/dagster-user-deployments/values.yaml)). Note that you have to set `dagster-user-deployments.enabled: true` in the `dagster/dagster` values-yaml to enable this.\n\n#### Or are they?\n\nThat having said, you might find it peculiar that in the values-yaml of the system deployment, _you need to specify the user code servers_. That looks like this:\n\n```yaml\nworkspace:\n    enabled: true\n    servers:\n      - host: \"k8s-example-user-code-1\"\n        port: 3030\n        name: \"user-code-example\"\n```\n\n**This means system and user deployments are not actually completely separated!**\n\nThis implies that, if you want to add a _new_ user code repository, not only do you need to:\n\n1. add the repo to the user code's `values.yaml` (via a PR in the Git repo of your company's platform team, probably);\n2. do a helm-upgrade of the corresponding `dagster/dagster-user-deployments` chart;\n\nbut because of the not-so-separation, you still need to:\n\n3. add the user code server to the system's `values.yaml` (via that same PR);\n4. and do a helm-upgrade of the corresponding `dagster/dagster` chart.\n\nFormally this is the process to go through. If you are fine with this, stop reading here. It's the cleanest solution anyway. But it is quite cumbersome, so...\n\nIf you are in a situation in which new repositories can get added multiple times a day - for instance because you are in the middle of a migration to Dagster, or you want a staging environment for every single PR - then read on.\n\n#### Give me more details\n\nHow it works is that [for every new repo Dagster spins up a (gRPC) server to host the user code](https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm#user-code-deployment). The separation is clear here. But the Dagster _system_ also needs to know about these user code servers, and it does so through a workspace-yaml file. If you run Dagit locally it relies on a `workspace.yaml` file; on Kubernetes it relies on a [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) - a Kubernetes object used to store non-confidential data in key-value pairs, e.g. the content of a file - which they named `dagster-workspace-yaml`.\n\nThis workspace-yaml is the connection between the system and the user code. The fact that the charts are designed as such that this workspace-yaml is created and modified through the system deployment rather than the user code deployment is the reason we need to redeploy the system. \n\n**But what if we could modify this workspace-yaml file ourselves? Can we make the system redeployment obsolete? Short answer: we can.**\n\n### Our solution\n\n_Disclaimer: what we present here is a workaround that we'll keep in place until the moment Dagster releases a version in which the Dagster user code deployment is **actually completely separated** from the system deployment. And it works like a charm._\n\n**Remember: the desired situation is that we do not have to edit the values-yaml files (through a PR) and redeploy all of Dagster for every new repo.**\n\nFirst of all, we added an extra ConfigMap in Kubernetes that contains the `values.yaml` for the `dagster/dagster-user-deployments` chart. We named it `dagster-user-deployments-values-yaml`. The fact that this is a ConfigMap is crucial to prevent conflicts (see next section).\n\nWith the extra ConfigMap in place, these are the steps when a repo gets added:\n1. Add the new repo to the `dagster-user-deployments-values-yaml` Configmap.\n2. Helm-upgrade the `dagster/dagster-user-deployments` chart with the content of that ConfigMap.\n3. Add the server to the `dagster-workspace-yaml` ConfigMap.\n4. Do a rolling restart of the `dagster-dagit` and `dagster-daemon` deployment to pull the latest workspace to these services.\n\n**Refresh the workspace in the UI and there it is, your new repo!**\n\nNotes:\n* The steps above are completely automatable through your favorite CI/CD pipeline automation tool.\n* There is no interaction with a (platform team) Git repo.\n* The process, unfortunately, still requires a restart of the system in order to pull the latest workspace-yaml to the system services. The daemon terminates, then restarts, and it might cause a short interruption. Note that this is unavoidable if you add a new repo, no matter how you add it. This could be avoided if a reload of the ConfigMap would be triggered upon a change, [which is possible](https://kubernetes.io/docs/concepts/configuration/configmap/#mounted-configmaps-are-updated-automatically) but not enabled.\n* If you want to make changes to an existing repo (not code changes but server setting changes), you only have to do the first step (and _modify_ instead of _add_).\n\n#### How to prevent conflicts\n\nWith many of your team members adding new Dagster repositories through an automated CI/CD pipeline, you might face the situation that 2 people are adding a new repo at around the same time. \n\nWhen this happens, the `dagster-user-deployments-values-yaml` ConfigMap cannot be uploaded in the first step because Kubernetes demands that you provide the _last-applied-configuration_ when doing an update. If it doesn't match, the upload fails. \n\nThis is perfect as we do not want to overwrite the changes of the conflicting flow. You can optionally build in a retry-mechanism that starts over with pulling the ConfigMap again.\n\n#### How to deploy from scratch\n\nThe above does not yet cover how we are able to deploy the Dagster system _and user code_ completely from scratch. Why do we want this? Well, for instance when somebody accidently deletes the `dagster` namespace for instance. Or hell breaks loose in any other physical or non-physical form. Or when we simply want to bump the Dagster version, actually.\n\nThe key to this is that we version both the `dagster-user-deployments-values-yaml` and `dagster-workspace-yaml` as a final step to the flow described above (we do it on S3, in a versioned bucket). Whenever we redeploy Dagster (with Ansible) we pull the latest versions and use them to compile both the values-yaml files from it. \n\n#### How to clean up old repositories\n\nThe above described automation _adds_ new repos but doesn't take care of old obsolete repos. The steps for removing a repo are the same for adding one. The exact implementation depends on your situation. You might want to automatically remove PR staging environments after closing a PR, for instance.\n\n### Conclusion\n\nDagster is an incredibly powerful tool that enabled us to build complex data pipelines with ease. This posts explains how we **streamlined the CI/CD pipeline for user code respositories**, which enabled us to migrate to Dagster very quickly and saves us lots of time on a daily basis.\n","meta":{"title":"The Why and How of Dagster User Code Deployment Automation","description":"If you frequently deploy new user code repositories in Dagster, you want to automate this process. However, this is not so straightforward as it may seem at first. This post explains what we did at Vandebron.","createdAt":"Fri Jul 08 2022 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/dagster-cicd.png","tags":"dagster, cicd, ci-cd, orchestration, data pipeline, kubernetes, migration, helm, ansible","author":"Pieter Custers","slug":"blog/cicd-dagster-user-code","formattedDate":"8 juli 2022","date":"Fri Jul 08 2022 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\n**A while back, our former technology manager Roy Derks covered the subject of component libraries here on the blog. From a technical perspective, he spoke about when you need one (and when you don’t need one) and what to consider when building one. Since then, obviously a lot has happened at Vandebron. But one of the more interesting things to happen is that design became an integrated part of the digital department, as opposed to previously being attached to marketing. In this new setup, one of the first major projects the design team was involved in was the alignment of our component libraries. And no that’s not a typo, that’s libraries as in the plural form of library. Confusing? I thought so too. In this blog I’ll try to explain further why that was the case, how the work actually helped us bridge the gap between design and development, and dissect the work of unifying those component libraries into one single source of truth and ultimately what’s to become our design system.**\n\n### A bit of a mess\nBefore we get into it, some background as to where we started out might be useful. As previously mentioned, the design team had just become a part of the digital department and one of the first tasks at hand was the creation of a design system. In the design team, we had previously worked with a certain set of brand guidelines, a style guide if you will, which had not necessarily been translated or aligned to the requirements of a digital product or development environment. Development had also created a set of stylesheets and libraries with reusable components which they used to reduce development time. Having it all separately might sound a bit counter-intuitive, but not very surprising if you consider designers and developers not being in the same department, working on a different timeline, different priorities and so forth. However, this only highlighted the importance of designers and developers working together and the need for a proper design system to help prevent creating a fence between the teams causing unnecessary and ineffective work on both sides. The result of this previous “unsynciness”, a rebrand in 2017, and a re-aligned techstack, was the existence of 3 different libraries and subsequently 3 different sources of truth within the development environment. To add to this, we also had separate design guidelines geared more towards brand/marketing purposes in the design team. Now came the rather massive task of unifying these and eventually, rather than having just a library, _having a system_. \n\n### Component library ≠ design system\nNow, there’s a lot of terminology here that might be difficult to grasp if you’re new to the subject. So I thought I’d clarify what we mean when referring to these, how they fit into the context of our situation, and how many of them we had!\n\n- #### Brand guidelines / style guide (design)\n  A set of guidelines and examples outlining all the visual elements of a brand such as logos, color, typography, imagery etc. and subsequently in what - - manner they should be applied. It can also be expanded to include more things brand related such as tone of voice, brand values and so forth. Often with brand guidelines, they are created from a marketing perspective and the digital experience(or product) aspect of how the brand should be applied/represented is usually thought about in the second hand, or not included at all. \n\n  _Amount: 1_\n    \n- #### Design kit/library (design)\n  A designer resource file with all the available building blocks that make up the digital design language of a brand and/or the user interface of a product. This is usually only visual(no code) and lives in the design software of the designer's choosing. For us this used to be Sketch, but we recently moved to Figma. Can also include documentation and examples of how the different building blocks should be applied and utilized. \n\n  _Amount: 1_\n  \n- #### Style sheet (front-end)\n  A set of styling properties to be applied when rendering a web page, usually in the format of CSS. This can include things related to the brand guidelines such as font size, colors, etc. but also things related to web layout such as the margins and paddings of different web elements.\n\n  _Amount: 1_\n\n- #### Component library (front-end)\n  A set of dynamic web components that can be used in a development environment in order to quickly build user interfaces. This helps to ensure consistency, to avoid rebuilding the same component more than once and to avoid changing said component in more places than one, and subsequently help reduce development time. \n\n  _Amount: 3_\n  \nAll of the above mentioned things, together with rigorous documentation, amount to what’s called a design system. Having it all combined in a structured way is key to getting the most out of such a system. In our case, most of these things were separate and not necessarily connected to each other. But what stands out most of the things above is probably the fact that we, over time, had amounted to 3 different component libraries. I mentioned earlier how that scenario had transpired so I won’t go into too much detail as to how that happened, but if you’re a developer in a small to medium-sized company and I mention “rebrand” and “new techstack” you can probably figure out how. However complex, this also proved to be an excellent opportunity for our developers and for us in the design team. We finally get to unify our component libraries into one, while simultaneously aligning it with our design kit and expanding the guidelines with new and updated documentation. Thus ensuring that designers and developers speak the same language and share the same single source of truth.\n\n### A guild forms\nTo kickstart this process we formed a project group(or ‘guild’) composed of 2 designers and 2 developers, each designer and developer from the two consumer-facing scrum teams. The idea was to let the developers work on the migration and unification of the component libraries in collaboration with us designers in the same project, making it easier to align and to create co-ownership of the product. Our first step was to decide on the structure of our component library, this way the developers could slot all the existing, reworked and new components into the right place in the new library. Easy enough right? Well, here comes our first challenge. We initially wanted to take an atomic approach and build our components from the well known and widely used atomic design principles. We also needed to consider the 3 different “product groups” which the library should apply to, all still utilizing the same style properties. \n\nVandebron has a wide range of products serving different platforms, with the visual language remaining the same but where the user interface might differ. This requires the top elements of the system(such as colors and typography) to be shared across all products, whereas the lower you get the more product-specific an element becomes. This is the reason why we wanted to structure the system according to the principles of Atomic Design first, in order to assign the components to a hierarchical structure.\n\n![Atomic Design](/images/AtomicDesign.jpg \"Atomic Design\")\n\nWith this approach the atoms would work like design tokens and the molecules would be components general enough that they’d be shared across all product groups, this CORE part of the library would essentially be the stylesheet that impacts all visual aspects of the digital brand experience. Only on organism-level do we start to differentiate what product group the component belongs to. So a change to the CORE parts of the library(atoms or molecules) would impact all components in all product groups.\n\nHowever, this approach actually made less sense from a development perspective. Not that it wouldn’t work or that the categorization didn’t make sense, but it would require us rewriting all the already existing components. Components that are actively in use. We deemed this approach a bit too high-risk and high-effort for the time being and started looking into alternatives, while still keeping the atomic structure as a more long-term goal. Another thing our initial idea didn’t take into account was the experience of the future main user of the library, **_the developer!_** Organizing a design system after the brand properties and product groups makes a lot of sense from a designers or a marketeers perspective, and it should probably still be presented outwards that way, but a component library is something else(remember?). So based on our development environment and the way we build our websites and apps our developers suggested a different structure:\n\n![Iteration](/images/Iteration.jpg \"Iteration\")\n\nIn this structure, similar to the previous one, the components are instead categorized and sorted by how they should be applied to the page or application that’s being built. Styles, layouts and inputs are general enough to be applied to all product groups whereas from the surface level the components start becoming more specific in their use case. That way, the components can be separated into specific or even several product groups. In this format the components themselves are not as intertwined as in the atomic structure, albeit still connected by the style element. So while it’s a bit more resistant to overall changes the main idea of having the same style properties applying to everything still works, and it helps us designers to better relate and contextualize what we’re designing from more of a development perspective, thus helping bridge the gap between development and design even further. The main insight we drew from this experience is to not let industry standards and certain trends dictate what you should do. Sure they’re important to keep an eye on, but do it with carefulness and always apply an asterisk to it. Figure out what works best for your specific situation and what’s realistic in the short-term vs. in the long-term. There’s no one-size-fits-all.\n\n### Speaking the same language\nWith the component library migration now underway, we started looking into ways to improve our system from the designers' side of things. As previously mentioned, we had just gone from using Sketch to using Figma and with that came a good opportunity to rebuild, adjust and expand our design kit also. We did that by removing, adding, simplifying and renaming a lot of what was in there since before and with the design kit now adjusted to match the component library we were now also speaking the same language. We can actually now compare this side-by-side with the tools we’re using. In Storybook we have attached the Figma design of every component, simply by activating the feature and pasting the link to its page or artboard in the Figma file. This will refresh in almost real-time if any changes are made so we can easily spot any differences and inconsistencies between what’s live and how the design looks. In Figma, we try to document all our components and give some context as to how it works and should be applied. This is now also directly visible to the developer in the context of the component library. Expanding on our documentation and exposing our digital design guidelines like that has been a great way to create a shared understanding of our designs. Rather than just an image being tossed over a fence, there is now quite literally a direct connection between design and development and therefore also more of a shared ownership.\n\n![Storybook & Figma](/images/StorybookFigma.jpg \"Storybook & Figma\")\n\n### Further defining the process\nAs all the alignment on the design side and the migration neared completion, we started seeing a lot of things that could be improved upon or even added to our component library. When we started logging these things down on our project backlog we quickly realized that the scope of our project had quickly been growing into something beyond what was initially intended, and that rather than giving us focus this guild format was instead at risk of creating an isolated bubble of knowledge surrounding the design system. This prompted us to gauge the opportunity and capacity among our development teams to instead tackle these tasks together, either alongside or within their respective day-to-day tasks. In order to do so we needed the buy-in from key stakeholders such as the product owners from the respective development teams. It’s obviously a big ask to get 1 developer from each team to work on improving a component library, especially when they’ve already given us a quarter on migration and have other important business and/or user needs to tend to. So instead, we looked into how we can embed the improvement and further development of our design system into the developers current processes and primary day-to-day work. We structure this by linking our component library improvement/addition tickets to relevant tickets in their respective sprints. In defining the workflow like this, our 2 designer 2 developer guild in effect rendered unnecessary and instead we opened up the co-ownership and contribution to all developers in all customer-facing development teams and in the process of it preventing isolating knowledge too much. In opening up the process like this, another positive side effect we see is the involvement, engagement and subsequent use of our component library going up. With the product designers now also actively a part of the front-end guild meetings, we have an ever bigger forum and bigger opportunity to build a world class component library and design system while also having more hands on deck to work on maintenance and improvements. We still have a long way to go, but all parts are now even more aligned and the future is looking bright!\n\n### What’s next\nIn the newly formed designer+developer guild, the work of defining requirements and improvements on the design system continues. From the design side we’re also looking to constantly improve on the documentation and the presentation of our system. This is something we imagine we’ll keep on doing continuously and iteratively for as long as it’s needed, if not even forever. After all, “design is never done” and a design system can and should be a living thing constantly evolving along with the products and the brand it serves, and in extension even the promise the brand and it’s products. In our case, that’s to aid in **accelerating the energy transition towards 100% renewable energy**. More on how we exactly do that, and how we always aim to design for impact, in the next blog post. Thanks for reading and stay tuned!\n\n\nPetter Andersson, Product Designer at Vandebron\n\n\n\n_If the type of work mentioned in this blog post sounds interesting to you, [take a look at our job openings here](https://werkenbij.vandebron.nl/l/en/)._ \n","meta":{"title":"The difference between a component library and a design system, and how they can help bridge the gap between design and development","description":"A while back we started a rather extensive project of migrating and unifying our component library, these are some of the learnings we made during the project.","createdAt":"Wed Jul 06 2022 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/WindmolenCover.jpg","imageSource":null,"tags":"product, design, design system, component library","author":"Petter Andersson","slug":"blog/the-difference-between-a-component-library-and-a-design-system","formattedDate":"6 juli 2022","date":"Wed Jul 06 2022 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\n# Signing and verfiying SOAP messages with wss4j and Scala\n\nSOAP is not dead. It is an established, XML-based and mature messaging protocol that comes with built-in security mechanisms, integrity checks, content validation and much more. A lot of enterprises and corporations are using it (sadly) still.\nJust recently, Vandebron had to implement a SOAP client to communicate with an external party. \nThis blog post will explain with code examples how we at Vandebron are signing and verifying SOAP messages for our latest SOAP client implementation. \n\nFor this process, we are using Apache's Web Service Security Library [wss4j](https://ws.apache.org/wss4j/) as it is a proven tool in the WSS context and provides, as a Java library, great interoperability with the programming language Scala.\n\n## Signing SOAP messages\n\nHere we will take a look at the necessary steps to sign a SOAP message like this one:\n```xml\n<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\">\n  <soapenv:Header/>\n  <soapenv:Body>\n    <heading>Hello World</heading>\n    <body>I am just a test</body>\n  </soapenv:Body>\n</soapenv:Envelope>\n```\nTo look after signing like this:\n```xml\n<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\">\n  <soapenv:Header>\n    <wsse:Security \n    soapenv:mustUnderstand=\"1\" xmlns:wsu=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd\" xmlns:wsse=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\">\n      <ds:Signature \n      Id=\"SIG-ec946953-2470-4689-ad2f-0c579e1e06e3\" xmlns:ds=\"http://www.w3.org/2000/09/xmldsig#\">\n        <ds:SignedInfo>\n          <ds:CanonicalizationMethod Algorithm=\"http://www.w3.org/2001/10/xml-exc-c14n#\">\n            <ec:InclusiveNamespaces PrefixList=\"soapenv\" xmlns:ec=\"http://www.w3.org/2001/10/xml-exc-c14n#\"/>\n          </ds:CanonicalizationMethod>\n          <ds:SignatureMethod Algorithm=\"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\"/>\n          <ds:Reference URI=\"#id-47817454-f6e2-470c-9109-870e7895e3e0\">\n            <ds:Transforms>\n              <ds:Transform Algorithm=\"http://www.w3.org/2001/10/xml-exc-c14n#\"/>\n            </ds:Transforms>\n            <ds:DigestMethod Algorithm=\"http://www.w3.org/2001/04/xmlenc#sha256\"/>\n            <ds:DigestValue>7KfPcTwDYWtLj4ZVWmWmVqX4IGwbBAAmUPigCdXdk4U=</ds:DigestValue>\n          </ds:Reference>\n        </ds:SignedInfo>\n        <ds:SignatureValue>\n          OBnbBWv8S70xDDn5uG++7cTRFa2Uz3D47oxTHuO163Y3/V7H35M1GHXbKaUDOHsgsfx3SdVmVi++ra06cpwJknzqoIQgDV9Qc0ydzfxljCqupPKBnfONDYJtihEE1jtQ0RP7OLzPVNUpgOgHqbLwJu2pRUA05ool+lxIs924OwPVPKyUryoYwWhwY1ttY4P+WY2L3ZqsH3fgoLCyjlvhDEAhsP9PCxsEzPSq3ECC55Nh7nqMoHPj2uNxonuMlPeYbrlMnwyiqEW8s3Sc+WmfiIOgekRE1AdNhpn3ARlO490nObQtXCU/TxeTfbh98TMbQRZWWyT4HuLS3fF6aeyD/Q==\n        </ds:SignatureValue>\n        <ds:KeyInfo Id=\"KI-e18395de-9a26-4cad-9501-7c6cf6c7c74a\">\n          <wsse:SecurityTokenReference wsu:Id=\"STR-daa47836-f1f9-4d71-95cc-b7bcc6051c84\">\n            <wsse:KeyIdentifier \n            ValueType=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-x509-token-profile-1.0#X509SubjectKeyIdentifier\" EncodingType=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-soap-message-security-1.0#Base64Binary\">\n              ox4ajWTdigy9oApTYs97CuCV/4k=\n            </wsse:KeyIdentifier>\n          </wsse:SecurityTokenReference>\n        </ds:KeyInfo>\n      </ds:Signature>\n    </wsse:Security>\n  </soapenv:Header>\n  <soapenv:Body \n    wsu:Id=\"id-47817454-f6e2-470c-9109-870e7895e3e0\" xmlns:wsu=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd\">\n      <heading>Hello World</heading>\n      <body>I am just a test</body>\n  </soapenv:Body>\n</soapenv:Envelope>\n```\n\nFor implementing the steps of the blog post you will need:\n- a SOAP service you want to send messages to\n- documentation of that SOAP service that describes:\n    - signature algorithm\n    - canonicalization method\n    - digest algorithm\n    - key identifier type\n- a private key with which you will sign your messages\n- a certificate that is the counterpart of the private key\n- (optional) a pool of trusted certificates\n\nOur private and public key pair are available in the PKCS#12-format (.p12 file extension). Check out [this](https://www.ssl.com/how-to/create-a-pfx-p12-certificate-file-using-openssl/) to learn more about this format and how to achieve it.\nThe pool of trusted certificates are in the [PKCS#7 format](https://www.ssl.com/guide/pem-der-crt-and-cer-x-509-encodings-and-conversions/) (.p7b file extension).\n\nFirst we have to setup the necessary dependencies:\n\n```scala\n   // in your build.sbt or project/Dependencies.scala\n  // enabling signing and signature verification for SOAP messages\n  lazy val webServiceSecurity = Seq(\n    \"org.apache.wss4j\" % \"wss4j\"                    % \"2.3.1\" pomOnly (),\n    \"org.apache.wss4j\" % \"wss4j-ws-security-dom\"    % \"2.3.1\",\n    \"org.apache.wss4j\" % \"wss4j-ws-security-common\" % \"2.3.1\"\n  )\n\n  libraryDependencies ++= webServiceSecurity\n```\n\nNext, we continue with a scala representation of our certificate we are using for signing:\n\n```scala\n  import org.apache.wss4j.dom.WSConstants\n  \n  // algorithm configuration\n  object SigningCertificate {\n    val CanonicalizationMethodURI: String = \"http://www.w3.org/2001/10/xml-exc-c14n#\"\n    val DigestAlgorithmURI: String        = DigestMethod.SHA256\n    val SignatureAlgorithmURI: String     = \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\"\n    val KeyIdentifierType: Int             = WSConstants.SKI_KEY_IDENTIFIER\n  }\n\n  case class SigningCertificate(keyStore: KeyStore, password: String) {\n    require(\n      keyStore.aliases().asScala.size == 1,\n      s\"Certificate of Keystore needs to have one alias but had ${keyStore.aliases().asScala.size}\"\n    )\n    val alias: String = keyStore.aliases().nextElement()\n\n    override def toString: String = s\"SigningCertificate(alias=$alias)\"\n  }\n```\nIn the documentation of the SOAP service that you want to call should stand some information regarding the canonicalization method, signature algorithm, digest algorithm, and the key identifier type. Those are algorithms and information that define the signing process and we explain roughly now.\n\nBefore signing a message it has to be canonicalized. \"Canonicalization is a method for generating a physical representation, the canonical form, of an XML document that accounts for syntactic changes permitted by the XML specification\" (from [here](https://www.di-mgt.com.au/xmldsig-c14n.html)). In our case, the Exclusive XML Canonicalization is used.\n\nThe digest algorithm is used to ensure the integrity of the message during the verification of a signature. The algorithm is used to calculate a hash of the signed message. It should be documented in the SOAP service documentation. Here we will use SHA256 as a hashing algorithm.\n\nThe signature algorithm describes how the message will be signed. It can be defined in the SOAP service documentation but in the worst case you can read this algorithm from the certificate itself by using [`keytool`](https://docs.oracle.com/en/java/javase/12/tools/keytool.html):\n```bash\n$ keytool -list -v -keystore signature.p12\nEnter keystore password: ...\n\n[...] # more information about the certificates\n\nSignature algorithm name: SHA256withRSA # thats what we are after!\n\n[...] # more information about the certificates\n```\nAccording to the keytool inspection we will use SHA256withRSA (http://www.w3.org/2001/04/xmldsig-more#rsa-sha256) for signing.\n\nLast but not least, in our signature, a `<KeyInfo>` element is included. This element contains information about the public key of the sender (us) and is needed for the signature verification once the message is received (read more [here](https://www.xml.com/pub/a/2001/08/08/xmldsig.html)). Since we have our public key provided we don't need to do much here. The `KeyIdentifierType` describes which form of key identifier is used to present the public key information.\n\nHaving all this information about our certificate in place, we build the mechanism to load in our signing certificate. For this, we create the object `KeyStoreBuilder`.\n\n```scala\nimport java.io.{File, FileInputStream}\n\nobject KeyStoreBuilder {\n\n  def loadSigningCertificate(signingCertificate: File, password: String): SigningCertificate = {\n    val fis = new FileInputStream(signingCertificate)\n    val ks: KeyStore               = KeyStore.getInstance(\"PKCS12\")\n    ks.load(fis, password.toCharArray)\n    SigningCertificate(ks, password)\n  } \n}\n```\nBear in mind, that you probably **don't** want to version any sensitive information like private keys and passwords hard-coded or in any environment variables, so a safe mechanism for storing/fetching passwords and certificates (like [Vault](https://www.hashicorp.com/products/vault)) should be in place.\n\nWith the signing certificate in place, we can actually start signing a message. The next code example contains quite some Java boilerplate from wss4j that is required to make the signing mechanism work.\n\nTo restrict the usage of Java classes to a small portion of our code we will firstly implement a conversion method `.toElem` inside of the companion object `SigningService`:\n\n```scala\n  import java.io.StringWriter\n  import javax.xml.transform.{OutputKeys, TransformerFactory}\n  import javax.xml.transform.dom.DOMSource\n  import javax.xml.transform.stream.StreamResult\n\n  import org.w3c.dom.Document\n\n  import scala.xml.Elem\n\n  object SigningService {\n    implicit class RichDocument(document: Document) {\n      private val tf = TransformerFactory.newInstance()\n\n      def toElem: Elem =\n        val transformer = tf.newTransformer()\n        transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, \"yes\");\n        val stringWriter = new StringWriter()\n        transformer.transform(new DOMSource(document), new StreamResult(stringWriter))\n        scala.xml.XML.loadString(stringWriter.getBuffer.toString)\n    }\n  }\n```\nWith that, we can convert any `Document` SOAP message representation back to the `scala.xml` supported  `Elem` format.\n\n```scala\nclass SigningService(signingCertificate: SigningCertificate) {\n\n  // importing our conversion method\n  import SigningService.RichDocument\n\n  /**\n    * REQUIRED, otherwise it will throw:\n    *\n    * org.apache.wss4j.common.ext.WSSecurityException:\n    * You must initialize the xml-security library correctly before you use it.\n    * Call the static method \"org.apache.xml.security.Init.init();\"\n    * to do that before you use any functionality from that library\n    */\n  org.apache.xml.security.Init.init()\n  \n  private val documentBuilderFactory = DocumentBuilderFactory.newInstance()\n  private val crypto: Merlin = getCrypto\n\n  crypto.setKeyStore(signingCertificate.keyStore)\n\n  def signElement(elem: Elem): Elem = {\n    documentBuilderFactory.setNamespaceAware(true)\n    // converting Elem to Document (Scala to Java conversion)\n    val doc = documentBuilderFactory.newDocumentBuilder().parse(new InputSource(new StringReader(elem.toString())))\n\n    // WSSecHeader wraps around the document we want to sign\n    val header = new WSSecHeader(doc)\n    header.setMustUnderstand(true)\n    header.insertSecurityHeader()\n\n    // start building Signature, use the (wrapper) header-instance\n    val builder = new WSSecSignature(header)\n    builder.setUserInfo(signingCertificate.alias, signingCertificate.password)\n\n    // setting algorithms\n    builder.setSignatureAlgorithm(SigningCertificate.SignatureAlgorithmURI)\n    builder.setSigCanonicalization(SigningCertificate.CanonicalizationMethodURI)\n    builder.setDigestAlgo(SigningCertificate.DigestAlgorithmURI)\n    builder.setKeyIdentifierType(SigningCertificate.KeyIdentifierType)\n    builder.setAddInclusivePrefixes(true)\n\n    // signing the document!\n    val signedDocument = builder.build(crypto)\n    // conversion back to Elem\n    signedDocument.toElem\n  }\n\n  private def getCrypto: Merlin = {\n    val properties = new Properties()\n    properties.setProperty(\"org.apache.wss4j.crypto.provider\", \"class org.apache.ws.security.components.crypto.Merlin\")\n    CryptoFactory.getInstance().asInstanceOf[Merlin]\n  }\n}\n```\n\nWss4j is a library that maintains an internal state during a signing process, but to avoid confusion it can be summarized as:\n1. `WSSecHeader` wraps around the document to be signed\n2. the WSSecHeader instance `header` will be used as part of the `WSSecSignature`-Builder\n3. the WSSecSignature instance `builder` gets configured with all necessary information, which algorithms are used for signing, digesting, canonicalization, which key identifier should be included. Those settings an vary from webservice to webservice.\n\nThe actual signing of the document, which is now nested like a matryoshka doll, is happening with the help of an instance of `Crypto`. `Crypto` will contain either a keystore or a truststore or even both. It needs to be specified in the `crypto.properties` file or a runtime which class of Crypto will be used.\n The most common one is [`Merlin`](https://ws.apache.org/wss4j/apidocs/org/apache/wss4j/common/crypto/Merlin.html).\nWe have decided to specify its configuration during runtime, since it is more visible than a properties file. Nevertheless, the `crypto.properties`-file needs to exist in your `resources` folder neverthless otherwise you will get a following `WSSecurityException`:\n```java\n  org.apache.wss4j.common.ext.WSSecurityException: No message with ID \"resourceNotFound\" found in resource bundle \"org/apache/xml/security/resource/xmlsecurity\"\n  [... rest of stacktrace ...]\n  Cause: java.nio.file.NoSuchFileException: crypto.properties\n```\n\nAnd that's it! The `KeyStoreBuilder` helps us to load a `SigningCertificate` and the `SigningService` uses this loaded certificate to sign SOAP messages. \nA receiver of our SOAP message has all the necessary information in our signature to verify that this message has not been tampered with and we are the original sender.\n\nThis verification is something we should also do on our side for incoming messages. So let's take a look at how we can verify the signature of received messages.\n\n## Verification of SOAP messages\n\nVerifying the signature of incoming messages is equally important to ensure that the connection is secure. A verification process will tell you if the message is coming from a trusted source and has not been tampered with.\n\nAs previously mentioned we need our source of truth, a pool of trusted public keys from all parties which will receive our SOAP messages. These build the basis of the trust store.\n\nWe will create a `TrustedCertificates` wrapper class in which we will load in the trust store and add this method to the `KeyStoreBuilder`.\n```scala\ncase class TrustedCertificates(keyStore: KeyStore)\n\nobject KeyStoreBuilder {\n\n    def loadTrustedCertificate(certificates: Seq[File]): TrustedCertificates = {\n    val ks = KeyStore.getInstance(KeyStore.getDefaultType)\n    // we just want the keystore to act as a truststore (only containing trusted certificates), so we initialize it empty\n    ks.load(null, null)\n    val cf = CertificateFactory.getInstance(\"X.509\")\n    certificates.foreach { file =>\n      CloseableUtil.using(getClass.getResourceAsStream(file.getPath)) { fis =>\n        val certPath = cf.generateCertPath(fis, \"PKCS7\")\n        certPath.getCertificates.asScala.toList.foreach { certificate =>\n          ks.setCertificateEntry(file.getName, certificate)\n        }\n      }\n    }\n    TrustedCertificates(ks)\n  }\n}\n```\nThis trust store is under the hood also just a KeyStore, without containing a private key that requires a password, that's why we can initialize the KeyStore with `null`-parameters.\n\nNow, the SigningService needs to be extended with this trusted certificates and a `verifySignatureOf`-method:\n\n```scala\nimport java.io.StringReader\nimport java.util.Properties\nimport javax.xml.parsers.DocumentBuilderFactory\n\nimport org.apache.wss4j.common.crypto.{ CryptoFactory, Merlin }\nimport org.apache.wss4j.dom.engine.WSSecurityEngine\nimport org.xml.sax.InputSource\n\nimport scala.util.{Failure, Success, Try}\nimport scala.xml.Elem\n\nclass SigningService(signingCertificate: SigningCertificate, trustedCertificates: TrustedCertificates) {\n\n    private val engine = new WSSecurityEngine()\n    private val documentBuilderFactory = DocumentBuilderFactory.newInstance()\n    private val crypto: Merlin = getCrypto\n\n    crypto.setKeyStore(signingCertificate.keyStore)\n    crypto.setTrustStore(trustedCertificates.keyStore)\n\n    def verifySignatureOf(elem: Elem): Boolean = {\n      documentBuilderFactory.setNamespaceAware(true)\n      val doc = documentBuilderFactory.newDocumentBuilder().parse(new InputSource(new StringReader(elem.toString())))\n\n      Try(engine.processSecurityHeader(doc, null, null, crypto)) match {\n        case Success(_) => true\n        case Failure(exception) =>\n          // replace with proper logging\n          println(\n            s\"Unsuccessful signature verification, it is most likely that the certificate used for signing is not in our Truststore: ${exception.getMessage}\")\n          false\n      }\n  }\n\n  private def getCrypto: Merlin = {\n    val properties = new Properties()\n    properties.setProperty(\"org.apache.wss4j.crypto.provider\", \"class org.apache.ws.security.components.crypto.Merlin\")\n    CryptoFactory.getInstance().asInstanceOf[Merlin]\n  }\n}\n```\n\nAnd with that, we have completed our roundtrip of signing and verifying SOAP messages!\n\nHere are gists, articles, and documentation that inspired and helped us to figure out the signing and verification process for our SOAP client. Feel free to check them out!\n\n* * *\n\n### Sources\n\n[WSSecurityVerifier by Luis Wolff](https://gist.github.com/luiswolff/1d388ec8c1d63cfb58974a6f826bc1be) \n\n[WSSecuritySigner by Luis Wolff](https://gist.github.com/luiswolff/64d15a99fbb5ec4b4e90eec04b09e053)\n\n[Unit Tests from ws-wss4j](https://github.com/apache/ws-wss4j/blob/master/ws-security-dom/src/test/java/org/apache/wss4j/dom/message/SignatureTest.java)\n\n[An Introduction to XML Digital Signatures](https://www.xml.com/pub/a/2001/08/08/xmldsig.html)\n\n[SOAP vs. REST](https://stackify.com/soap-vs-rest/)","meta":{"title":"Signing and verifying SOAP messages with wss4j and Scala","description":"This blogpost will explain with code examples how we at Vandebron are signing and verifying SOAP messages for our latest SOAP client implementation.","createdAt":"Mon Jun 28 2021 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/soap.jpg","imageSource":"https://cdn.pixabay.com/photo/2020/03/15/18/36/wash-4934590_960_720.jpg","tags":"SOAP, xml, scala, wss4j, signature, verification","author":"Katrin Grunert","slug":"blog/how-to-sign-soap-messages","formattedDate":"28 juni 2021","date":"Mon Jun 28 2021 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nAt the end of March, we organized a public hackathon to create solutions to tackle climate challenges. After having done internal hackathons, we thought it was time to share our technologies with other innovative people and companies. Together with a group of partners, and enthusiastic participants, spend three full days of (remote) hacking with great results.\n\n### Why organize a public hackathon?\n\nClimate change is one of the many pressing challenges our society is currently facing. At [Vandebron](https://vandebron.nl/), we want to continue finding ways to tackle this immense challenge. That’s why we decided to organize a 3-day GreenTech hackathon that ran from March 31st to April 2nd, 2021. We've been organizing internal hackathons for the past four years, to foster innovation within our company and allow our developers to work on something exciting without any constraints. If you want to read more about why we organize internal hackathons, you can find an article by our CTO [here](https://www.vandebron.tech/blog/power-regular-hackathons). \n\nBy organizing a public hackathon, we hoped to attract a bigger audience, possibly even outside our country, The Netherlands, and attract partners to work together with. We succeeded in both, and together with [Hack the Planet](https://hack-the-planet.io/) and [Top Dutch Solar Racing](https://solarracing.nl/), we wanted to find technological solutions to problems in wildlife conservation and renewable energy. For these three days, all participants got the opportunity to work on challenges from our partners, access their technology and knowledge, and got the chance to win unique prizes. Also, we organized a free event with speakers Florian Dirkse ([The Ocean Cleanup](https://theoceancleanup.com/)), Thijs Suijten (Hack the Planet) and Heleen Klinkert ([Nieuw Groen](https://nieuw-groen.nl/)). \n\n### Looking back\n\nThe event started on March 31st, when all hackathon challenges were presented and the participants could select which challenge they wanted to work on. People from all over The Netherlands (and even beyond) signed up for the hackathon, ranging from students from the University of Amsterdam to young professionals looking for a job. The first challenge the participants could subscribe to was from Vandebron itself, where teams got the opportunity to use a selection of our Electronic Vehicle (EV) data. With this data, they could for example make a forecast on the amount of charging sessions we could expect on a typical day. Second, our partner Hack the Planet presented their challenge that was aimed at thinking of innovative solutions for their project [Hack the Poacher](https://www.hackthepoacher.com/). With Hack the Poacher, they install smart camera traps in African wildlife reservations to detect poachers. The teams could use their camera traps and data to create more solutions to map the poachers or use the camera traps for other needs. Finally, the students from Top Dutch Solar Racing presented a challenge to simulate the race they were supposed to join at the end of the year in Australia. Using their weather and traffic data, the teams could simulate the race and predict how much time they would need to complete the race. After selecting a challenge, all teams started the hackathon and participated in sessions to learn more about the challenges to get started.\n\nAll teams continued working on the hackathon challenge on the second day, after a nice warming-up quiz about climate change in the morning. For most teams this second day was when their project started to take shape, and they got a better idea about what they would be presenting on the final day. This second day was also an opportunity for non-technical people to get to know Vandebron and their partners better as we organized inspirational sessions with talks from different speakers in the afternoon. One of the co-founders from The Ocean Cleanup, Florian Dirkse, inspired us with his story behind making a difference in the world. After which, one of our hackathon partners Thijs Suijten, from Hack the Planet, demonstrated how technology can be used for the good. Our third, and final, speaker Heleen Klinkert (Nieuw Groen), showed how we can compensate for our CO2 emissions by storing them in the soil.\n\nOn the final day of the hackathon, all teams had to finalize their projects and create a presentation for the closing ceremony. During this ceremony, all participants and partners looked back at the past three days and shared what they had been working on during the hackathon. For every challenge, one team could win and take home several prizes, sponsored by [Marie-Stella-Maris](https://marie-stella-maris.com/), [EV Experience](https://evexperience.nl/), and [Klimaatroute](https://www.klimaatroute.nl/). The first presentations were for the Vandebron challenge about EV forecasts. This challenge was won by not one but two teams as the jury and audience were so impressed by their solutions. Both teams created not only the forecast based on the sample data provided, but also created interactive dashboards. On the challenge for Hack the Planet, the team that won came up with a unique solution to use the camera traps to detect wild animals on the streets. For countries like India, this is a huge problem, as wild animals get stuck in traffic or walk through rural areas. The final winner of the hackathon was a group of students that simulated the Top Dutch Solar Racing trip through Australia and forecasted they could complete the race within 7 days.\n\n### Thanks everyone\n\nI'd like to thank all the participants, prize/challenge partners, and speakers for their efforts during these days. The GreenTech Hackathon 2021 was a huge success thanks to everyone that has been involved. Keep following the [vandebron.tech](https://vandebron.tech) to be updated on future hackathons and events.\n","meta":{"title":"Looking back at the Vandebron GreenTech Hackathon 2021","description":"At the end of March, we organized a public hackathon to create solutions to tackle climate challenges. After having done internal hackathons, we thought it was time to share our technologies with other innovative people and companies.","createdAt":"Mon Apr 05 2021 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/looking-back-at-vandebron-greentech-hackathon-2021.png","tags":"hackathon, innovation","author":"Roy Derks","slug":"blog/looking-back-at-vandebron-greentech-hackathon-2021","formattedDate":"5 april 2021","date":"Mon Apr 05 2021 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nAt Vandebron we have been organizing a regular hackathon for the last four years. Every three months we organize a two-day event. At first glance this seems quite an investment. Eight days a year, almost losing two working-weeks of productivity for your teams! \n\nOur company is like any other. Our roadmaps are stuffed, our backlogs are never-ending and pressure for delivering value to our customers is always present. Our ambitions are always higher than what we can handle with the amount of teams and people available. We like to say: ‘the energy transition can’t wait!’, but we sure do have to prioritize our projects very carefully.\n\nHowever this does not stop us from organizing our quarterly hackathons. Most of the time our regular hackathons are light-weight. People within the company know how it works. We try not to waste too much time in ‘organizing’ the event. We get right to it. \n\n#### Reasons why you should be organizing (regular) hackathons:\n- Fun - this reason does not need much explanation. Working on challenging, fun and creative ideas with uncertain outcome in a not-business-as-usual way. It makes you step out of your daily comfort zone and explore new things. \n- Walk the extra mile - Some team-members will have the energy, enthusiasm and commitment to use their spare time to fuel their curiosity and bring new insights to the workplace. These are the same people that you also expect to walk the extra mile if the team- or company objectives are at stake. This is in that same spare time! But in the end, if you value your teams to continuously think about new ideas, insights and work on out-of-the-box ideas, it is not a weird idea to create this environment within the company.\n- Bottled up energy - our people are focused on reaching goals and objectives. Every day, every week and every sprint the focus is clear. This also means that there is not always time for creative or high risk escapades that could hurt the overall team objectives. This might give an unsatisfied feeling to people. If the bottled up energy can not be released, engineers might get frustrated. But maybe even more important, you might be missing opportunities for the company.\n- Cross team collaboration - in an agile way of working the concept of the team is very important. At Vandebron we focus on teams staying together for a long period of time. This makes the relationship between individuals stronger, the knowledge of the product deeper and the team as a whole more effective. However, the company is bigger than your team. There might be different ways of connecting with other people within your company, but a hackathon is an ideal way of linking yourself up with people that you can learn from. It can really bring you new insights as an individual, and it will also be an investment for improved cross-team collaboration going forward.\n- Learning organisation - as mentioned, hackathons give you an excellent opportunity to learn new things. For yourself, but definitely also for the company. In my experience I often see that topics get selected that have high-risk and high-reward kind of characteristics. These topics can be scary to touch, which make you go out of your comfort zone. This is where you learn the most! These high-risk and high-reward projects are also very likely to fail, meaning that the reward is not as high as expected, or the complexity and risks are even greater than anticipated. At these moments the pressure-cooker of a hackathon is very valuable, because it forces the participants to draw conclusions in a short time-frame. The insights gained from these projects can be used to further steer the roadmap. And last but not least, it supports building a culture of being bold enough to try new things, and fail fast. I’ve noticed this is appreciated by a lot of people within the company and the hackathon contributes to a culture of innovation.\n\n#### Our most important learnings over the years\n- Spotlights on - It is good to put teams and their results in the spotlight. Let them take the podium and make sure there is an audience. However don’t make it too much about winning. Ideas that have completely failed are just as important as over-engineered fancy product demos. At Vandebron we like to declare ‘winners’ in different categories: ‘Fun & Original’, ’Impactful’, ‘Exploration & Learning’ and ‘Technical achievement’. \n- Harvest your ideas continuously - during normal work and life you hit those topics that you would like to investigate a bit deeper. But while you stumble upon such a topic you don’t have the time to dive into it. So therefore, write your idea down and publish it in the ‘hackathon-idea-box’ for everyone to see! It might already give you some good conversations during coffee or lunch, and it might already generate you some people that would like to join forces with you during the hackathon. Because rest assured, a new hackathon is always coming up!\n- To-theme-or-not-to-theme - we have experimented with adding a theme to a hackathon. It can help the company to generate ideas and action in a certain area of interest. It also helps to generate more focus within the company on a certain persistent challenge that we feel deserves a solution. Although everyone will be working on different sub-topics the full event will be experienced as more correlated and unified. But be careful not to push normal business-projects disguised as hackathon projects to your teams. This goes against the basic concept of a hackathon. At Vandebron we sometimes pick a theme if we would like to motivate people to think about ideas in a certain direction. But most of the time we keep it open.\n- Participation is optional. - At Vandebron we have autonomous teams with professionals that can manage their own agenda. As a team and as an individual. We put effort in promoting the hackathon by trying to make people enthusiastic about participating. But in the end people make their own decisions. Sometimes the team and company objectives do need to have priority, but the teams are perfectly able to make this judgement call themselves.\n- Magnify impact - show everyone what the impact is they have been making. It is good if people recognize how some projects have become reality and that feedback will be appreciated by the community. It gives people a feeling that the podium of the hackathon is a strong force. And ultimately that is how you also proof the value of organizing a hackathon.\n\nFor our next hackathon we are opening our (virtual) doors also for guests, as we are organizing a GreenTech hackathon with other sustainability minded companies (‘Hack the Planet’ and ‘Top Dutch Solar Racing’). You can find more information and sign up via [this link](https://www.vandebron.tech/greentech-hackathon). It is the first time we do it like this, and we sure will learn another thing or two!\n","meta":{"title":"The power of regular hackathons","description":"At Vandebron we have been organizing a regular hackathon for the last four years. Every three months we organize a two-day event. At first glance this seems quite an investment. Eight days a year, almost losing two working-weeks of productivity for your teams!","createdAt":"Fri Mar 19 2021 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/power-regular-hackathons.png","tags":"hackathon, innovation, scrum","author":"Arno van den Berg","slug":"blog/power-regular-hackathons","formattedDate":"19 maart 2021","date":"Fri Mar 19 2021 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nIn October 2020 D2IQ [announced](https://d2iq.com/blog/d2iq-takes-the-next-step-forward) that they are moving onwards with their Kubernetes offering. Vandebron has been a D2IQ customer for their DCOS offering, we were just in the middle of a migration of our first workloads to DCOS Enterprise. We have evaluated the D2IQ K8s offering and decided to go for another Kubernetes product. We had a few migrations over the years, we migrated from Azure to AWS, we migrated workloads from normal instances to spot instances and all these migrations were done with nearly any downtime. We plan to reduce the downtime to a couple of minutes this migration and this is a real challenge. The first challenge that we will discuss today: We want to pair our Kubernetes clusters to the DCOS/Mesos clusters, while we move a workload it should be able to connect to its dependencies in the DCOS cluster. We use DCOS for our NoSQL databases like Cassandra, internal data that we want to keep internal. Pairing DCOS and Kubernetes clusters enable us to reduce downtime, enabling us to switch back if we run into issues and move faster because it reduces complexity.\n\n## L4LB\n\nThe internal layer 4 load balancer DCOS provides is used in the majority of our workloads. When our data scientists schedule a spark driver, they connect to the spark dispatcher through the Layer 4 load balancer. Most of the DCOS frameworks use this Layer 4 load balancer as an internal service discovery tool, with Vandebron we use this layer 4 load balancer to communicate between services. In a default DCOS set up this load balancer responds on domain names like: `spark-dispatcher.marathon.l4lb.thisdcos.directory:7077`\n\nWhen we ping the spark dispatcher we get the following:\n\n```bash\nPING spark-dispatcher.marathon.l4lb.thisdcos.directory (11.155.161.35) 56(84) bytes of data.\n64 bytes from 11.155.161.35 (11.155.161.35): icmp_seq=1 ttl=64 time=0.024 ms\n```\n\nAfter some investigation we found out that this IP range is not actually on a network interface, it is a Linux kernel functionality called `IPVS`. With IPVS you can do layer 4 load balancing, you provide the target location and the location you want to respond on.\n\nWhen we search for the IP from the spark dispatcher with ipvsadm, we get 3 results:\n\n```bash\nsudo ipvsadm -L -n |grep --color '11.155.161.35\\|$'\nTCP  11.155.161.35:80 wlc\n  -> 10.2.7.146:16827             Masq    1      0          0\nTCP  11.155.161.35:4040 wlc\n  -> 10.2.7.146:16826             Masq    1      0          0\nTCP  11.155.161.35:7077 wlc\n  -> 10.2.7.146:16825             Masq    1      0          0\n````\n\nAs you can see the IP `11.155.161.35` points towards `10.2.7.146`, even the ports are configured and forwarded. We can add our route with ipvsadm, to understand IPVS a bit better. For example:\n\n```bash\nsudo ipvsadm -A -t 1.2.3.4:80 -s wlc # we add the target server and assign the scheduler\nsudo ipvsadm -a -r 10.2.7.146:16825 -t 1.2.3.4:80 -m # we configure the real server and target server and configure Masquerading\ncurl 1.2.3.4:80\n{\n  \"action\" : \"ErrorResponse\",\n  \"message\" : \"Missing protocol version. Please submit requests through http://[host]:[port]/v1/submissions/...\",\n  \"serverSparkVersion\" : \"2.3.4\"\n}\n```\n\nThis results in that the spark dispatcher now also is available on `1.2.3.4:80`. As mentioned before we wanted to connect our DCOS and Kubernetes clusters, getting hundreds of entries from ipvsadm and manually adding them one by one didn’t sound appealing to us. Especially if you consider that sometimes services fail and run on a different port or different host after recovery, maintaining this by hand would be a nightmare. We therefore decided to build a tool to sync IPVS entries from DCOS to Kubernetes.\n\n## Stack\n\nWithin Vandebron we have our tech stack, we strongly believe it is good to eat your own dog food. When possible and when our use cases are similar we use the same tools as our Developers use. The parts of the stack we will be using are:\n\n- AWS ELB in front of Traefik 1.7\n- DCOS\n- Kubernetes\n\nWithin our platform team, we use Golang as our scripting language. Golang gives us the ability to build binary files with all the required libraries in the binary, we don’t have to install any packages, we do not even need to install Golang on the machine the application will be running on.\n\nIn our DCOS cluster we use Traefik 1.7, this version of Traefik only forwards HTTP requests. We decided to use Traefik to expose a JSON endpoint so we can gather the IPVS information from this location.\n\n## ipvs-server\n\nWithin our DCOS cluster we will expose the IPVS information through a JSON endpoint. We have built a tool for this to expose this information in multiple ways. In the next section, we are going to discuss some of the concepts and choices we made, we won’t deep dive into Go specifics. We have provided the entire code for this project in the examples directory of our GitHub repo:\n<https://github.com/Vandebron/tech-blog>\n\nFirst, let’s discuss the library we use: <https://github.com/nanobox-io/golang-lvs>. This library in its essence translates to ipvsadm commands, it helped save us time to implement this ourselves. There are some gotcha’s, such as newlines are not filtered out from the output. We solved this by cleaning up some of the data.\n\nIn the `childChan` function we create a go channel that is responsible for polling `ipvsadm` every 10 seconds and stores the result in a couple of variables we use in our HTTP endpoints. IPVS is a Linux kernel functionality and should be highly performant, we do not want to trigger kernel panics when the server gets overloaded with requests. We expect that every 10 seconds gives us accurate enough results, we can always lower this interval to ensure faster results. We also added in this function the string manipulation to ensure all the newlines were gone in the JSON output. The newline gave issues when we tried to add the IPVS scheduler entries.\n\n```go\nfunc childChan(c chan bool) {\n   fmt.Println(\"Starting time based IPVS Admin poll\")\n\n   pollInterval := 10\n   timerCh := time.Tick(time.Duration(pollInterval) * time.Second)\n   // Time based loop to generate Global variable\n   for range timerCh {\n       select {\n       // when shutdown is received we break\n       case <-c:\n           fmt.Println(\"Received shutdown, stopping timer\")\n           break\n       default:\n           var err error\n           listIpvs.Save()\n           ipvsString = fmt.Sprintln(listIpvs.Services)\n\n           res := &responseObject{\n               Services: listIpvs.Services,\n           }\n \n           ipvsJSONbyte, err := json.Marshal(res)\n           if err != nil {\n               logToErr.Printf(\"ERROR: -- Marshal JSON -- %v\\n\", err)\n           }\n \n           ipvsString = string(ipvsJSONbyte)\n           ipvsJSON = strings.Replace(ipvsString, `\\n`, ``, -1)\n           if debug != false {\n               logToOut.Println(\"DEBUG: -- ipvsJSON --\", ipvsJSON)\n           }\n       }\n   }\n}\n```\n\nNext is the index handler, we set our headers correctly and print the result as we would receive through ipvsadm. The index is mainly for our platform engineers to debug and verify the output. Thanks to this overview we found much faster that there was a newline hidden in the scheduler output.\n\n```go\nfunc index() http.Handler {\n   // Generating the Index\n   return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\n       // Only available when debug is on\n       if debug != false {\n           logToOut.Println(\"DEBUG: -- index --\", ipvsString)\n       }\n \n       if r.URL.Path != \"/\" {\n           http.Error(w, http.StatusText(http.StatusNotFound), http.StatusNotFound)\n           return\n       }\n       w.Header().Set(\"Content-Type\", \"text/plain; charset=utf-8\")\n       // Site security testers expect this header to be set\n       w.Header().Set(\"X-Content-Type-Options\", \"nosniff\")\n       w.WriteHeader(http.StatusOK)\n       fmt.Fprintln(w, ipvsString)\n   })\n}\n```\n\nThe JSON endpoint is what we use in the client communicate with the server. \n\n```go\nfunc jsonz() http.Handler {\n   // Generating the Index\n   return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\n       // Only available when debug is on\n       if debug != false {\n           logToOut.Println(\"DEBUG: -- jsonz --\", ipvsJSON)\n       }\n \n       if r.URL.Path != \"/json\" {\n           http.Error(w, http.StatusText(http.StatusNotFound), http.StatusNotFound)\n           return\n       }\n       w.Header().Set(\"Content-Type\", \"application/json; charset=utf-8\")\n       // Site security testers expect this header to be set\n       w.Header().Set(\"X-Content-Type-Options\", \"nosniff\")\n       w.WriteHeader(http.StatusOK)\n       fmt.Fprintln(w, ipvsJSON)\n   })\n}\n```\n\nWe ask our Developers often to implement a basic health endpoint, in DCOS we use this to see if a service needs to be restarted. In our application we enable set the statusOK in the index or in the JSON endpoint.\n\n```go\nfunc healthz() http.Handler {\n   // Generating the healthz endpoint\n   return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n       if atomic.LoadInt32(&healthy) == 1 {\n           w.WriteHeader(http.StatusNoContent)\n           return\n       }\n       w.WriteHeader(http.StatusServiceUnavailable)\n   })\n}\n```\n\nIn our logging and tracing functions we want to register the clients that are connecting, this gives us information where calls are coming from. It helps us debugging if we see weird behaviour.\n\n```go\nfunc tracing(nextRequestID func() string) func(http.Handler) http.Handler {\n   // Tracing the http requests so its easier to check if server is reached\n   return func(next http.Handler) http.Handler {\n       return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n           requestID := r.Header.Get(\"X-Request-Id\")\n           if requestID == \"\" {\n               requestID = nextRequestID()\n           }\n           ctx := context.WithValue(r.Context(), requestIDKey, requestID)\n           w.Header().Set(\"X-Request-Id\", requestID)\n           next.ServeHTTP(w, r.WithContext(ctx))\n       })\n   }\n}\n\nfunc logging(logToOut *log.Logger) func(http.Handler) http.Handler {\n   // Creating logging entry tracing the http requests\n   return func(next http.Handler) http.Handler {\n       return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n           defer func() {\n               requestID, ok := r.Context().Value(requestIDKey).(string)\n               if !ok {\n                   requestID = \"unknown\"\n               }\n               logToOut.Println(requestID, r.Method, r.URL.Path, r.RemoteAddr, r.UserAgent())\n           }()\n           next.ServeHTTP(w, r)\n       })\n   }\n}\n```\n\nIPVS needs to be executed with root privileges, to ensure this is correct we get the userid and print it when starting the server.\n\n```go\n// getProcessOwner function is to see who is running the process. It needs to be a sudo / root user\nfunc getProcessOwner() string {\n   stdout, err := exec.Command(\"ps\", \"-o\", \"user=\", \"-p\", strconv.Itoa(os.Getpid())).Output()\n   if err != nil {\n       logToErr.Printf(\"ERROR: -- getProcessOwner -- %v\\n\", err)\n       os.Exit(1)\n   }\n   return string(stdout)\n}\n```\n\nWe added the init function to ensure we print results the moment the server starts up, if we would not do this it would take 10 seconds for the go channel to activate\n\n```go\nfunc init() {\n   // Placing the Save and val in the init, else we will need to wait for channel to perform its first run\n   listIpvs.Save()\n   ipvsString = fmt.Sprintln(listIpvs.Services)\n}\n```\n\nIn the main function, we set the configurable flags, such as debugging to show error messages. It proved useful during the creation of this tool to keep track and print output. If we would print the output at every call to our logs, our Elastic cluster would get thousands of logs that add little to no value.\n\nWe configure the listen port in the flags, we can use the portIndex from DCOS to assign a random port on the host to listen on. We also provided to print the version we are running. In our versioning, we use a constant to list the application semver version, we also provide the git-commit hash.\nWhen we begin the server we print the version information, the port we listen on and the user running the process. We then start the server process with the go channel, in setting up the go channel we ensure that when the server stops we try to gracefully stop the server within a 30-second timeframe. Since our ipvsadm timer is 10 seconds it should be able to cleanly shutdown within that period.\n\n### Docker build\n\nIn the repository, we have included a Dockerfile and a script to build the Dockerfile. In this Dockerfile, we pass the git commit hash to the go install. This way we always get the Git Hash from our GitHub repo and we can use this information in our version output.\n\n### DCOS service.json\n\nIn the repository, we have provided the service.json file, since it is opinionated on using Traefik you might need to change it. But in this service.json you see how we set up Traefik, the health check, and port index. Since the Mesos UCR container has fewer abstractions and has fewer limited capabilities. We can run the IPVS server inside a UCR container and get all the output as if we were running this directly as root on the host machine.\n\n## ipvs-client\n\nThe IPVS client is the component we use in the Kubernetes environment. The client connects to the server and gets the IPVS entries from the IPVS server inside our DCOS cluster. It then adds these IPVS entries to each node in the Kubernetes cluster. You, therefore, need to run each client per Kubernetes node.\n\nYou can find the code from the IPVS client in our repository.\n\n```go\nfunc httpGet(remoteURL string) []byte {\n   if debug != false {\n       _, err := url.ParseRequestURI(remoteURL)\n       if err != nil {\n           panic(err)\n       }\n   }\n\n   req, err := http.NewRequest(http.MethodGet, remoteURL, nil)\n   if err != nil {\n       logToErr.Fatalf(\"ERROR: -- new HTTP request -- %v\", err)\n   }\n\n   ipvsClient := http.Client{\n       Timeout: time.Second * 2, // Timeout after 2 seconds\n   }\n   req.Header.Set(\"User-Agent\", \"go-ipvs-get \\tversion: \"+version+\"\\t Git Commit: \"+gitCommit)\n   res, err := ipvsClient.Do(req)\n   if err != nil {\n       logToErr.Fatalf(\"ERROR: -- ipvsClient -- %v\\n\", err)\n   }\n\n   if res.Body != nil {\n       defer res.Body.Close()\n   }\n\n   body, readErr := ioutil.ReadAll(res.Body)\n   if readErr != nil {\n       logToErr.Fatalf(\"ERROR: -- body -- %v\\n\", readErr)\n   }\n\n   return body\n}\n```\n\nIn the httpGet function we can debug the URL and check if it is valid. Again we set the correct headers and retrieve the JSON body.\n\n```go\nfunc unmarshal(body []byte) []lvs.Service {\n\n   res := &responseObject{\n       Services: listIpvs.Services,\n   }\n\n   jsonErr := json.Unmarshal(body, &res)\n   if jsonErr != nil {\n       logToErr.Fatalf(\"ERROR: -- Unmarshal -- %v \\n\", jsonErr)\n   }\n\n   if debug != false {\n       logToOut.Fatalf(\"DEBUG: -- res -- %v \\n\", res.Services)\n   }\n\n   r := res.Services\n\n   return r\n}\n```\n\nIn the unmarshal function we unmarshal the JSON and turn it in a slice of lvs.Service.\n\n```go\nfunc addServers(remoteAddr string) {\n   body := httpGet(remoteAddr)\n   jsonData := unmarshal(body)\n\n   for i, v := range jsonData {\n       if debug != false {\n           logToOut.Printf(\"DEBUG: -- range jsonDATA --\\n\")\n           logToOut.Printf(\"ipvsCount=%v, value=%v\", i, v)\n       }\n\n       err := lvs.DefaultIpvs.AddService(v)\n       if err != nil {\n           logToErr.Printf(\"ERROR: -- AddService -- %v\", err)\n       }\n \n       i++\n       ipvsServerCount = float64(i)\n   }\n}\n```\n\nIn the addServers function we add the servers to IPVS.\n\n```go\nfunc clientChan(c chan bool) {\n   logToOut.Println(\"Starting time based IPVS Admin add\")\n\n   pollInterval := 10\n   timerCh := time.Tick(time.Duration(pollInterval) * time.Second)\n   // Time based loop to generate Global variable\n   for range timerCh {\n       select {\n       // when shutdown is received we break\n       case <-c:\n           logToOut.Println(\"Received shutdown, stopping timer\")\n           break\n       default:\n\n           logToOut.Println(\"Clearing & Adding servers...\")\n           // Before we add Servers we need to clear the existing list\n           lvs.Clear()\n           addServers(remoteAddr)\n           if debug != false {\n               logToOut.Printf(\"IPVS servers added:\\t%v\", ipvsServerCount)\n           }\n       }\n   }\n}\n```\n\nLike we did in the IPVS server we create a go channel to poll every 10 seconds the server endpoint. We perform this to get at a set interval the IPVS entries.\n\nSince we run the IPVS client as a binary directly on the Kubernetes hosts we build the binary with a few parameters we pass to the go build command. The binary we build with this command we host on an internal s3 bucket, we can download this binary with systemd unit files.\n\n```bash\nGOOS=linux\nGOARCH=amd64\nGIT_COMMIT=$(git rev-list -1 HEAD)\n\nexport GOOS\nexport GOARCH\nexport GIT_COMMIT\n\nenv GOOS=${GOOS} GOARCH=${GOARCH} go build -v -ldflags \"-X main.gitCommit=${GIT_COMMIT}\" .\n```\n\nWhen we run the IPVS client we can verify if the IPVS routes are added by running the `ipvsadm -L -n` command.\n\n### Unit files\n\nSince IPVS is part of the Linux kernel it is hard to deploy this in a docker container, the capabilities are more restricted in Kubernetes. We decided to deploy the IPVS client on each host machine through a systemd unit file, the main reason was that we ran into restrictions that slowed us down and this is not a permanent solution. By adding the IPVS client on the machines alone does not make it possible for containers to use the IPVS routes. We needed to add NET_ADMIN capabilities to all containers using the l4lb loadbalancer locations and configure `hostNetworking: true` in the Kubernetes pods.\n\nWe provided a deployment.yml file that runs a Ubuntu docker container with ipvsadm only installed extra. When the pods are deployed in this deployment you can use kubectl exec to get into the pod and run the `ipvsadm -L -n` command.\n\n## Vacancy at Vandebron\n\nWe are looking for a platform engineer in Vandebron. As you can understand this is not a typical scenario we daily run across, but it is part of the workloads that we will support when working on our platform. Within Vandebron we try to use the best technology available, when it is not available we build it. Due to this as platform engineers, we have many interesting challenges and offer engineers to support further than only a strict domain. We support all components of our entire platform, regardless if it is a Linux kernel issue like this, involves setting up and maintaining a NoSQL cluster, or helping the business with something like requesting a certificate.\n\nIf you are interested in learning more about this position, take a look at our Vacancy and get in contact with us.\n<https://werkenbij.vandebron.nl/>\n","meta":{"title":"Migrating from DCOS to Kubernetes, dealing with the l4lb loadbalancer","description":"When you want minimal downtime, you need to build your own tools","createdAt":"Fri Mar 05 2021 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/migrating-dcos-kubernetes-l4lb.jpg","imageSource":"https://pixabay.com/users/praesentator-4372890/","tags":"Kubernetes, k8s, mesos, l4lb, ipvs, ipvsadm","author":"Rogier Dikkes","slug":"blog/migrating-dcos-kubernetes-l4lb","formattedDate":"5 maart 2021","date":"Fri Mar 05 2021 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nCypress is a game-changer in the automation testing world, the way that Cypress was built and its architecture allows us as testers to cover more scenarios.\n\nCypress is not Selenium; in fact, it is different. And the way to build and design a framework should be different as well.\n\nThe most famous design technique in Selenium is the Page Object Model, and many testers use the same design technique with Cypress. Even that Cypress on their official website [recommended](https://www.cypress.io/blog/2019/01/03/stop-using-page-objects-and-start-using-app-actions/) us not to go with that approach.\n\n## Page Object Model\n\nThe main benefit of using the page object model Is to make the automation framework maintenance-friendly. We can define a specific page's selectors in a separate file and then use these selectors in our test cases.\n\n```js\nclass SignInPage {\n  visit() {\n    cy.visit(\"/signin\");\n  }\n  getEmailError() {\n    return cy.get(`[data-testid=SignInEmailError]`);\n  }\n  getPasswordError() {\n    return cy.get(`[data-testid=SignInPasswordError]`);\n  }\n  fillEmail(value) {\n    const field = cy.get(`[data-testid=SignInEmailField]`);\n    field.clear();\n    field.type(value);\n    return this;\n  }\n  fillPassword(value) {\n    const field = cy.get(`[data-testid=SignInPasswordField]`);\n    field.clear();\n    field.type(value);\n    return this;\n  }\n  submit() {\n    const button = cy.get(`[data-testid=SignInSubmitButton]`);\n    button.click();\n  }\n}\nexport default SignInPage;\n```\n\nThe main two downsides using the typical page object model with cypress are:\n\n- Page objects introduce an additional state into the tests, separate from the application’s internal state. This makes understanding the tests and failures harder.\n- Page objects make tests slow because they force the tests to always go through the application user interface.\n\n## Component-Based Architecture\n\nOn the other hand, a React application is component-based, where a specific page will be built from a collection of components. And components in React can be used on different pages too. So if we want to use the Page Object Model, we may define the same locator twice on different pages.\n\nSo having these two facts, At Vandebron, we came up with a new way to design our Cypress Automation framework by creating a separate JavaScript file for every component in our application, inside a folder called `components` within our Cypress project as below:\n\n```js\n// Locators\nexport const getEmailError = () => cy.get(`[data-testid=SignInEmailError]`);\nexport const getPasswordError = () =>\n  cy.get(`[data-testid=SignInPasswordError]`);\nexport const emailField = () => cy.get(`[data-testid=SignInEmailField]`);\nexport const passwordField = () => cy.get(`[data-testid=SignInPasswordField]`);\nexport const submitButton = () => cy.get(`[data-testid=SignInSubmitButton]`);\n\n// Actions\nexport const visit = () => cy.visit(\"/signin\");\nexport const performLogin = (email, password) => {\n  emailField().clear().type(email);\n  passwordField().clear().type(password);\n  submitButton().click();\n};\n```\n\nHaving it built this way, we eliminated all the previous problems mentioned earlier; we are not adding any classes, and we are defining objects within our test cases. And the most important part is that we are following the way that Cypress recommends it.\n\nAnd after defining the component locators and actions, we can import them inside our test case and use them as below:\n\n```js\nimport LoginComponent from \"../components/loginComponent\";\nimport Menu from \"../components/Menu\";\n\ndescribe(\"Test Login Page\", () => {\n  it(\"should show an error message if the password in wrong\", () => {\n    LoginComponent.visit();\n    LoginComponent.performLogin(\"email@gmail.com\", \"wrongPassword\");\n    LoginComponent.getPasswordError().should(\"be.visible\");\n  });\n  it(\"should show the logout button if the user logged in succesfully\", () => {\n    LoginComponent.visit();\n    LoginComponent.performLogin(\"email@gmail.com\", \"correctPassword\");\n    Menu.LogoutButton().should(\"be.visible\");\n  });\n});\n```\n\nAnd as you can see, our test cases are readable for anyone! And if any locator changes in any of the components, we can easily fix it in one location and from the same file. And lastly, if a component will be used in different places, we can use the same code.\n\nIn the next article, I will talk about how we use Cypress in our manual testing during the sprint and how it saves us tons of time and effort.\n","meta":{"title":"Cypress.io Component Design Technique for React Applications","description":"Cypress is a game-changer in the automation testing world, the way that Cypress was built and its architecture allows us as testers to cover more scenarios.","createdAt":"Fri Feb 05 2021 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/cypress-component-design-technique-for-react-applications.png","tags":"Cypress, Testing, React","author":"Hatem Hatamleh","slug":"blog/cypress-component-design-technique-for-react-applications","formattedDate":"5 februari 2021","date":"Fri Feb 05 2021 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nIn Vandebron we have been using container clusters to host our services since the foundation of our Big Data team. \nRecently our cluster of choice has declared End-Of-Life development stage, so we decided to take a step forward and get a ticket for the Kubernetes boat.\n\nA change in the OS that is used to run your services and applications can look quite challenging and not everyone is on the same experience level. To make everyone comfortable it is a good choice to give everyone the possibility to play with the new tools and learn what can be done and how: **you need a sandbox.**\n\nOur developers are provided with a Macbook and at the moment of writing there some options you can go for when deciding how to set up your playground:\n\n- **Docker CE Kubernetes**: This is the easiest solution since there is a handy button to run your containers into a Kubernetes environment.\n\n- **Vagrant and Virtualbox**: This solution is the one that can give you more control and you can easily create a cluster the size you want, but you need to be handy with VMs, the hypervisor of choice, and Vagrant. It's the old school way to do it but, while it's a chunk your platform engineers can bite, it can be a steep and frustrating process for people that are not used to handle VMs.\n\n- **Multipass + some bash magic glue**: Since Canonical created this tool for macOS, creating an Ubuntu VM became a breeze and you can have a single, easily manageable VM with its networking up and running in less than a minute, without having to handle disks, distros, and stuff. On top of it, the command line interface is straight forward and it has just the basic commands we will need, so wrapping the entire process into a bash script is a piece of cake.\n\nI have found this super cool in-depth [article](https://jyeee.medium.com/kubernetes-on-your-macos-laptop-with-multipass-k3s-and-rancher-2-4-6e9cbf013f58) from Jason Yee (kudos to you bruh) that guided me through the installation of my first single node Kubernetes cluster.\n\nThe process is not that long but it involves a lot of copy/pasting and, once learned the basics, I didn't want to go under the same process more times, plus it could be interesting for me as a Platform Engineer, but it may be boring and pointless for developers who just want to have a sandbox replica of what they are working on in the remote environment.\nMy automator (aka do-it-once-never-do-it-again) spirit kicked in and I decided to wrap every step in a small command-line tool with only 3 options:\n- **install**\n- **cleanup**\n- **help**\n\n\n### What is happening under the hood\n\nWhat the script does is substantially automating all the steps needed to:\n1. Create a new VM using Multipass (tool released by Canonical)\n2. Fetch the VM IP address and adding it to your local `/etc/hosts` file\n3. Install k3s (a lightweight distribution of Kubernetes) on top of the VM\n4. Install the Kubernetes command-line tools on your laptop\n5. Install Helm (the Kubernetes package manager) on your laptop\n6. Install cert-manager (certificate manager) package on top of your k3s cluster\n7. Install Rancher (a Kubernetes control plane) package on top of your k3s cluster\n\nIf you are looking for a more in-depth breakdown of the single steps you can download and inspect [the script](https://gist.githubusercontent.com/nikotrone/50b1a5f8d137411879eb2467e689bfbe/raw/090b4b4323d96ac28d96bbb346e2e657073722e6/bronernetes) (one of the many advantages of [OpenSource](https://en.wikipedia.org/wiki/Open_source) projects) or checkout and read the original [article](https://jyeee.medium.com/kubernetes-on-your-macos-laptop-with-multipass-k3s-and-rancher-2-4-6e9cbf013f58): it explains line by line what the specific commands are doing.\n\n#### 1. Multipass VM\n[Multipass](https://multipass.run/) is a tool from Canonical (the company developing and maintaining the Ubuntu Linux distribution) that leverages Hyperkit (macOS feature to handle virtualization) to create and handle a Virtual Machine directly on your Mac.\n\n#### 2. Edit /etc/hosts\nOnce we have our VM up and running we need to make it available with an easy url that is also gonna be used to generate the SSL certificate, in our case we picked up `rancher.localdev`.\nIt is important to have a name setup in the beginning since this one will need to match with the certificate so we can use it programmatically.\n\n#### 3. Install K3S\nThis step is pretty straightforward: just fetch a script that is publicly available on the [k3s official website](https://get.k3s.io) and feed it to your bash.\nK3s is a lightweight version of Kubernetes with all the needed dependencies and executable packaged in a convenient installation script. Because of its light nature, it is often used in embedded devices that have a limited amount of resources to offer.\n\n#### 4 & 5. Kubernetes and Helm cli\n**Kubernetes cli** (`kubectl`) is used to talk and interact with your Kubernetes cluster. It can be used to manage multiple clusters according to the content of your KUBECONFIG environment variable. \nThe variable itself contains just a path to where your cluster configuration is stored, so you can switch from a cluster to another by simply pointing to another file that contains the configuration of another cluster.\n\n**Helm** instead is the \"package manager\" of Kubernetes: you can use it to add repositories to specific `charts` which are the blueprint that contains a way to install a specific tool on your cluster.\nBoth of these tools have to be installed and run from your local laptop, either in the case you are managing a local VM or in the case you are interacting with a remote cluster.\n\n#### 6 & 7. cert-manager and Rancher\n\n**Rancher** is the control plane for our cluster: it provides a GUI and an overview of our single node cluster. It offers other goodies like management of multiple clusters, deployed on different locations like AWS Azure and GCP or even on your own hardware, plus certificate deployment and some other handy functionalities.\n\n**cert-manager** is installed via Helm chart and it is the tool used by Rancher to generate and deploy a certificate across the entire cluster.\n\n### How to use it\n\nAll the steps will involve the use of a Terminal window\n#### Installation\nThe first thing you need to do is download [this script](https://gist.github.com/nikotrone/50b1a5f8d137411879eb2467e689bfbe) and save it in a folder on your Mac (let's assume `~/bronernetes`) by executing\n```bash\n    mkdir ~/bronernetes\n    cd ~/bronernetes\n    curl https://gist.githubusercontent.com/nikotrone/50b1a5f8d137411879eb2467e689bfbe/raw/090b4b4323d96ac28d96bbb346e2e657073722e6/bronernetes > bronernetes\n    export PATH=$PATH:$(pwd)\n```\n\nNow we have the toolset and you can confirm it works by simply running `bronernetes help`.\n\n#### Spin up Kubernetes\nThe next step is to run the installation process with the command `bronernetes install`\n\n#### Clean up\nWhen you are done or you just want to hard reset your environment you can just type `bronernetes cleanup` and it will take care of cleaning up the VM you just used, leaving you with a pristine machine, as nothing ever happened :)\n\n### Conclusion\n\nHaving a sandbox is very useful to play around with the concepts of a new setup or service and it packs up a huge amount of positive sides. No matter what is the language or the nature of the system you are trying to replicate, it can be challenging and involve a long list of instructions or manual operations and, sometimes, even dedicated hardware. Although with some bash glue, it is possible to automate most of those processes and the investment cost can be enormously beneficial for yourself (less work the next time you do it) and for the other people working with you (they can use the tool, comment and suggest improvements). Most of all, in the case of infrastructure, it helps raise the knowledge of \"what's going on here\" and documents for the ones interested in taking a trip down the rabbit hole.\n","meta":{"title":"How to Spin Up A Kubernetes Cluster On Your Macbook","description":"It is can be useful to create a disposable Kubernetes sandbox to play with when you are exploring a new application and how it could work.","createdAt":"Mon Jan 25 2021 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/spin-up-kubernetes-on-macbook.jpg","imageSource":"https://pixabay.com/it/users/mari_sparrow-13090456/","tags":"Kubernetes, k8s, local","author":"Marco Nicotra","slug":"blog/spin-up-kubernetes-on-macbook","formattedDate":"25 januari 2021","date":"Mon Jan 25 2021 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nAt Vandebron we're maintaining a component library called [Windmolen](https://windmolen.netlify.app/) (Dutch for \"wind turbine\"). And if you've ever built a component library, you probably dealt with optimizing and converting icons before. With SVGO and SVGR you can do this at scale, without compromising the quality or size of your icons.\n\n## The problem\n\nThe web is full of icons, and often these icons are rendered from SVG files to ensure you can increase (or decrease) the size of the icons depending on the use case. Designers often create these icons from design tools like Adobe Photoshop or Sketch. Although these icons might look pretty, exporting a SVG out of these tools is often difficult as [this article](https://medium.com/sketch-app-sources/the-best-way-to-export-an-svg-from-sketch-dd8c66bb6ef2) explains. Also, added lot of code in the form of metadata is added to the SVG file. Let's have a look at what a typical SVG file exported out of Sketch looks like:\n\n```svg\n<!-- something.svg -->\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<svg width=\"14px\" height=\"14px\" viewBox=\"0 0 14 14\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n    <!-- Generator: Sketch 46 (44423) - http://www.bohemiancoding.com/sketch -->\n    <title>last</title>\n    <desc>Created with Sketch.</desc>\n    <defs></defs>\n    <g id=\"Page-1\" stroke=\"none\" stroke-width=\"1\" fill=\"none\" fill-rule=\"evenodd\">\n        <g id=\"last\" transform=\"translate(2.000000, 0.000000)\" fill-rule=\"nonzero\" fill=\"#666666\">\n            <polygon id=\"Fill-2\" points=\"6.6902923 9.6812703 9.3700469 7.0005052 6.6902923 4.3187297 2.37257308 0 0 2.37358354 4.3177192 6.6902923 4.6279322 7.0005052 4.3177192 7.3107182 0 11.6274269 2.37257308 14\"></polygon>\n        </g>\n    </g>\n</svg>\n```\n\nThe SVG file above holds a lot of information about Sketch, such as the `title` of the icon and a `desc`ription. Next to that, there's a lot of elements that could be combined into one element to reduce the file size.\n\n## Optimizing SVGs\n\nWhat's cool about SVG files is that you can optimize and minify them, without affecting what the SVG looks like. This is something you can try out yourself using the website [SVGOMG](https://jakearchibald.github.io/svgomg/), which is powered by the library SVGO that you'll learn more about later.\n\n\nYou can optimize the SVG file above by following these steps:\n\n1. Go to [https://jakearchibald.github.io/svgomg/](https://jakearchibald.github.io/svgomg/)\n2. Click on `Paste markup` an paste the SVG code that you exported from Sketch (a.k.a. the SVG file above)\n3. You will see the icon rendered, now you have to either click at the `Copy as a text` or `Download` button to get the optimized SVG file\n\nWith these simple steps you've optimized the SVG from over 450 bytes, which is already small, to 173 bytes (a decrease of over 62%!). If you'd open this file in the editor of your choice, you can see a lot of the useless (meta)data from the original file has been deleted. Also, the different elements of the SVG are combined in a single `path` that renders the icon:\n\n```svg\n<!-- something.svg -->\n<svg width=\"14\" height=\"14\" xmlns=\"http://www.w3.org/2000/svg\">\n  <path d=\"M8.69 9.681l2.68-2.68-2.68-2.682L4.373 0 2 2.374 6.318 6.69l.31.31-.31.31L2 11.628 4.373 14z\" fill-rule=\"nonzero\" fill=\"#666\"/>\n</svg>\n```\n\nThis SVG can be even further optimized by checking the \"Prefer viewbox to width/height\" in SVGOMG, but let's save that for later when we use SVGO instead.\n\n## Using SVGO\n\nBy using SVGOMG you've already experienced what power [SVGO](https://github.com/svg/svgo) has, as SVGOMG is described by its creators as *\" SVGO's Missing GUI, aiming to expose the majority if not all the configuration options of SVGO\"*. Instead of using the GUI, you can also use SVGO directly from the command line as a CLI-tool or as a Node.js module. For the sake of this article, we'll be using it solely as CLI.\n\nSVGO can be installed globally on your machine, or locally in your project, from npm by running:\n\n```bash\nnpm i -g svgo\n\n# Yarn equivalent\nyarn add -G svgo\n```\n\nAfter doing this you can run `svgo` from the command line and optimize any SVG file instantly. But, you don't want to do this manually on your machine anytime you're adding a new icon to a project (or component library). Therefore, you can also add SVGO to a project locally and add a script to the `package.json` file to optimize all SVGs in a certain directory.\n\n```json\n// package.json\n{\n // ...\n \"scripts\": {\n     // ...\n    \"optimize-svg\": \"svgo --config=.svgo.yml -f ./src/assets/icons\"\n }\n}\n```\n\nThe `optimize-svg` script will run SVGO in the directory `src/assets/icons` and optimize all the SVG files based on the settings in `.svgo.yml`. This file is where you can configure the rules for SVGO, as the previously mentioned \"Prefer viewbox to width/height\":\n\n```yaml\n# .svgo.yml\nplugins:\n  - removeViewBox: false\n  - removeDimensions: true # this deletes width/height and adds it to the viewBox\n  - removeDoctype: true\n  - removeComments: true\n  - removeMetadata: true\n  - removeEditorsNSData: true\n  - cleanupIDs: true\n  - removeRasterImages: true\n  - removeUselessDefs: true\n  - removeUnknownsAndDefaults: true\n  - removeUselessStrokeAndFill: true\n  - removeHiddenElems: true\n  - removeEmptyText: true\n  - removeEmptyAttrs: true\n  - removeEmptyContainers: true\n  - removeUnusedNS: true\n  - removeDesc: true\n  - prefixIds: false\n  - prefixClassNames: false\n```\n   \nFrom the rules above you'll get an idea about all the redundant and useless lines of code that might be present in your SVG files. But luckily, they will all get removed when you run the command `npm run optimize-svg`.\n\n## Converting SVGs with SVGR\n\nYou've now learned how to optimize your SVG files, and are probably wondering how to use these files in a React application. To render an SVG in React, you need to either configure Webpack in a way that it knows how to deal with SVG files or use a library called SVGR. By default, any application created with `create-react-app` can render SVG files as a component, using the following `import` statement:\n\n```jsx\n// MyComponent.jsx\nimport React from 'react';\nimport { ReactComponent as MySVG } from './something.svg';\n\nconst MyComponent = () => {\n  return (\n    <div>\n      <MySVG />\n    </div>\n  );\n}\nexport default MyComponent;\n```\n\nMore information about how this is done can be found in [this article](https://blog.logrocket.com/how-to-use-svgs-in-react/), but let me show you how to solve that with SVGR.\n\nWith [SVGR](https://react-svgr.com/) you can convert SVG files into React Components, either by adding it to Webpack or by using the SVGR CLI or Node.js module. In the same way, as we optimized the SVGs from the command line with SVGO, we can also convert these icons from the command line with SVGR:\n\n```json\n// package.json\n{\n // ...\n \"scripts\": {\n     // ...\n    \"optimize-svg\": \"svgo --config=.svgo.yml -f ./src/assets/icons\",\n    \"convert-svg\": \"svgr -d ./src/components/Icon ./src/assets/icons\"\n }\n}\n```\n\nWhenever you run the command `npm run convert-svg` a JSX file will be created for every SVG file that's present in the directory `src/assets/icons`. These JSX files can be found in the directory `src/components/Icons`, together with an `index.js` file that exports all these components from this directory.\n\nAn example of such a converted SVG file is:\n\n\n```jsx\n// MySVG.jsx\nimport * as React from 'react';\n\nconst MySVG = (props) => (\n  <svg viewBox=\"0 0 14 14\" xmlns=\"http://www.w3.org/2000/svg\" {...props}>\n  <path d=\"M8.69 9.681l2.68-2.68-2.68-2.682L4.373 0 2 2.374 6.318 6.69l.31.31-.31.31L2 11.628 4.373 14z\" fill-rule=\"nonzero\" fill=\"#666\"/>\n  </svg>\n);\n\nexport default MySVG;\n```\n\nAnd, as we now have a directory filled with converted SVGs these can be imported into any React component like this:\n\n```jsx\n// MyComponent.jsx\nimport React from 'react';\nimport MySVG from './MySVG.jsx';\n\nconst MyComponent = () => {\n  return (\n    <div>\n      <MySVG />\n    </div>\n  );\n}\nexport default MyComponent;\n```\n\nOften SVGR is used alongside SVGO, so you can even automatically optimize all SVGS that will be converted by SVGR. This is done by adding the flag `--no-svgo true` and point it towards your SVGO configuration file:\n\n```json\n// package.json\n{\n // ...\n \"scripts\": {\n     // ...\n    \"convert-svg\": \"svgr -d ./src/components/Icon ./src/assets/icons --no-svgo true --svgo-config .svgo.yml\"\n }\n}\n```\n\nBy running the `convert-svg` script you both optimize and convert all the SVG files in `src/assets/icons` to React components based on optimized SVGs.\n\n## Reading further\n\nThe examples in this post are the tip of the metaphorical iceberg on what problems SVGO and SVGR can solve. There are many other features you can enable, such as using them as Node.js modules or enabling TypeScript support. To read further make sure to have a look at the SVGR [playground](https://react-svgr.com/playground/) or [documentation](https://react-svgr.com/docs/getting-started/).\n","meta":{"title":"Optimizing, Converting And Exporting SVG Icons In React","description":"If you've ever build a component library, you probably dealt with optimizing and converting icons before. With SVGO and SVGR you can do this at scale.","createdAt":"Thu Dec 10 2020 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/optimizing-converting-and-exporting-svg-icons-in-react.jpg","tags":"React, component library","author":"Roy Derks","slug":"blog/optimizing-converting-and-exporting-svg-icons-in-react","formattedDate":"10 december 2020","date":"Thu Dec 10 2020 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nHere at Vandebron, we have several projects which need to compute large amounts of data. To achieve acceptable results, we had to choose a computing tool that should have helped us to build such algorithms.\n\nAs you may have read in other articles our main backend language is Scala so the natural choice to build distributed parallel algorithms was indeed Spark.\n\n## What is Spark\n\nWe will briefly introduce Spark in the next few lines and then we will dive deep into some of its key concepts.\n\nSpark is an ETL distributed tool. ETL are three phases that describe a general procedure for moving data from a source to a destination.\n\n![ETL Diagram](/images/etlprocess.png \"ETL\")\n\n- **_Extract_** is the act of retrieving data from a data source which could be a database or a file system.\n- **_Transform_** is the core part of an algorithm. As you may know, functional programming is all about transformation. Whenever you write a block of code in Scala you go from an initial data structure to a resulting data structure, the same goes with Spark but the data structures you use are specific Spark structures we will describe later.\n- **_Load_** is the final part. Here you need to save (load) the resulting data structure from the transformation phase to a data source. This can either be the same as the extract phase or a different one.\n- **_Distributed_**: Spark is meant to be run in a cluster of nodes. Each node runs its own JVM and every Spark data structure can/should be distributed among all the nodes of the cluster (using serialization) to parallelize the computation.\n\n### Spark data structure: RDD, DataFrame, and Dataset\n\nThe core of Spark is its _distributed resilient dataset (RDD)_.\n\n![Spark API history](/images/sparkapihistory.png \"Spark API history\")\n\nAn **_RDD_** is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. _Extracting_ data from a source creates an RDD. Operating on the RDD allows us to _transform_ the data. Writing the RDD _loads_ the data into the end target like a database for example). They are made to be distributed over the cluster to parallelize the computation.\n\nA **_DataFrame_** is an abstraction on top of an RDD. It is the first attempt of Spark (2013) to organize the data inside and RDD with an SQL-like structure. With dataframe, you can actually make a transformation in an SQL fashion. Every element in a dataframe is a Row and you can actually transform a dataframe to another by adding or removing columns.\n\nA **_DataSet_** finally is a further abstraction on top of a dataframe to organize data in an OO fashion (2015). Every element in a dataset is a case class and you can operate transformation in a scala fashion from a case class to another.\n\n## Spark in action\n\nLet’s see now some code samples from our codebase to illustrate in more detail each of the ETL phases.\n\n### Extract\n\nThe extraction phase is the first step in which you gather the data from a datasource.\n\n```scala\nval allConnections = sparkSession\n.read\n.jdbc(connectionString, tableName, props)\n\nval selectedConnections = allConnections\n.select(ColumnNames.head, ColumnNames.tail: _*)\n\nval p4Connections = selectedConnections\n.filter(allConnections(\"HasP4Day activated\").equalTo(1))\n.filter(allConnections(\"HasP4INT activated\").equalTo(1))\n.as[Connection]\n\np4Connections.show()\n```\n\nFor most people the extraction phase is just the first line (the invocation to the read method), they are not wrong because extracting means reading data from a datasource (in this case an SQL server database). I decided to include in this phase also some filtering and projection operations because I think these are not really part of the algorithm, this is still the preparation phase before you actually process the data. We can ultimately say that _preparing the data_ is something in between extraction and transformation therefore it is up to you to decide which phase it belongs to.\n\n### Transform\n\nTransformation phase is the core of the algorithm. Here you actually process your data to reach your final result.\n\n```java scala\nusageDF\n.groupBy('ConnectionId, window('ReadingDate, \"1 day\"))\n.agg(\n    sum('Consumption).as(\"Consumption\"),\n    sum('OffPeak_consumption).as(\"OffPeak_consumption\"),\n    sum('Peak_consumption).as(\"Peak_consumption\"),\n    sum('Production).as(\"Production\"),\n    sum('OffPeak_production).as(\"OffPeak_production\"),\n    sum('Peak_production).as(\"Peak_production\"),\n    first('ReadingDate).as(\"ReadingDate\"),\n    first('marketsegment).as(\"marketsegment\"),\n    collect_set('Source).as(\"Sources\"),\n    collect_set('Tag).as(\"Tags\"),\n    max('Last_modified).as(\"Last_modified\")\n)\n.withColumn(\n    \"Tag\", when(array_contains('Tags, “Interpolated”),\nlit(Tag.Interpolated.toString)).otherwise(lit(“Measured”)))\n.withColumn(\"Source\",\nwhen(size('Sources) > 1,\nlit(Source.Multiple.toString)).otherwise(mkString('Sources)))\n.orderBy('ConnectionId, 'ReadingDate)\n.drop(\"window\", \"sources\", \"tags\")\n```\n\nIn this specific example, we are processing connection usage data by aggregating it daily. In the `usageDF` we have 15 minutes interval usage data, now we want to show to the user the same data but with a different aggregation interval (1 day). So we group the whole data by connection id and window the reading date by 1 day (A window function calculates a return value for every input row of a table based on a group of rows [Introducing Window Functions in Spark SQL - The Databricks Blog](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html).\n\nOnce the data is grouped we can aggregate it, using the `agg` method which allows us to call the aggregation functions over the dataframe (for example: `sum`, `first`,`max` or `collect_set`). Successively we transform the dataframe to suit our visualization needs, the methods used are self-explanatory and the documentation is very clear. [Getting Started - Spark 3.0.1 Documentation](https://spark.apache.org/docs/latest/sql-getting-started.html)\n\n### Load\n\nThe final phase is the one which `save`, `put`, `show` the transformed data into the target data source.\n\n```java scala\ndataFrame\n.select(columns.head, columns.tail: _*)\n.write\n.cassandraFormat(tableName, keySpace)\n.mode(saveMode)\n.save()\n```\n\nIn this specific case, we will save our dataframe into a Cassandra database. In Spark, methods used to achieve the load phase are called _actions_. It is very important to distinguish Spark actions from the rest because actions are the only ones that trigger Spark to actually perform the whole transformation chain you have defined previously.\n\nIf our transformation phase, as we described above, wasn’t followed by an action (for example `save`) nothing would have happened, the software would have simply terminated without doing anything.\n\n## One concept to rule them all\n\n```java scala\nval rdd1 = sc.parallelize(1 to 10)\nval rdd2 = sc.parallelize(11 to 20)\nval rdd2Count = rdd1.map(\nx => rdd2.values.count() * x //This will NEVER work!!!!\n)\n```\n\n_One does not simply use RDD inside another RDD_. (Same goes for Dataframes or Datasets).\n\nThis is a very simple concept that leads very often to lots of questions because many people just want to use Spark as a normal scala library. But this is not possible due to the inner distributed nature of Spark and its data structures. We have said that an RDD is a resilient distributed dataset, let’s focus on the word _distributed_, it means that the data inside it is spread across the nodes of the cluster. Every node has its own JVM and it is called _Executor_, except for the master node where your program starts which is called _Driver_:\n\n![Spark cluster overview](/images/spark-cluster-overview.png \"Spark cluster overview\")\n\nYour code starts from the Driver and a copy is distributed to all executors, this also means that each executor needs to have the same working environment of the Driver, for Scala it is not a problem since it just needs a JVM to run. (but we will see that if you use _pySpark_ you need to take extra care when you distribute your application.) Every Spark data structure you have defined in your code will also be distributed across the executors and every time you perform a transformation it will be performed to each chunk of data in each executor.\n\nNow let’s go back to our example, a `map` is a transformation on `rdd1` this means that block inside will be executed at the executor level, if we need `rdd2` to perform this block Spark should somehow serialize the whole `rdd2` and send it to each executor. You can understand now that _it is really not possible to serialize the whole RDD since it is by its nature already a distributed data structure_. So what can you do to actually perform such computation we showed in the example? The solution is “simple”: _prepare your data in such a way that it will be contained in one single RDD_. To do so you can take advantage of all the transformation functions Spark has to offer such `map` `join` `union` `reduce` etc.\n\n## Next step…\n\nWe have explained all the main concepts of Spark and we have shown some real snippets of our codebase. In the next article, I would like to show you a real-life problem we have solved in our company using [_pySpark_](https://spark.apache.org/docs/latest/api/python/index.html). I will show you how to customize Spark infrastructure to correctly parallelize the ETL algorithm you have built.\n","meta":{"title":"Fueling the Energy Transition With Spark - Part 1","description":"Our main backend language is Scala, and by using Spark we build distributed parallel algorithms to fuel the Energy Transition. But why is Spark the best choice for that job?","createdAt":"Wed Nov 04 2020 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/fueling-the-energy-transition-with-spark-part-1.jpg","imageSource":"https://www.pexels.com/photo/shallow-focus-photography-of-light-bulbs-2764942","tags":"spark, scala","author":"Rosario Renga","slug":"blog/fueling-the-energy-transition-with-spark-part-1","formattedDate":"4 november 2020","date":"Wed Nov 04 2020 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nAt Vandebron we organize a two-day long Hackathon every quarter, and a colleague and I took this chance to dig into the wonderful world of GraalVM.\n\nI've first heard of GraalVM around two years ago when Oleg Šelajev toured through Java User Groups in Germany and held talks about GraalVM. [Here](https://www.youtube.com/watch?v=GinNxS3OSi0) is one from 2019 (not Germany, but Spain this time).\n\nGraalVM promises a significant speedup in compile times and as I am working with Scala, which is notoriously known for its long compile times, this seems interesting. Furthermore, GraalVM provides functionality to build native executables. Meaning, an application can be run without a Java Virtual Machine (JVM).\n  \nThanks to the Hackathon I finally took the time to get to know GraalVM a bit better. With this blog post, I want to share our findings, experiences, and results, as they might be helpful for you too!\n\n## What is GraalVM?\n\nGraalVM is a high-performance JVM that supports efficient ahead-of-time (AOT) and just-in-time (JIT) compilation, but also allows non-JVM languages (e.g. Ruby, Python, C++) to run on the JVM. The ahead-of-time compilation feature is the base for creating native executable programs, meaning an application can be run independently from the JVM. Seeing the versatile features of GraalVM, it is worth looking a bit under its hood.\n\nActually, GraalVM is defined by three main technologies:\n\n- [Graal compiler](https://www.graalvm.org/reference-manual/jvm/), a high-performance JIT-compiler that can make JVM applications run faster from within the JVM\n- [SubstrateVM](https://www.graalvm.org/reference-manual/native-image/SubstrateVM/), includes the necessary components to run a JVM-app as a native executable ( Garbage Collector, Thread Scheduler, etc.)\n- [Truffle Language Implementation Framework](https://www.graalvm.org/graalvm-as-a-platform/language-implementation-framework/), the basis for the polyglot support from GraalVM\n\nOur motivation for trying out GraalVM was tackling the pain points of Scala, Java projects, and microservices. Shipping microservices written in Scala as Docker containers to your production system comes with the cost that startup can be a bit slow, having JVM and Docker overhead, and that those containers can be fairly large, as the application can only be run with a JVM. See [Building Docker images](#building-docker-images) for more information.\n\nDuring the hackathon, we were most interested in building native images for Scala applications. Hoping to reduce the size of our docker containers and reducing up the startup time.\n\n## Project setup\n\nThe project we worked on during the Hackathon is an API that should be used for applicants to submit their applications at Vandebron in the future. By exposing one endpoint through which a resume and contact information can be submitted.\n\nIt is also a good project to test out GraalVM, nothing too complex but also not as simple as \"Hello World\".\n\nThe full setup can be found [on Github](https://github.com/kgrunert/apply-at-vdb). But I'll summarise the used stack below. The project is built around the following libraries, no particular reason, simply because I like them.\n\n- _cats_ for working with effects, such as IO\n- _http4s_ for running the server\n- _tapir_ for defining the endpoints\n- _circe_ for JSON de/serialisation\n- _pureconfig_ for reading config-files\n- _logback_ for logging\n\nThe project can be run via `sbt run` and with Postman or similar a POST-request can be sent like so:\n\n```json\nPOST localhost:8080/api/v1/apply\n\n{\n\t\"email\": \"my@email.de\",\n\t\"name\": \"My Name\",\n\t\"phoneNumber\": \"+310123456789\",\n\t\"applicationBase64\": \"VGhpcyBjb3VsZCBiZSB5b3VyIGFwcGxpY2F0aW9uIQ==\"\n}\n\nResponse:\n\"*confetti* Thanks for handing in your application, we will get back to you within the next days! *confetti*\"\n```\n\n## Setup GraalVM with sbt\n\nWith this initial project setup in mind, GraalVM needs to be installed locally.\n\nFor the installation of GraalVM the [setup guide](https://www.graalvm.org/docs/getting-started-with-graalvm/#install-graalvm) can be followed.\n\nAfter the installation sbt needs to know that not the regular JDK/JVM is used. This can be done with the `java-home` option on sbt bootup.\nTo make the path to GraalVM a bit more accessible and easy to use it can be exported as an environment variable.\n\n```bash\nexport GRAAL_HOME=/Library/Java/JavaVirtualMachines/graalvm-ce-java8-20.1.0/Contents/Home\nsbt -java-home $GRAALHOME\n```\n\nThe path to GraalVM can vary depending on OS and installation. We followed the basic installation for macOS.\n\nNow sbt using GraalVM can be verified with:\n\n```bash\nsbt -java-home $GRAALHOME\nscala> eval System.getProperty(\"java.home\")\n[info] ans: String = /Library/Java/JavaVirtualMachines/graalvm-ce-java8-20.1.0/Contents/Home/jre\n```\n\nThat means everything running in this sbt instance is getting compiled by GraalVM. Awesome!\n\nThe next step is to become strong and independent and learn how to run without an underlying JVM with the help of building native images.\n\n## Building native images\n\nGraalVM ships with the [GraalVM Updater](https://www.graalvm.org/reference-manual/graalvm-updater/) (`gu`) to install the `native-image` on your machine.\n\n```bash\n$GRAALHOME/bin/gu install native-image\n```\n\n[sbt-native-packager](https://sbt-native-packager.readthedocs.io/en/latest/) provides functionality to build packages efficiently (e.g. building Docker images) and added to that, it also provides support for building native images.\nIn order to build native images with sbt commands this plugin has to be added to the project:\n\n```java scala\n// inside project/plugins.sbt\naddSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.7.3\")\n```\n\nAnd the `GraalVMNativeImagePlugin` needs to be enabled:\n\n```java scala\n// inside build.sbt\nenablePlugins(GraalVMNativeImagePlugin)\n```\n\nFrom within sbt it should be able to autocomplete and suggest graal-commands, e.g.:\n\n```java scala\nsbt:apply-at-vdb> graalvm\ngraalvm-native-image:       graalvmNativeImageOptions\n```\n\nWith that setup, native images are just a stone's throw away!\n\n---\n\n### Disclaimer\n\nThe next three sections are not a write-up but rather the main steps we had to take to make the project work. This includes failing images and troubleshooting.\nI want to keep this in because it might be interesting for others when they have to troubleshoot.\nFor the summary and happy path, you can jump directly to [Roundup](#roundup).\n\n---\n\n### First try building a native image\n\nNext up `graalvm-native-image:packageBin` can be run from within sbt. This might take a while (on our systems it took about a minute)\n\nSome warnings start to pop up:\n\n```\n[error] warning: unknown locality of class Lnl/vandebron/applyatvdb/Main$anon$exportedReader$macro$24$1;, assuming class is not local. To remove the warning report an issue to the library or language author. The issue is caused by Lnl/vandebron/applyatvdb/Main$anon$exportedReader$macro$24$1; which is not following the naming convention.\n\n[error] warning: unknown locality of class Lfs2/internal/Algebra$Done$2$;, assuming class is not local. To remove the warning report an issue to the library or language author. The issue is caused by Lfs2/internal/Algebra$Done$2$; which is not following the naming convention.\n```\n\nThe library-specific warnings can be ignored for now. Ultimately it fails with:\n\n```\nError: com.oracle.graal.pointsto.constraints.UnresolvedElementException:\nDiscovered unresolved type during parsing: org.slf4j.impl.StaticLoggerBinder.\nTo diagnose the issue you can use the --allow-incomplete-classpath option.\nThe missing type is then reported at run time when it is accessed the first time.\n```\nActually a good hint on where to start fine-tuning the GraalVM config:\n\n```java scala\n// inside build.sbt\ngraalVMNativeImageOptions ++= Seq(\n\t\"--allow-incomplete-classpath\",\n)\n```\n\nSome things like a `StaticLoggerBinder` only get resolved at runtime, meaning at build time the classpath needs to be allowed to be incomplete. This option allows resolution errors to be ignored at build time and only pop up during runtime.\n\nDuring the build of a native image, GraalVM tries to resolve those runtime dependencies already at compile-time, as it is part of the Ahead-Of-Time-compilation process. With this flag, GraalVM knows \"hey, don't worry about it now, we cross the bridge when we get there\" (or something like that).\n\n### Adding resource files\n\nA `reload` (or restart) of sbt is needed to activate these new options. And we can try to build the native image up new.\nThis time the build finished successfully and the executable file `target/graalvm-native-image/apply-at-vdb` has been created!\nThis is an executable that can be run without a JVM:\n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n```\n\nBut what's that? It actually cannot be started...\n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n*** An error occured! ***\nCannot convert configuration to a de.erewl.pricetracker.server.Config. Failures are:\nat the root:\n- Key not found: 'host'.\n- Key not found: 'port'.\n```\n\nThe first three lines relate to the error that occurred during the first build. It simply says that logging hasn't been set up correctly (maybe due to the absence of a `src/main/resources/logback.xml` or some other misconfiguration), triggering the default setting of not logging anything at all.\nThe second error states that a configuration file does not have the right keys or cannot be found at all.\nLooking into `src/main/resources`:\n\n```bash\nls src/main/resources/\napplication.conf logback.xml\n```\n\nand peeking into `application.conf`:\n\n```bash\ncat src/main/resources/application.conf\n\thost = \"localhost\"\n\tport = 8080\n```\n\nHm, so everything is actually in place. But somehow GraalVM can't find those files.\nIt still requires some more GraalVM fine-tuning here.\n\nBy default, GraalVM doesn't include any resource or configuration-files.\nThe option `-H:ResourceConfigurationFiles=path/to/resource-config.json` defines a path to a JSON configuration file. So inside the `resource-config.json` we can include our `application.conf` and our `logback.xml`.\n\nBut writing those config files can be tedious and it is difficult in larger projects to find all necessary classes that need to be included. GraalVM provides some support with writing those files and actually does all the work. In the project's root directory a configs-folder can be created which will contain all necessary config-files.\n\nFor writing the configuration files we will build a normal JAR-file with the help of the `sbt-assembly` plugin. Adding it to the project like so:\n\n```java scala sbt\n  // inside project/plugins.sbt\n  addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\")\n```\n\nThe JAR-file will be built with `sbt assembly`.\n\nWith that we can now start the application, providing the path to the JAR-file that just has been created:\n\n```bash\nmkdir configs\n$GRAALHOME/bin/java -agentlib:native-image-agent=config-output-dir=./configs -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n```\n\nWith the command above the JAR gets to run with GraalVM but adds [dynamic lookups](https://www.graalvm.org/reference-manual/native-image/Configuration/#assisted-configuration-of-native-image-builds) that are being intercepted during runtime and written to the files: `jni-config.json`, `proxy-config.json`, `reflect-config.json` and `resource-config.json`.\n\nThose generated files can be included in the GraalVMNativeImageOptions:\n\n```java scala\n// build.sbt\ngraalVMNativeImageOptions ++= Seq(\n\t\"--allow-incomplete-classpath\",\n\t\"-H:ResourceConfigurationFiles=../../configs/resource-config.json\",\n\t\"-H:ReflectionConfigurationFiles=../../configs/reflect-config.json\",\n\t\"-H:JNIConfigurationFiles=../../configs/jni-config.json\",\n\t\"-H:DynamicProxyConfigurationFiles=../../configs/proxy-config.json\"\n)\n```\n\nThe build with those updated options should succeed and the app can be run once again: \n\n```bash\ntarget/graalvm-native-image/apply-at-vdb\n\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n```\n\nStill no logging, sadly. But the server is actually running and responds to POST requests via its exposed endpoint:\n\n```json\nPOST localhost:8080/api/v1/apply\n\n{\n\t\"email\": \"my@email.de\",\n\t\"name\": \"My Name\",\n\t\"phoneNumber\": \"+310123456789\",\n\t\"applicationBase64\": \"VGhpcyBjb3VsZCBiZSB5b3VyIGFwcGxpY2F0aW9uIQ==\"\n}\n\nResponse:\n\"*confetti* Thanks for handing in your application, we will get back to you within the next days! *confetti*\"\n```\n\nThe next and last step will investigate why logging is not picked up by GraalVM.\n\n### Investigating the missing logging\n\nSo first I wanted to have a look if it was an overall issue with logging. I stepped back from using logging-framework and tried the most basic logging with the java-integrated `java.util.Logging`. GraalVM's [docs](https://www.graalvm.org/docs/Native-Image/user/LOGGING) stated that GraalVM supports any logging that depends on that.\n\nBuilding and running the native-image with `java.util.Logging` instead of `logback` succeeded and everything is logged properly.\n\nSo it must be something with the dependencies?\n\nFor further investigation, I added the [sbt-dependency-graph](https://github.com/jrudolph/sbt-dependency-graph) plugin and checked out the dependency-tree with `sbt dependencyBrowserTree`. The library `logback` wasn't included in the dependency tree.\nWhich is odd, since `logback` is clearly present in the project's library-dependencies.\n\n```java scala\n// inside build.sbt\nlibraryDependencies ++= Seq(\n\t...\n\t\"ch.qos.logback\" % \"logback-classic\" % \"1.2.3\" % Runtime,\n\t\"ch.qos.logback\" % \"logback-core\" % \"1.2.3\" % Runtime,\n\t...\n)\n```\n\nHaving a closer look, the appendix `% Runtime` on logback's dependency is present.\n\nNot sure where this was coming from but it is most probably blindly copy-pasted from somewhere when gathering the dependencies for this project.\n\n[sbt reference manual](https://www.scala-sbt.org/1.x/docs/Scopes.html#Scoping+by+the+configuration+axis) states that the appendix `Runtime` defines that this dependency will be only included in the runtime classpath.\n\nSo this explains probably why logging was only working when the server was run from inside sbt.\n\nWith removing this and building the native-image, `logback` appears in the dependency-tree, and logging works when the native image is executed!\n\nThis \"bug\" was interesting as it emphasized what GraalVM can NOT do for you. Dynamic class loading/linking can not be supported by GraalVM as classes and dependencies have to be present during compile time to make a fully functional application. \n\n### Roundup\n\nA successful setup of sbt and GraalVM to build native-images requires to:\n\n- install GraalVM's native-image functionality via it's graal-updater: \n  ```bash\n  gu install native-image\n  ```\n- add sbt-native-packager and sbt-assembly to sbt:\n  ```java scala sbt\n  // inside project/plugins.sbt\n  addSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.7.3\")\n  addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\")\n  ```\n- enable the GraalVM-Plugin:\n  ```java scala sbt\n  // inside build.sbt\n  enablePlugins(GraalVMNativeImagePlugin)\n  ```\n- create a fat JAR and define which resource and configuration files should be intergated by intercepting look up calls during its execution:\n  ```bash\n  sbt assembly\n  mkdir configs\n  $GRAALHOME/bin/java -agentlib:native-image-agent=config-output-dir=./configs -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n  ```\n- fine-tune GraalVM with the following options and include the files that have been created in the previous step:\n  ```java scala\n  // build.sbt\n  graalVMNativeImageOptions ++= Seq(\n    \"--allow-incomplete-classpath\",\n    \"-H:ResourceConfigurationFiles=../../configs/resource-config.json\",\n    \"-H:ReflectionConfigurationFiles=../../configs/reflect-config.json\",\n    \"-H:JNIConfigurationFiles=../../configs/jni-config.json\",\n    \"-H:DynamicProxyConfigurationFiles=../../configs/proxy-config.json\"\n  )\n  ```\n- build the native image with:\n  ```bash\n  sbt graalvm-native-image:packageBin\n  ```\n- run the executable file without the need of java\n  ```\n  ./target/graalvm-native-image/apply-at-vdb\n  ```\n\nEven without benchmarking, you notice that the startup time is way faster than with a traditional JAR-file and the application is up and running almost instantly.\n\nIt is worth noting that the creation of a native image is a quite time-consuming process. For this project, it took between 1 and 2 minutes. This is, of course, something a CI/CD-Server like Jenkins would take care of but it has to be kept in mind. \n\nWith a working native-image, it is time to dockerize.\n\n## Building Docker images\n\nIn this section two Docker containers will be built. One, following the \"normal\"-java way and the other will be using the native-image to build a Docker-container without Java.\n\nBefore getting started with native images, a regular JAR-file and Docker image for comparison can be built.\n\nWith the [sbt-assembly](https://github.com/sbt/sbt-assembly) plugin you can create JAR-files with all of its dependencies (fat JARs).\n`sbt assembly` creates this `target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar` which has a size of around 42MB:\n\n```shell\n sbt assembly \n ls -lh target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n\n  ...  ...   42M   target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar\n```\n\nThis application can be run locally via `java -jar target/scala-2.12/apply-at-vdb-assembly-0.1.0-SNAPSHOT.jar` with the prerequisite that Java is installed on that machine.\n\nCreating the Docker image for this JAR-file can be done manually, but luckily `sbt-native-package` supports building regular Docker images out of the box, only the `DockerPlugin` needs to be enabled:\n\n```java scala\n// build.sbt\nenablePlugins(DockerPlugin)\n```\n\n`sbt docker:publishLocal` creates the Docker image `apply-at-vdb`.\n \n```shell\ndocker images | grep apply-at-vdb\n  apply-at-vdb \t0.1.0-SNAPSHOT \t\tf488d4c06f28 \t555MB\n```\n\nA whopping 555MB for a tiny app exposing one endpoint which JAR-file was only 42MB. But to run this JAR-file in a container, this container needs to ship with a JVM, and that's where the overhead lies.\n\nWith that Docker image and JAR-file as a reference, we can now look into how the native-image operates together with Docker.\n\nGraalVM does not support cross-building, meaning an application cannot be expected to be built in a MacOS environment and run in a Linux environment. It has to be built and run on the same platform. With the help of Docker, the desired built environment can be provided.\nThe `Dockerfile` looks as follows:\n```docker\nFROM oracle/graalvm-ce AS builder\nWORKDIR /app/vdb\nRUN gu install native-image\nRUN curl https://bintray.com/sbt/rpm/rpm > bintray-sbt-rpm.repo \\\n\t&& mv bintray-sbt-rpm.repo /etc/yum.repos.d/ \\\n\t&& yum install -y sbt\nCOPY . /app/vdb\nWORKDIR /app/vdb\nRUN sbt \"graalvm-native-image:packageBin\"\n\nFROM oraclelinux:7-slim\nCOPY --from=builder /app/vdb/target/graalvm-native-image/apply-at-vdb ./app/\nCMD ./app/apply-at-vdb\n\n```\n\nAnd can be run with:\n```bash\ndocker build -t native-apply-at-vdb .\n```\nThe Dockerfile describes to do the following:\nThe first docker container, as the name implies, is the builder. As a base image the official [GraalVM image](https://hub.docker.com/r/oracle/graalvm-ce) is used. \n\nThis image needs two more things, GraalVM's native-image command, and sbt, and this is what the two follow-up rows are providing. Once that's done, the project is copied into this container and the native image is built from within sbt.\n\nThe next steps bring the native executable into its own docker container.\nAs a base image, we use an Oracle Linux image and from our builder-container, we copy the native executable to this new container. The last step is that the app gets run on container startup.\n\n`docker run -p 8080:8080 -it native-apply-at-vdb` starts the container and shows that everything is working just as before.\n\nBut what about the image size? Let's have a look.\n```\ndocker images | grep apply-at-vdb\n  native-apply-at-vdb\t\tlatest              17b559e78645\t\t199MB\n  apply-at-vdb\t\t\t0.1.0-SNAPSHOT      f488d4c06f28\t\t555MB\n```\nThat is impressive! We created an app that is approx. 2.8 times smaller than our original app.\n\n## Summary\n\nWe learned how to set up a Scala project with GraalVM, what steps have to be taken to build a native image with GraalVM, and let it run inside a Docker container. We also received a good overview of what's possible with GraalVM and what's not.\n\nThe initial start and setup of GraalVM with sbt is pretty easy and straightforward. Getting GraalVM to compile an sbt project is nice and simple. \n\nThis Hackathon showed us that it is difficult and requires a lot of fine-tuning to integrate GraalVM into an existing project or product. At Vandebron we work with a complex stack of technologies including Spark, Kafka, and Akka which made it difficult to port the findings from this small toy service to one of our existing microservices. This made extensive troubleshooting in the Hackathon not possible.\n\nAll in all, GraalVM allows you to give up some Java overhead and create significant smaller Docker images. Sadly, this comes at the cost of giving up dynamic linking and class loading. \nA silver lining is, that inside Scala's ecosystem this rarely a problem. Scala relies heavily on compile-time mechanisms for detecting bugs early and creating type-safe applications (read [here](https://blog.softwaremill.com/small-fast-docker-images-using-graalvms-native-image-99c0bc92e70b) but also see e.g. [Scala's compiler phases](https://typelevel.org/scala/docs/phases.html)).\n\n* * *\n\n## Sources and Reading\n- [Building Serverless Scala Services with GraalVM](https://www.inner-product.com/posts/serverless-scala-services-with-graalvm/) by Noel Welsh\n- [Small & fast Docker images using GraalVM’s native-image](https://blog.softwaremill.com/small-fast-docker-images-using-graalvms-native-image-99c0bc92e70b) by Adam Warski\n- [Run Scala applications with GraalVM and Docker](https://medium.com/rahasak/run-scala-applications-with-graalvm-and-docker-a1e67701e935) by @itseranga\n- [Getting Started with GraalVM and Scala](https://medium.com/graalvm/getting-started-with-graalvm-for-scala-d0a006dec1d1) by Oleg Šelajev\n- [Updates on Class Initialization in GraalVM Native Image Generation](https://medium.com/graalvm/updates-on-class-initialization-in-graalvm-native-image-generation-c61faca461f7) by \nChristian Wimmer\n- [GraalVM's Reference Manuals](https://www.graalvm.org/reference-manual/)\n","meta":{"title":"Building native images and compiling with GraalVM and sbt","description":"At Vandebron we organized a two-day long Hackathon, a colleague and I took the chance to dig into the wonderful world of GraalVM.","createdAt":"Tue Oct 06 2020 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/building-native-images-and-compiling-with-graalvm-and-sbt.jpg","imageSource":"https://pixabay.com/users/lumix2004-3890388/","tags":"graalvm, scala","author":"Katrin Grunert","slug":"blog/building-native-images-and-compiling-with-graalvm-and-sbt","formattedDate":"6 oktober 2020","date":"Tue Oct 06 2020 00:00:00 GMT+0000 (Coordinated Universal Time)"}},{"content":"\nTwo months ago, I started my journey at Vandebron. One of the projects I first dove into was their efforts to build a [component library](https://windmolen.netlify.app/). Something I was already familiar with from previous companies I worked at. \n\nOn the internet, you can find many articles that describe why a reusable component library is a good investment for your development team(s). Although there's much to say about the advantages of component libraries, most articles don't state the (obvious) disadvantages such projects can have. In this post, I'll point out some of our learnings and why you might not need such a reusable component library.\n\n## About component libraries\n\nOften you find yourself repeating the same lines of code to make, for example, a button or the layout of a page look nice, especially when you're working on multiple projects. Or as a designer, you get frustrated every time the styling for a part of the application is off when a new page or project is created. Many companies have already found multiple solutions to preventing themselves from repeating styling, which is the main reason for design inconsistencies. And therefore component libraries were created.\n\nA component library is a collection of all the styled parts (or components) of a website or multiple websites that make it easier for developers to reuse these parts. Also, designers will know for sure that all components in the component library adhere to their designs, and therefore all projects that use these components will conform. Often these libraries consist of different layers of components, for example, offering atoms, molecules, and organisms when an [Atomic Design](https://bradfrost.com/blog/post/atomic-web-design/) pattern is applied. Following this pattern, developers can use the parts to style their templates and pages consistently.\n\nComponent libraries are becoming more and more popular with the rise of JavaScript libraries and frameworks like React and Vue. These technologies are very suitable for quickly building interactive components that you can use in your application, and can easily be exposed as a library on NPM or Github Packages. At Vandebron, we're building all our web and mobile applications with React and React Native and are using [Storybook](https://storybook.js.org/) to develop our components in a shared library between the engineering and design teams. This can potentially create a lot of advantages for both the developers and designers, as you can read below.\n\n## Why you *might* need a component library\n\nBefore deciding to create a component library for your team or company, you probably want to hear about the advantages such a project can lead to. The main advantages of component libraries are briefly mentioned in the first section above and are often defined as:\n\n- **Reducing code duplication**: With a component library, you can create components that can be shared across multiple websites or applications. This way you no longer have to duplicate styling in different projects. This can seriously decrease the amount of code duplication that you have in your projects, also reducing the number of bugs or design inconsistencies.\n\n- **Preventing design inconsistencies**: By adding all your components and styled parts to the component library you're certain that these will look the same on all the places they're used. Not only will all the components look the same on every page, when designers make a change to one of these components they can be easily updated on all the places they're used.\n\n- **Easier collaborating**: Component libraries make it easier for developers and designers to collaborate on applications and designs, with the component library as the common \"playground\". By using a tool, like Storybook, you can also make this playground visible to non-technical people and show what components are already available to use for new features.\n\nBut these advantages come at a certain price, as I'll explain in the next section.\n\n## Disadvantages of component libraries\n\nBesides the obvious advantages of a component library, it can also have serious disadvantages that are listed below. Whether or not these disadvantages apply to you depends on numerous things that are discussed later on in this article.\n\n- **Increasing complexity**: With all attempts to make code more generic,  an increased level of complexity also comes to play. Reusable components should be easy to extend or customize, which requires you to think about the different use cases beforehand or force you to add many different variations to a component. With every new project that starts to use the component library, you get the risk of increasing the complexity of the library even more.\n\n- **Time-consuming**: Every time you want to add a component to your project, you need to create that component in the component library first and import it locally in the project to test it. Therefore you need to be working in multiple projects at the same time, which requires you to set up a more time-consuming workflow. Also, when you want to use this new component from the library, you have to publish a new version of the library to make the component available.\n\n- **Conflicting dependencies**: When you're using different versions of dependencies across your projects and the component library, you're forced to sync those with each other. Imagine having, for example, an older version of React running in one of your projects that doesn't use a recent React API that you want to use in your component library. In this scenario, you either have to update that project or are unable to keep your component library on par with the latest release of your dependency on React. Both solutions have pros and cons, and would rather be avoided.\n\nAs mentioned before, there are reasons why these disadvantages might apply to you that are the team size, the number of teams and projects at the company, development or release lifecycles, and how your source code is organized. It clearly doesn't make sense to invest in a component library if you have just a small amount of people work on just one project, or a sole team is working on all the different projects making it easier to manage code duplication or design inconsistencies.\n\n## Considerations before starting\n\nThere are two main alternatives that you need to take into consideration before building a reusable component library, which is (obviously) using or extending an existing component library or sourcing your code in a monorepo. \n\n- **Existing component libraries:** Using an existing component library is an efficient way to create consistently (web) pages and reduce the amount of complexity of your own project, while also taking advantage of best practices of large open-source projects. Popular examples of component libraries are [Ant Design For React](https://ant.design/docs/react/introduce) or [various implementations](https://material.io/develop) for Google's Material Design. These libraries allow you to move quickly without having all the overhead of creating complex components but limit you to the design guidelines of these component libraries.\n\n- **Monorepo:** If you don't want to take advantage of existing libraries or are very keen to apply your own styling to components across multiple applications without having to copy-paste the code, you can host the source code of applications in a monorepo. With the monorepo approach, you can create a shared folder that includes all the components used by your applications. This makes it possible to apply changes with a simple pull request and import these components from every project in that repository.\n\nBesides these two alternatives, you also need to have proper design guidelines set by your designer(s). When the design guidelines are flexible and fluctuating, you could be structuring components incorrectly with the risk of doing a lot of work that will be omitted once the project evolves.\n\n## To summarize\n\nComponent libraries are a great way to reduce the amount of code duplication in your applications, prevent design inconsistencies, and increase collaborations between developers, designers, and different teams. But this comes with increased complexity, slower development cycles, and possible code conflicts between projects. Therefore you should consider if using an existing component library or having a monorepo for your source code is a workable solution. At Vandebron we decided to build our own component library (called [windmolen](https://windmolen.netlify.app/)) and if you'd decide the same, then be sure that your design guidelines are properly structured and mature enough.\n","meta":{"title":"When (Not) To Build A Reusable Component Library","description":"You can find much information on why a reusable component library is a good investment, but most articles don't state the (obvious) disadvantages..","createdAt":"Mon Oct 05 2020 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/when-not-to-build-a-reusable-component-library.jpg","imageSource":"https://pixabay.com/users/stevepb-282134/","tags":"React, component library","author":"Roy Derks","slug":"blog/when-not-to-build-a-reusable-component-library","formattedDate":"5 oktober 2020","date":"Mon Oct 05 2020 00:00:00 GMT+0000 (Coordinated Universal Time)"}}]},"__N_SSG":true}