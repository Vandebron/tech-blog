<!DOCTYPE html><html lang="en"><head><script async="" src="https://www.google-analytics.com/analytics.js"></script><link rel="stylesheet" href="https://d381m57et8llfk.cloudfront.net/20210128-6054/static/css/dc7f55cf12b36887a247.fonts.css"/><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
                ga('create', 'UA-49472651-19', { 'storage': 'none' });
                ga('send', 'pageview');</script><script src="https://unpkg.com/react-bootstrap@next/dist/react-bootstrap.min.js" crossorigin="anonymous"></script><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>Fueling the Energy Transition With Spark - Part 1</title><meta name="Description" content="Our main backend language is Scala, and by using Spark we build distributed parallel algorithms to fuel the Energy Transition. But why is Spark the best choice for that job?"/><meta property="og:title" content="Fueling the Energy Transition With Spark - Part 1"/><meta property="og:description" content="Our main backend language is Scala, and by using Spark we build distributed parallel algorithms to fuel the Energy Transition. But why is Spark the best choice for that job?"/><meta property="og:image" content="https://www.vandebron.tech/images/fueling-the-energy-transition-with-spark-part-1.jpg"/><meta name="next-head-count" content="7"/><link rel="preload" href="/pr-preview/pr-47/_next/static/css/cd9ae78b876ddb947f31.css" as="style"/><link rel="stylesheet" href="/pr-preview/pr-47/_next/static/css/cd9ae78b876ddb947f31.css" data-n-g=""/><link rel="preload" href="/pr-preview/pr-47/_next/static/css/7523c8b3ab0d1496040c.css" as="style"/><link rel="stylesheet" href="/pr-preview/pr-47/_next/static/css/7523c8b3ab0d1496040c.css" data-n-g=""/><noscript data-n-css=""></noscript><link rel="preload" href="/pr-preview/pr-47/_next/static/chunks/webpack-0b9cc56c94819e8023ea.js" as="script"/><link rel="preload" href="/pr-preview/pr-47/_next/static/chunks/framework.87b9127d6cd191ae1c9a.js" as="script"/><link rel="preload" href="/pr-preview/pr-47/_next/static/chunks/3ef630e34cd10ba68f9d468ac363ff81c534e1e9.4f04f20a6eec7675b717.js" as="script"/><link rel="preload" href="/pr-preview/pr-47/_next/static/chunks/main-419ac17f185f95278b96.js" as="script"/><link rel="preload" href="/pr-preview/pr-47/_next/static/chunks/7b9503d2.4af90216c47085b5c8e9.js" as="script"/><link rel="preload" href="/pr-preview/pr-47/_next/static/chunks/ad364940.a83a6a4548b93404854d.js" as="script"/><link rel="preload" href="/pr-preview/pr-47/_next/static/chunks/226daff62b2d4bc1e9a7210a7f6000a544319a74.fbdd4d016a5c47ddc9db.js" as="script"/><link rel="preload" href="/pr-preview/pr-47/_next/static/chunks/pages/_app-efc965c1f8dfb605c86b.js" as="script"/><link rel="preload" href="/pr-preview/pr-47/_next/static/chunks/1c49f5f00355f650bee3d37484f094be037a30fa.0c099ecf2acefc71ba39.js" as="script"/><link rel="preload" href="/pr-preview/pr-47/_next/static/chunks/pages/blog/%5Bslug%5D-96888a9e27f1650e1c92.js" as="script"/></head><body><div id="__next"><div class="Container-module__container" as="header" style="padding-top:30px;padding-bottom:30px;margin-bottom:30px"><div class="Row-module__row Row-module__align-items-center Row-module__justify-content-between"><div class="Col-module__col-box-sizing Col-module__col Col-module__col-12 Col-module__col-md-auto Col-module__col-lg-auto Col-module__col-sm-12"><div><div style="cursor:pointer;user-select:none;display:flex;align-items:baseline"><svg width="140" viewBox="0 0 140 22" xmlns="http://www.w3.org/2000/svg"><path d="M9.569 21.109h-4.25L0 6.274h4.597l2.86 9.249 2.86-9.249h4.598L9.57 21.11m13.234-4.258a3.159 3.159 0 0 1 0-6.316 3.156 3.156 0 0 1 3.144 2.884v.543a3.157 3.157 0 0 1-3.144 2.889m3.2-9.81a7.38 7.38 0 1 0 0 13.3c.136.425.537.73 1.003.73h3.174V6.313h-3.174c-.466 0-.867.304-1.003.728m64.522 3.567a3.155 3.155 0 0 0-3.141 2.888v.542a3.155 3.155 0 0 0 3.14 2.885 3.16 3.16 0 0 0 3.165-3.157 3.163 3.163 0 0 0-3.164-3.158m0 10.536a7.337 7.337 0 0 1-3.195-.727 1.056 1.056 0 0 1-1.006.727h-3.171V0h4.23v7.09a7.377 7.377 0 0 1 10.522 6.676c.001 4.074-3.298 7.378-7.38 7.378m26.406-4.22a3.155 3.155 0 0 1-3.141-2.886v-.542a3.155 3.155 0 0 1 3.141-2.888 3.163 3.163 0 0 1 3.164 3.159 3.161 3.161 0 0 1-3.164 3.156m.012-10.511c-5.054 0-7.481 3.978-7.481 7.356v.025c0 3.402 2.427 7.38 7.48 7.38 5.054 0 7.48-3.978 7.48-7.38 0-3.403-2.426-7.381-7.48-7.381m-60.448 4.196a3.155 3.155 0 0 1 3.141 2.888v.542a3.156 3.156 0 0 1-3.141 2.885 3.16 3.16 0 0 1-3.165-3.157 3.163 3.163 0 0 1 3.165-3.158m0 10.536a7.377 7.377 0 0 1-7.381-7.378A7.377 7.377 0 0 1 59.635 7.09V0h4.23v21.144h-3.17c-.473 0-.866-.303-1.006-.727a7.342 7.342 0 0 1-3.195.727zm51.349-14.856v4.222a3.074 3.074 0 0 0-.244-.013c-1.64 0-3.138 1.057-3.138 2.585v8.025h-4.234V6.276h3.17c.473 0 .867.303 1.006.727a7.353 7.353 0 0 1 3.196-.727c.082 0 .162.01.244.012zm25.525 4.21c-1.467 0-2.348.886-2.348 2.584v8.09h-4.285V6.277h3.172c.472 0 .865.304 1.005.727a7.347 7.347 0 0 1 3.195-.727s1.239-.059 2.514.343c3.027.904 3.379 3.962 3.379 5.599v8.955h-4.284v-8.091c0-.883-.205-2.585-2.348-2.585zm-93.333 0c-1.467 0-2.348.886-2.348 2.584v8.09h-4.284V6.277h3.17c.474 0 .866.304 1.006.727a7.354 7.354 0 0 1 3.196-.727s1.239-.059 2.514.343c3.028.904 3.378 3.962 3.378 5.599v8.955h-4.284v-8.091c0-.883-.204-2.585-2.348-2.585zm36.24 1.755h-5.453c-.116-.003-.55-.055-.325-.637a3.336 3.336 0 0 1 6.134.093c.147.495-.246.541-.356.544m2.55-3.796c-1.308-1.384-3.145-2.214-5.284-2.214-4.254 0-7.324 3.598-7.324 7.5 0 4.15 3.398 7.398 7.399 7.398a6.789 6.789 0 0 0 3.976-1.258c1.183-.806 2.165-2.013 2.844-3.574h-4.278c-.503.73-1.208 1.31-2.542 1.31-1.635 0-3.045-.983-3.22-2.468h10.393c.327-2.566-.202-4.806-1.964-6.694" fill="#333D47" fill-rule="evenodd"></path></svg>Â <span class="Text-module__text-default Text-module__u-font-h3 Text-module__u-font-color-green" style="font-size:28px;margin:0;line-height:auto">.tech</span></div></div></div><div class="Col-module__col-box-sizing Col-module__col Col-module__col-12 Col-module__col-md-auto Col-module__col-lg-auto Col-module__col-sm-12"><div class="Flex-module__d-flex Flex-module__flex-row Flex-module__align-items-stretch Flex-module__justify-content-start Flex-module__justify-content-sm-between"><ul class="Navigation-module__navigation-wrapper" id="styled-navigation" style="margin-right:25px"><li class="Navigation-module__navigation-link-wrapper"><a class="Pressable-module__button Pressable-module__text Navigation-module__navigation-link Navigation-module__navigation-link-active" name="Home" url="/"><span>Home</span></a></li><li class="Navigation-module__navigation-link-wrapper"><a class="Pressable-module__button Pressable-module__text Navigation-module__navigation-link" name="About" url="/about"><span>About</span></a></li><li class="Navigation-module__navigation-link-wrapper"><a class="Pressable-module__button Pressable-module__text Navigation-module__navigation-link" name="vandebron.nl" url="https://vandebron.nl"><span>vandebron.nl</span></a></li><div class="Navigation-module__navigation-magic-line" style="transition:none;left:0;width:0"></div></ul><div><a class="Pressable-module__button Pressable-module__text" href="https://github.com/vandebron/" target="_blank" rel="noreferrer" style="margin-right:10px"><svg role="img" viewBox="0 0 24 24" width="1em" height="1em" class="Icon-module__icon Icon-module__u-font-color-charcoal-gray" style="font-size:150%"><title>GitHub icon</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg><span class="visually-hidden">Vandebron on Github</span></a><a class="Pressable-module__button Pressable-module__text" href="https://dev.to/vandebron/" target="_blank" rel="noreferrer" style="margin-right:10px"><svg viewBox="0 0 448 512" width="1em" height="1em" class="Icon-module__icon Icon-module__u-font-color-charcoal-gray" style="font-size:150%"><path fill="currentColor" d="M120.12 208.29c-3.88-2.9-7.77-4.35-11.65-4.35H91.03v104.47h17.45c3.88 0 7.77-1.45 11.65-4.35 3.88-2.9 5.82-7.25 5.82-13.06v-69.65c-.01-5.8-1.96-10.16-5.83-13.06zM404.1 32H43.9C19.7 32 .06 51.59 0 75.8v360.4C.06 460.41 19.7 480 43.9 480h360.2c24.21 0 43.84-19.59 43.9-43.8V75.8c-.06-24.21-19.7-43.8-43.9-43.8zM154.2 291.19c0 18.81-11.61 47.31-48.36 47.25h-46.4V172.98h47.38c35.44 0 47.36 28.46 47.37 47.28l.01 70.93zm100.68-88.66H201.6v38.42h32.57v29.57H201.6v38.41h53.29v29.57h-62.18c-11.16.29-20.44-8.53-20.72-19.69V193.7c-.27-11.15 8.56-20.41 19.71-20.69h63.19l-.01 29.52zm103.64 115.29c-13.2 30.75-36.85 24.63-47.44 0l-38.53-144.8h32.57l29.71 113.72 29.57-113.72h32.58l-38.46 144.8z"></path></svg><span class="visually-hidden">Vandebron on Dev.to</span></a><a class="Pressable-module__button Pressable-module__text" href="https://medium.com/vandebron/" target="_blank" rel="noreferrer"><svg viewBox="0 0 448 512" width="1em" height="1em" class="Icon-module__icon Icon-module__u-font-color-charcoal-gray" style="font-size:150%"><path fill="currentColor" d="M0 32v448h448V32H0zm372.2 106.1l-24 23c-2.1 1.6-3.1 4.2-2.7 6.7v169.3c-.4 2.6.6 5.2 2.7 6.7l23.5 23v5.1h-118V367l24.3-23.6c2.4-2.4 2.4-3.1 2.4-6.7V199.8l-67.6 171.6h-9.1L125 199.8v115c-.7 4.8 1 9.7 4.4 13.2l31.6 38.3v5.1H71.2v-5.1l31.6-38.3c3.4-3.5 4.9-8.4 4.1-13.2v-133c.4-3.7-1-7.3-3.8-9.8L75 138.1V133h87.3l67.4 148L289 133.1h83.2v5z"></path></svg><span class="visually-hidden">Vandebron on Medium</span></a></div></div></div></div></div><div class="Container-module__container"><div class="Row-module__row"><div class="Col-module__col-box-sizing Col-module__col Col-module__col-12"><h2 class="Text-module__text-default Text-module__u-font-h2">Fueling the Energy Transition With Spark - Part 1</h2></div><div class="Col-module__col-box-sizing Col-module__col Col-module__col-12"><p class="Text-module__text-default Text-module__u-font-body"><span class="Text-module__text-default">By Rosario Renga on 4 november 2020</span></p></div></div><div class="Row-module__row"><div class="Col-module__col-box-sizing Col-module__col Col-module__col-12"><p class="Text-module__text-default Text-module__u-font-body"><div class="Image-module__image-container" style="padding-bottom:50%"><div style="height:200px" class="lazyload-placeholder"></div></div><a class="Pressable-module__button Pressable-module__text" href="https://www.pexels.com/photo/shallow-focus-photography-of-light-bulbs-2764942">Image source</a></p></div></div><div class="Row-module__row" style="margin-bottom:60px"><div class="Col-module__col-box-sizing Col-module__col Col-module__col-12"><p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">Here at Vandebron, we have several projects which need to compute large amounts of data. To achieve acceptable results, we had to choose a computing tool that should have helped us to build such algorithms.</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">As you may have read in other articles our main backend language is Scala so the natural choice to build distributed parallel algorithms was indeed Spark.</p>
<h2 class="Text-module__text-default Text-module__u-font-h2">What is Spark</h2>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">We will briefly introduce Spark in the next few lines and then we will dive deep into some of its key concepts.</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">Spark is an ETL distributed tool. ETL are three phases that describe a general procedure for moving data from a source to a destination.</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]"><img src="/images/etlprocess.png" alt="ETL Diagram" style="width:100%" title="ETL" node="[object Object]"/></p>
<ol style="margin-block-start:0;margin-block-end:30px">
<li class="Text-module__text-default Text-module__u-font-body" style="margin-bottom:0"><strong><em>Extract</em></strong> is the act of retrieving data from a data source which could be a database or a file system.</li>
<li class="Text-module__text-default Text-module__u-font-body" style="margin-bottom:0"><strong><em>Transform</em></strong> is the core part of an algorithm. As you may know, functional programming is all about transformation. Whenever you write a block of code in Scala you go from an initial data structure to a resulting data structure, the same goes with Spark but the data structures you use are specific Spark structures we will describe later.</li>
<li class="Text-module__text-default Text-module__u-font-body" style="margin-bottom:0"><strong><em>Load</em></strong> is the final part. Here you need to save (load) the resulting data structure from the transformation phase to a data source. This can either be the same as the extract phase or a different one.</li>
<li class="Text-module__text-default Text-module__u-font-body" style="margin-bottom:0"><strong><em>Distributed</em></strong>: Spark is meant to be run in a cluster of nodes. Each node runs its own JVM and every Spark data structure can/should be distributed among all the nodes of the cluster (using serialization) to parallelize the computation.</li>
</ol>
<h3 class="Text-module__text-default Text-module__u-font-h3">Spark data structure: RDD, DataFrame, and Dataset</h3>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">The core of Spark is its <em>distributed resilient dataset (RDD)</em>.</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]"><img src="/images/sparkapihistory.png" alt="Spark API history" style="width:100%" title="Spark API history" node="[object Object]"/></p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">An <strong><em>RDD</em></strong> is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. <em>Extracting</em> data from a source creates an RDD. Operating on the RDD allows us to <em>transform</em> the data. Writing the RDD <em>loads</em> the data into the end target like a database for example). They are made to be distributed over the cluster to parallelize the computation.</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">A <strong><em>DataFrame</em></strong> is an abstraction on top of an RDD. It is the first attempt of Spark (2013) to organize the data inside and RDD with an SQL-like structure. With dataframe, you can actually make a transformation in an SQL fashion. Every element in a dataframe is a Row and you can actually transform a dataframe to another by adding or removing columns.</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">A <strong><em>DataSet</em></strong> finally is a further abstraction on top of a dataframe to organize data in an OO fashion (2015). Every element in a dataset is a case class and you can operate transformation in a scala fashion from a case class to another.</p>
<h2 class="Text-module__text-default Text-module__u-font-h2">Spark in action</h2>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">Letâs see now some code samples from our codebase to illustrate in more detail each of the ETL phases.</p>
<h3 class="Text-module__text-default Text-module__u-font-h3">Extract</h3>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">The extraction phase is the first step in which you gather the data from a datasource.</p>
<pre><div style="color:#f8f8f2;background:#272822;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code class="language-scala" style="color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none">val allConnections = sparkSession
.read
.jdbc(connectionString, tableName, props)

val selectedConnections = allConnections
.select(ColumnNames.head, ColumnNames.tail: _*)

val p4Connections = selectedConnections
.filter(allConnections(&quot;HasP4Day activated&quot;).equalTo(1))
.filter(allConnections(&quot;HasP4INT activated&quot;).equalTo(1))
.as[Connection]

p4Connections.show()</code></div></pre>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">For most people the extraction phase is just the first line (the invocation to the read method), they are not wrong because extracting means reading data from a datasource (in this case an SQL server database). I decided to include in this phase also some filtering and projection operations because I think these are not really part of the algorithm, this is still the preparation phase before you actually process the data. We can ultimately say that <em>preparing the data</em> is something in between extraction and transformation therefore it is up to you to decide which phase it belongs to.</p>
<h3 class="Text-module__text-default Text-module__u-font-h3">Transform</h3>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">Transformation phase is the core of the algorithm. Here you actually process your data to reach your final result.</p>
<pre><div style="color:#f8f8f2;background:#272822;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code class="language-java" style="color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none">usageDF
.groupBy(&#x27;ConnectionId, window(&#x27;ReadingDate, &quot;1 day&quot;))
.agg(
    sum(&#x27;Consumption).as(&quot;Consumption&quot;),
    sum(&#x27;OffPeak_consumption).as(&quot;OffPeak_consumption&quot;),
    sum(&#x27;Peak_consumption).as(&quot;Peak_consumption&quot;),
    sum(&#x27;Production).as(&quot;Production&quot;),
    sum(&#x27;OffPeak_production).as(&quot;OffPeak_production&quot;),
    sum(&#x27;Peak_production).as(&quot;Peak_production&quot;),
    first(&#x27;ReadingDate).as(&quot;ReadingDate&quot;),
    first(&#x27;marketsegment).as(&quot;marketsegment&quot;),
    collect_set(&#x27;Source).as(&quot;Sources&quot;),
    collect_set(&#x27;Tag).as(&quot;Tags&quot;),
    max(&#x27;Last_modified).as(&quot;Last_modified&quot;)
)
.withColumn(
    &quot;Tag&quot;, when(array_contains(&#x27;Tags, âInterpolatedâ),
lit(Tag.Interpolated.toString)).otherwise(lit(âMeasuredâ)))
.withColumn(&quot;Source&quot;,
when(size(&#x27;Sources) &gt; 1,
lit(Source.Multiple.toString)).otherwise(mkString(&#x27;Sources)))
.orderBy(&#x27;ConnectionId, &#x27;ReadingDate)
.drop(&quot;window&quot;, &quot;sources&quot;, &quot;tags&quot;)</code></div></pre>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">In this specific example, we are processing connection usage data by aggregating it daily. In the <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">usageDF</code> we have 15 minutes interval usage data, now we want to show to the user the same data but with a different aggregation interval (1 day). So we group the whole data by connection id and window the reading date by 1 day (A window function calculates a return value for every input row of a table based on a group of rows <a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" node="[object Object]" style="color:inherit" target="_blank">Introducing Window Functions in Spark SQL - The Databricks Blog</a>.</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">Once the data is grouped we can aggregate it, using the <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">agg</code> method which allows us to call the aggregation functions over the dataframe (for example: <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">sum</code>, <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">first</code>,<code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">max</code> or <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">collect_set</code>). Successively we transform the dataframe to suit our visualization needs, the methods used are self-explanatory and the documentation is very clear. <a href="https://spark.apache.org/docs/latest/sql-getting-started.html" node="[object Object]" style="color:inherit" target="_blank">Getting Started - Spark 3.0.1 Documentation</a></p>
<h3 class="Text-module__text-default Text-module__u-font-h3">Load</h3>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">The final phase is the one which <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">save</code>, <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">put</code>, <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">show</code> the transformed data into the target data source.</p>
<pre><div style="color:#f8f8f2;background:#272822;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code class="language-java" style="color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none">dataFrame
.select(columns.head, columns.tail: _*)
.write
.cassandraFormat(tableName, keySpace)
.mode(saveMode)
.save()</code></div></pre>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">In this specific case, we will save our dataframe into a Cassandra database. In Spark, methods used to achieve the load phase are called <em>actions</em>. It is very important to distinguish Spark actions from the rest because actions are the only ones that trigger Spark to actually perform the whole transformation chain you have defined previously.</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">If our transformation phase, as we described above, wasnât followed by an action (for example <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">save</code>) nothing would have happened, the software would have simply terminated without doing anything.</p>
<h2 class="Text-module__text-default Text-module__u-font-h2">One concept to rule them all</h2>
<pre><div style="color:#f8f8f2;background:#272822;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto;border-radius:0.3em"><code class="language-java" style="color:#f8f8f2;background:none;text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none">val rdd1 = sc.parallelize(1 to 10)
val rdd2 = sc.parallelize(11 to 20)
val rdd2Count = rdd1.map(
x =&gt; rdd2.values.count() * x //This will NEVER work!!!!
)</code></div></pre>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]"><em>One does not simply use RDD inside another RDD</em>. (Same goes for Dataframes or Datasets).</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">This is a very simple concept that leads very often to lots of questions because many people just want to use Spark as a normal scala library. But this is not possible due to the inner distributed nature of Spark and its data structures. We have said that an RDD is a resilient distributed dataset, letâs focus on the word <em>distributed</em>, it means that the data inside it is spread across the nodes of the cluster. Every node has its own JVM and it is called <em>Executor</em>, except for the master node where your program starts which is called <em>Driver</em>:</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]"><img src="/images/spark-cluster-overview.png" alt="Spark cluster overview" style="width:100%" title="Spark cluster overview" node="[object Object]"/></p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">Your code starts from the Driver and a copy is distributed to all executors, this also means that each executor needs to have the same working environment of the Driver, for Scala it is not a problem since it just needs a JVM to run. (but we will see that if you use <em>pySpark</em> you need to take extra care when you distribute your application.) Every Spark data structure you have defined in your code will also be distributed across the executors and every time you perform a transformation it will be performed to each chunk of data in each executor.</p>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">Now letâs go back to our example, a <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">map</code> is a transformation on <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">rdd1</code> this means that block inside will be executed at the executor level, if we need <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">rdd2</code> to perform this block Spark should somehow serialize the whole <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">rdd2</code> and send it to each executor. You can understand now that <em>it is really not possible to serialize the whole RDD since it is by its nature already a distributed data structure</em>. So what can you do to actually perform such computation we showed in the example? The solution is âsimpleâ: <em>prepare your data in such a way that it will be contained in one single RDD</em>. To do so you can take advantage of all the transformation functions Spark has to offer such <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">map</code> <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">join</code> <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">union</code> <code style="background:rgb(0,0,0, 0.1);padding:2px 4px;font-size:80%;color:#000">reduce</code> etc.</p>
<h2 class="Text-module__text-default Text-module__u-font-h2">Next stepâ¦</h2>
<p class="Text-module__text-default Text-module__u-font-body" node="[object Object]">We have explained all the main concepts of Spark and we have shown some real snippets of our codebase. In the next article, I would like to show you a real-life problem we have solved in our company using <a href="https://spark.apache.org/docs/latest/api/python/index.html" node="[object Object]" style="color:inherit" target="_blank"><em>pySpark</em></a>. I will show you how to customize Spark infrastructure to correctly parallelize the ETL algorithm you have built.</p></div></div></div><footer style="padding-top:30px;padding-bottom:30px;background-color:#363639"><div class="Container-module__container"><div class="Row-module__row Row-module__align-items-center" style="margin-bottom:15px"><div class="Col-module__col-box-sizing Col-module__col"><div><div style="cursor:pointer;user-select:none;display:flex;align-items:baseline"><svg width="140" viewBox="0 0 140 22" xmlns="http://www.w3.org/2000/svg"><path d="M9.569 21.109h-4.25L0 6.274h4.597l2.86 9.249 2.86-9.249h4.598L9.57 21.11m13.234-4.258a3.159 3.159 0 0 1 0-6.316 3.156 3.156 0 0 1 3.144 2.884v.543a3.157 3.157 0 0 1-3.144 2.889m3.2-9.81a7.38 7.38 0 1 0 0 13.3c.136.425.537.73 1.003.73h3.174V6.313h-3.174c-.466 0-.867.304-1.003.728m64.522 3.567a3.155 3.155 0 0 0-3.141 2.888v.542a3.155 3.155 0 0 0 3.14 2.885 3.16 3.16 0 0 0 3.165-3.157 3.163 3.163 0 0 0-3.164-3.158m0 10.536a7.337 7.337 0 0 1-3.195-.727 1.056 1.056 0 0 1-1.006.727h-3.171V0h4.23v7.09a7.377 7.377 0 0 1 10.522 6.676c.001 4.074-3.298 7.378-7.38 7.378m26.406-4.22a3.155 3.155 0 0 1-3.141-2.886v-.542a3.155 3.155 0 0 1 3.141-2.888 3.163 3.163 0 0 1 3.164 3.159 3.161 3.161 0 0 1-3.164 3.156m.012-10.511c-5.054 0-7.481 3.978-7.481 7.356v.025c0 3.402 2.427 7.38 7.48 7.38 5.054 0 7.48-3.978 7.48-7.38 0-3.403-2.426-7.381-7.48-7.381m-60.448 4.196a3.155 3.155 0 0 1 3.141 2.888v.542a3.156 3.156 0 0 1-3.141 2.885 3.16 3.16 0 0 1-3.165-3.157 3.163 3.163 0 0 1 3.165-3.158m0 10.536a7.377 7.377 0 0 1-7.381-7.378A7.377 7.377 0 0 1 59.635 7.09V0h4.23v21.144h-3.17c-.473 0-.866-.303-1.006-.727a7.342 7.342 0 0 1-3.195.727zm51.349-14.856v4.222a3.074 3.074 0 0 0-.244-.013c-1.64 0-3.138 1.057-3.138 2.585v8.025h-4.234V6.276h3.17c.473 0 .867.303 1.006.727a7.353 7.353 0 0 1 3.196-.727c.082 0 .162.01.244.012zm25.525 4.21c-1.467 0-2.348.886-2.348 2.584v8.09h-4.285V6.277h3.172c.472 0 .865.304 1.005.727a7.347 7.347 0 0 1 3.195-.727s1.239-.059 2.514.343c3.027.904 3.379 3.962 3.379 5.599v8.955h-4.284v-8.091c0-.883-.205-2.585-2.348-2.585zm-93.333 0c-1.467 0-2.348.886-2.348 2.584v8.09h-4.284V6.277h3.17c.474 0 .866.304 1.006.727a7.354 7.354 0 0 1 3.196-.727s1.239-.059 2.514.343c3.028.904 3.378 3.962 3.378 5.599v8.955h-4.284v-8.091c0-.883-.204-2.585-2.348-2.585zm36.24 1.755h-5.453c-.116-.003-.55-.055-.325-.637a3.336 3.336 0 0 1 6.134.093c.147.495-.246.541-.356.544m2.55-3.796c-1.308-1.384-3.145-2.214-5.284-2.214-4.254 0-7.324 3.598-7.324 7.5 0 4.15 3.398 7.398 7.399 7.398a6.789 6.789 0 0 0 3.976-1.258c1.183-.806 2.165-2.013 2.844-3.574h-4.278c-.503.73-1.208 1.31-2.542 1.31-1.635 0-3.045-.983-3.22-2.468h10.393c.327-2.566-.202-4.806-1.964-6.694" fill="#ffffff" fill-rule="evenodd"></path></svg>Â <span class="Text-module__text-default Text-module__u-font-h3 Text-module__u-font-color-green" style="font-size:28px;margin:0;line-height:auto">.tech</span></div></div></div></div><div class="Row-module__row Row-module__align-items-center"><div class="Col-module__col-box-sizing Col-module__col"><span class="Text-module__text-default" style="color:white">Â© Vandebron</span></div></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"content":"\nHere at Vandebron, we have several projects which need to compute large amounts of data. To achieve acceptable results, we had to choose a computing tool that should have helped us to build such algorithms.\n\nAs you may have read in other articles our main backend language is Scala so the natural choice to build distributed parallel algorithms was indeed Spark.\n\n## What is Spark\n\nWe will briefly introduce Spark in the next few lines and then we will dive deep into some of its key concepts.\n\nSpark is an ETL distributed tool. ETL are three phases that describe a general procedure for moving data from a source to a destination.\n\n![ETL Diagram](/images/etlprocess.png \"ETL\")\n\n- **_Extract_** is the act of retrieving data from a data source which could be a database or a file system.\n- **_Transform_** is the core part of an algorithm. As you may know, functional programming is all about transformation. Whenever you write a block of code in Scala you go from an initial data structure to a resulting data structure, the same goes with Spark but the data structures you use are specific Spark structures we will describe later.\n- **_Load_** is the final part. Here you need to save (load) the resulting data structure from the transformation phase to a data source. This can either be the same as the extract phase or a different one.\n- **_Distributed_**: Spark is meant to be run in a cluster of nodes. Each node runs its own JVM and every Spark data structure can/should be distributed among all the nodes of the cluster (using serialization) to parallelize the computation.\n\n### Spark data structure: RDD, DataFrame, and Dataset\n\nThe core of Spark is its _distributed resilient dataset (RDD)_.\n\n![Spark API history](/images/sparkapihistory.png \"Spark API history\")\n\nAn **_RDD_** is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. _Extracting_ data from a source creates an RDD. Operating on the RDD allows us to _transform_ the data. Writing the RDD _loads_ the data into the end target like a database for example). They are made to be distributed over the cluster to parallelize the computation.\n\nA **_DataFrame_** is an abstraction on top of an RDD. It is the first attempt of Spark (2013) to organize the data inside and RDD with an SQL-like structure. With dataframe, you can actually make a transformation in an SQL fashion. Every element in a dataframe is a Row and you can actually transform a dataframe to another by adding or removing columns.\n\nA **_DataSet_** finally is a further abstraction on top of a dataframe to organize data in an OO fashion (2015). Every element in a dataset is a case class and you can operate transformation in a scala fashion from a case class to another.\n\n## Spark in action\n\nLetâs see now some code samples from our codebase to illustrate in more detail each of the ETL phases.\n\n### Extract\n\nThe extraction phase is the first step in which you gather the data from a datasource.\n\n```scala\nval allConnections = sparkSession\n.read\n.jdbc(connectionString, tableName, props)\n\nval selectedConnections = allConnections\n.select(ColumnNames.head, ColumnNames.tail: _*)\n\nval p4Connections = selectedConnections\n.filter(allConnections(\"HasP4Day activated\").equalTo(1))\n.filter(allConnections(\"HasP4INT activated\").equalTo(1))\n.as[Connection]\n\np4Connections.show()\n```\n\nFor most people the extraction phase is just the first line (the invocation to the read method), they are not wrong because extracting means reading data from a datasource (in this case an SQL server database). I decided to include in this phase also some filtering and projection operations because I think these are not really part of the algorithm, this is still the preparation phase before you actually process the data. We can ultimately say that _preparing the data_ is something in between extraction and transformation therefore it is up to you to decide which phase it belongs to.\n\n### Transform\n\nTransformation phase is the core of the algorithm. Here you actually process your data to reach your final result.\n\n```java scala\nusageDF\n.groupBy('ConnectionId, window('ReadingDate, \"1 day\"))\n.agg(\n    sum('Consumption).as(\"Consumption\"),\n    sum('OffPeak_consumption).as(\"OffPeak_consumption\"),\n    sum('Peak_consumption).as(\"Peak_consumption\"),\n    sum('Production).as(\"Production\"),\n    sum('OffPeak_production).as(\"OffPeak_production\"),\n    sum('Peak_production).as(\"Peak_production\"),\n    first('ReadingDate).as(\"ReadingDate\"),\n    first('marketsegment).as(\"marketsegment\"),\n    collect_set('Source).as(\"Sources\"),\n    collect_set('Tag).as(\"Tags\"),\n    max('Last_modified).as(\"Last_modified\")\n)\n.withColumn(\n    \"Tag\", when(array_contains('Tags, âInterpolatedâ),\nlit(Tag.Interpolated.toString)).otherwise(lit(âMeasuredâ)))\n.withColumn(\"Source\",\nwhen(size('Sources) \u003e 1,\nlit(Source.Multiple.toString)).otherwise(mkString('Sources)))\n.orderBy('ConnectionId, 'ReadingDate)\n.drop(\"window\", \"sources\", \"tags\")\n```\n\nIn this specific example, we are processing connection usage data by aggregating it daily. In the `usageDF` we have 15 minutes interval usage data, now we want to show to the user the same data but with a different aggregation interval (1 day). So we group the whole data by connection id and window the reading date by 1 day (A window function calculates a return value for every input row of a table based on a group of rows [Introducing Window Functions in Spark SQL - The Databricks Blog](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html).\n\nOnce the data is grouped we can aggregate it, using the `agg` method which allows us to call the aggregation functions over the dataframe (for example: `sum`, `first`,`max` or `collect_set`). Successively we transform the dataframe to suit our visualization needs, the methods used are self-explanatory and the documentation is very clear. [Getting Started - Spark 3.0.1 Documentation](https://spark.apache.org/docs/latest/sql-getting-started.html)\n\n### Load\n\nThe final phase is the one which `save`, `put`, `show` the transformed data into the target data source.\n\n```java scala\ndataFrame\n.select(columns.head, columns.tail: _*)\n.write\n.cassandraFormat(tableName, keySpace)\n.mode(saveMode)\n.save()\n```\n\nIn this specific case, we will save our dataframe into a Cassandra database. In Spark, methods used to achieve the load phase are called _actions_. It is very important to distinguish Spark actions from the rest because actions are the only ones that trigger Spark to actually perform the whole transformation chain you have defined previously.\n\nIf our transformation phase, as we described above, wasnât followed by an action (for example `save`) nothing would have happened, the software would have simply terminated without doing anything.\n\n## One concept to rule them all\n\n```java scala\nval rdd1 = sc.parallelize(1 to 10)\nval rdd2 = sc.parallelize(11 to 20)\nval rdd2Count = rdd1.map(\nx =\u003e rdd2.values.count() * x //This will NEVER work!!!!\n)\n```\n\n_One does not simply use RDD inside another RDD_. (Same goes for Dataframes or Datasets).\n\nThis is a very simple concept that leads very often to lots of questions because many people just want to use Spark as a normal scala library. But this is not possible due to the inner distributed nature of Spark and its data structures. We have said that an RDD is a resilient distributed dataset, letâs focus on the word _distributed_, it means that the data inside it is spread across the nodes of the cluster. Every node has its own JVM and it is called _Executor_, except for the master node where your program starts which is called _Driver_:\n\n![Spark cluster overview](/images/spark-cluster-overview.png \"Spark cluster overview\")\n\nYour code starts from the Driver and a copy is distributed to all executors, this also means that each executor needs to have the same working environment of the Driver, for Scala it is not a problem since it just needs a JVM to run. (but we will see that if you use _pySpark_ you need to take extra care when you distribute your application.) Every Spark data structure you have defined in your code will also be distributed across the executors and every time you perform a transformation it will be performed to each chunk of data in each executor.\n\nNow letâs go back to our example, a `map` is a transformation on `rdd1` this means that block inside will be executed at the executor level, if we need `rdd2` to perform this block Spark should somehow serialize the whole `rdd2` and send it to each executor. You can understand now that _it is really not possible to serialize the whole RDD since it is by its nature already a distributed data structure_. So what can you do to actually perform such computation we showed in the example? The solution is âsimpleâ: _prepare your data in such a way that it will be contained in one single RDD_. To do so you can take advantage of all the transformation functions Spark has to offer such `map` `join` `union` `reduce` etc.\n\n## Next stepâ¦\n\nWe have explained all the main concepts of Spark and we have shown some real snippets of our codebase. In the next article, I would like to show you a real-life problem we have solved in our company using [_pySpark_](https://spark.apache.org/docs/latest/api/python/index.html). I will show you how to customize Spark infrastructure to correctly parallelize the ETL algorithm you have built.\n","meta":{"title":"Fueling the Energy Transition With Spark - Part 1","description":"Our main backend language is Scala, and by using Spark we build distributed parallel algorithms to fuel the Energy Transition. But why is Spark the best choice for that job?","createdAt":"Wed Nov 04 2020 00:00:00 GMT+0000 (Coordinated Universal Time)","coverImage":"images/fueling-the-energy-transition-with-spark-part-1.jpg","imageSource":"https://www.pexels.com/photo/shallow-focus-photography-of-light-bulbs-2764942","tags":"spark, scala","author":"Rosario Renga","slug":"blog/fueling-the-energy-transition-with-spark-part-1","formattedDate":"4 november 2020","date":"Wed Nov 04 2020 00:00:00 GMT+0000 (Coordinated Universal Time)"}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"fueling-the-energy-transition-with-spark-part-1"},"buildId":"RKMyrngNMIxpxRXecSBfn","assetPrefix":"/pr-preview/pr-47","isFallback":false,"gsp":true}</script><script nomodule="" src="/pr-preview/pr-47/_next/static/chunks/polyfills-28654a8145d7603786fc.js"></script><script src="/pr-preview/pr-47/_next/static/chunks/webpack-0b9cc56c94819e8023ea.js" async=""></script><script src="/pr-preview/pr-47/_next/static/chunks/framework.87b9127d6cd191ae1c9a.js" async=""></script><script src="/pr-preview/pr-47/_next/static/chunks/3ef630e34cd10ba68f9d468ac363ff81c534e1e9.4f04f20a6eec7675b717.js" async=""></script><script src="/pr-preview/pr-47/_next/static/chunks/main-419ac17f185f95278b96.js" async=""></script><script src="/pr-preview/pr-47/_next/static/chunks/7b9503d2.4af90216c47085b5c8e9.js" async=""></script><script src="/pr-preview/pr-47/_next/static/chunks/ad364940.a83a6a4548b93404854d.js" async=""></script><script src="/pr-preview/pr-47/_next/static/chunks/226daff62b2d4bc1e9a7210a7f6000a544319a74.fbdd4d016a5c47ddc9db.js" async=""></script><script src="/pr-preview/pr-47/_next/static/chunks/pages/_app-efc965c1f8dfb605c86b.js" async=""></script><script src="/pr-preview/pr-47/_next/static/chunks/1c49f5f00355f650bee3d37484f094be037a30fa.0c099ecf2acefc71ba39.js" async=""></script><script src="/pr-preview/pr-47/_next/static/chunks/pages/blog/%5Bslug%5D-96888a9e27f1650e1c92.js" async=""></script><script src="/pr-preview/pr-47/_next/static/RKMyrngNMIxpxRXecSBfn/_buildManifest.js" async=""></script><script src="/pr-preview/pr-47/_next/static/RKMyrngNMIxpxRXecSBfn/_ssgManifest.js" async=""></script></body></html>